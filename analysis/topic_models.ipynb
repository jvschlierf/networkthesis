{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"32G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.1.0\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeutralFile = spark.read.parquet(\"../../Files/Submissions/score/done/Neutr_vacc_d.parquet\")\n",
    "ProFile = spark.read.parquet(\"../../Files/Submissions/score/done/Pro_vacc_d.parquet\")\n",
    "AntiFile = spark.read.parquet(\"../../Files/Submissions/score/done/Anti_vacc_d.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base.document_assembler import DocumentAssembler\n",
    "from sparknlp.base.finisher import Finisher\n",
    "from sparknlp.annotator.stop_words_cleaner import StopWordsCleaner\n",
    "from sparknlp.annotator.normalizer import Normalizer\n",
    "from sparknlp.annotator.token import Tokenizer\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total = unionAll([NeutralFile, ProFile, AntiFile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_n = NeutralFile.sample(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Total.sampleBy(\"class_II\", fractions={\n",
    "    0.0: 0.10,\n",
    "    1.0: 0.10,\n",
    "    2.0: 0.10\n",
    "}, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove stopwords\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"cleanText\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"disabled\")\n",
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "# clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\") \\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner,  \n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the pipeline\n",
    "nlp_model = nlp_pipeline.fit(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pipeline to transform dataframe.\n",
    "processed_df  = nlp_model.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cleanText: string, score: bigint, subreddit: string, created_utc: bigint, class_II: bigint, document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, token: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, normalized: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, cleanTokens: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, tokens: array<string>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298255"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_df = processed_df.select('subreddit', 'score', 'created_utc','tokens')\n",
    "tokens_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=1000, minDF=3.0)\n",
    "# train the model\n",
    "cv_model = cv.fit(tokens_df)\n",
    "# transform the data. Output column name will be features.\n",
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1833:====================================================> (52 + 1) / 53]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL, NUMBER OF TOPICS: 22\n",
      "The lower bound on the log likelihood of the entire corpus: -18170112.909964096\n",
      "The upper bound on perplexity: 6.5659815286261995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "k=22\n",
    "lda = LDA(k=k, maxIter=10, optimizer='em')\n",
    "model = lda.fit(vectorized_tokens)\n",
    "ll = model.logLikelihood(vectorized_tokens)\n",
    "lp = model.logPerplexity(vectorized_tokens)\n",
    "\n",
    "print( f\"ALL, NUMBER OF TOPICS: {k}\")\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "vocab = cv_model.vocabulary\n",
    "topics = model.describeTopics(maxTermsPerTopic = 30)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(vectorized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformed[['subreddit', 'score','created_utc', 'topicDistribution']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf = out.coalesce(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.to_pickle(\"../../Files/models/topics_n_7_distr.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_range = [4,5,6,7,8,9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../Files/models/topic_an.txt\", \"w\") as output:\n",
    "    output.write(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "get\n",
      "time\n",
      "say\n",
      "work\n",
      "thing\n",
      "feel\n",
      "good\n",
      "life\n",
      "need\n",
      "ve\n",
      "year\n",
      "way\n",
      "come\n",
      "tell\n",
      "vaccinate\n",
      "death\n",
      "day\n",
      "right\n",
      "try\n",
      "die\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "get\n",
      "time\n",
      "say\n",
      "work\n",
      "thing\n",
      "need\n",
      "life\n",
      "good\n",
      "feel\n",
      "year\n",
      "ve\n",
      "way\n",
      "come\n",
      "death\n",
      "tell\n",
      "vaccinate\n",
      "day\n",
      "right\n",
      "die\n",
      "try\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "get\n",
      "time\n",
      "say\n",
      "work\n",
      "thing\n",
      "life\n",
      "feel\n",
      "good\n",
      "ve\n",
      "year\n",
      "need\n",
      "way\n",
      "come\n",
      "tell\n",
      "day\n",
      "vaccinate\n",
      "death\n",
      "try\n",
      "right\n",
      "die\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "# extract vocabulary from CountVectorizer\n",
    "\n",
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model4.save('../../Files/models/topic_a_7_d.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import DistributedLDAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model5 = DistributedLDAModel.load('../../Files/models/topic_a_7_d.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "topics = model5.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cv_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_rdd = topics.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('../../Files/models/topic_a_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('../../Files/models/topic_p_n.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=7, maxIter=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('reddit_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7913a71c8e34b61ec22a3bd55dc7c3ae241fdb6cefe9aa5fb754c5121c0e8c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
