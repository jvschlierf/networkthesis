{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/3147567/.ivy2/cache\n",
      "The jars for the packages stored in: /home/3147567/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e45ca135-8ed1-40ae-884d-60e42ac84df7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.1.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 296ms :: artifacts dl 81ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e45ca135-8ed1-40ae-884d-60e42ac84df7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/12ms)\n",
      "22/10/28 09:36:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"32G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.1.0\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "NeutralFile = spark.read.parquet(\"../../Files/Submissions/score/done/Neutr_vacc_d.parquet\")\n",
    "ProFile = spark.read.parquet(\"../../Files/Submissions/score/done/Pro_vacc_d.parquet\")\n",
    "AntiFile = spark.read.parquet(\"../../Files/Submissions/score/done/Anti_vacc_d.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base.document_assembler import DocumentAssembler\n",
    "from sparknlp.base.finisher import Finisher\n",
    "from sparknlp.annotator.stop_words_cleaner import StopWordsCleaner\n",
    "from sparknlp.annotator.normalizer import Normalizer\n",
    "from sparknlp.annotator.token import Tokenizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import size, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total = unionAll([NeutralFile, ProFile, AntiFile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_n = NeutralFile.sample(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Total.sampleBy(\"class_II\", fractions={\n",
    "    0.0: 0.10,\n",
    "    1.0: 0.10,\n",
    "    2.0: 0.10\n",
    "}, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove stopwords\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"cleanText\") \\\n",
    "    .setOutputCol(\"document\") \n",
    "    # .setCleanupMode(\"\")\n",
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "# clean unwanted characters and garbage\n",
    "# normalizer = Normalizer() \\\n",
    "#     .setInputCols([\"token\"]) \\\n",
    "#     .setOutputCol(\"normalized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"token\") \\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "            tokenizer,\n",
    "            # normalizer,\n",
    "            stopwords_cleaner,  \n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the pipeline\n",
    "nlp_model = nlp_pipeline.fit(sample_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pipeline to transform dataframe.\n",
    "processed_df  = nlp_model.transform(sample_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "473254"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_df = processed_df.select('subreddit', 'score', 'created_utc','tokens')\n",
    "tokens_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", minDF=3.0, maxDF=0.3, vocabSize=5000)\n",
    "# train the model\n",
    "cv_model = cv.fit(tokens_df)\n",
    "# transform the data. Output column name will be features.\n",
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/28 09:40:10 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/10/28 09:40:10 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood of the model -37739477.21228218\n",
      "Log Perplexity of the model 7.649306893528074\n",
      "topic: 0\n",
      "*************************\n",
      "[NUM]\n",
      "remove\n",
      "coronavirus\n",
      "police\n",
      "biden\n",
      "say\n",
      "man\n",
      "covid\n",
      "covid[NUM]\n",
      "woman\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "m\n",
      "like\n",
      "good\n",
      "think\n",
      "remove\n",
      "look\n",
      "pandemic\n",
      "covid\n",
      "new\n",
      "people\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "[NUM]\n",
      "virus\n",
      "china\n",
      "people\n",
      "_\n",
      "corona\n",
      "coronavirus\n",
      "world\n",
      "market\n",
      "covid[NUM]\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "[NUM]\n",
      "people\n",
      "[num]\n",
      "year\n",
      "like\n",
      "know\n",
      "time\n",
      "state\n",
      "think\n",
      "work\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "[NUM]\n",
      "new\n",
      "case\n",
      "coronavirus\n",
      "india\n",
      "covid[NUM]\n",
      "report\n",
      "Â \n",
      "china\n",
      "york\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "trump\n",
      "mask\n",
      "[NUM]\n",
      "coronavirus\n",
      "people\n",
      "say\n",
      "covid\n",
      "delete\n",
      "wear\n",
      "house\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "[NUM]\n",
      "coronavirus\n",
      "test\n",
      "covid[NUM]\n",
      "[url]\n",
      "covid[num]\n",
      "covid\n",
      "death\n",
      "case\n",
      "positive\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "k=7\n",
    "lda = LDA(k=k, maxIter=10,  topicConcentration=0.5)\n",
    "model = lda.fit(vectorized_tokens)\n",
    "ll = model.logLikelihood(vectorized_tokens)\n",
    "lp = model.logPerplexity(vectorized_tokens)\n",
    "\n",
    "topics = model.describeTopics(maxTermsPerTopic = 10)\n",
    "vocab = cv_model.vocabulary\n",
    "print(f\"Log Likelihood of the model {ll}\")\n",
    "print(f\"Log Perplexity of the model {lp}\")\n",
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(vectorized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.describeTopics(maxTermsPerTopic = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.describeTopics(maxTermsPerTopic = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "[NUM]\n",
      "covid\n",
      "dose\n",
      "get\n",
      "covid[NUM]\n",
      "day\n",
      "feel\n",
      "pfizer\n",
      "vaccinate\n",
      "shot\n",
      "vaccination\n",
      "m\n",
      "say\n",
      "like\n",
      "covid[num]\n",
      "good\n",
      "people\n",
      "second\n",
      "go\n",
      "new\n",
      "effect\n",
      "remove\n",
      "coronavirus\n",
      "week\n",
      "arm\n",
      "time\n",
      "booster\n",
      "moderna\n",
      "[num]\n",
      "work\n",
      "experience\n",
      "hour\n",
      "start\n",
      "know\n",
      "year\n",
      "ve\n",
      "take\n",
      "want\n",
      "health\n",
      "need\n",
      "receive\n",
      "pain\n",
      "bad\n",
      "help\n",
      "month\n",
      "mandate\n",
      "think\n",
      "today\n",
      "unvaccinated\n",
      "symptom\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "[NUM]\n",
      "covid\n",
      "dose\n",
      "get\n",
      "covid[NUM]\n",
      "day\n",
      "feel\n",
      "vaccinate\n",
      "pfizer\n",
      "shot\n",
      "vaccination\n",
      "m\n",
      "say\n",
      "like\n",
      "covid[num]\n",
      "second\n",
      "good\n",
      "people\n",
      "go\n",
      "new\n",
      "remove\n",
      "effect\n",
      "coronavirus\n",
      "[num]\n",
      "week\n",
      "arm\n",
      "time\n",
      "booster\n",
      "moderna\n",
      "experience\n",
      "work\n",
      "hour\n",
      "start\n",
      "know\n",
      "year\n",
      "ve\n",
      "take\n",
      "want\n",
      "receive\n",
      "need\n",
      "health\n",
      "pain\n",
      "bad\n",
      "month\n",
      "help\n",
      "mandate\n",
      "think\n",
      "symptom\n",
      "today\n",
      "unvaccinated\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "[NUM]\n",
      "covid\n",
      "dose\n",
      "get\n",
      "covid[NUM]\n",
      "day\n",
      "feel\n",
      "pfizer\n",
      "vaccinate\n",
      "shot\n",
      "vaccination\n",
      "m\n",
      "say\n",
      "like\n",
      "covid[num]\n",
      "good\n",
      "people\n",
      "second\n",
      "go\n",
      "remove\n",
      "new\n",
      "effect\n",
      "coronavirus\n",
      "week\n",
      "time\n",
      "arm\n",
      "booster\n",
      "moderna\n",
      "[num]\n",
      "work\n",
      "experience\n",
      "hour\n",
      "start\n",
      "know\n",
      "take\n",
      "year\n",
      "ve\n",
      "want\n",
      "receive\n",
      "health\n",
      "need\n",
      "bad\n",
      "pain\n",
      "month\n",
      "help\n",
      "mandate\n",
      "think\n",
      "today\n",
      "symptom\n",
      "unvaccinated\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "[NUM]\n",
      "covid\n",
      "dose\n",
      "get\n",
      "covid[NUM]\n",
      "day\n",
      "feel\n",
      "pfizer\n",
      "vaccination\n",
      "vaccinate\n",
      "shot\n",
      "m\n",
      "say\n",
      "like\n",
      "covid[num]\n",
      "good\n",
      "people\n",
      "go\n",
      "second\n",
      "remove\n",
      "effect\n",
      "new\n",
      "coronavirus\n",
      "week\n",
      "time\n",
      "arm\n",
      "[num]\n",
      "booster\n",
      "moderna\n",
      "work\n",
      "experience\n",
      "hour\n",
      "start\n",
      "know\n",
      "year\n",
      "ve\n",
      "take\n",
      "want\n",
      "health\n",
      "receive\n",
      "need\n",
      "bad\n",
      "pain\n",
      "help\n",
      "month\n",
      "mandate\n",
      "think\n",
      "today\n",
      "symptom\n",
      "unvaccinated\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "[NUM]\n",
      "covid\n",
      "dose\n",
      "get\n",
      "covid[NUM]\n",
      "day\n",
      "feel\n",
      "pfizer\n",
      "vaccinate\n",
      "shot\n",
      "vaccination\n",
      "m\n",
      "say\n",
      "like\n",
      "covid[num]\n",
      "second\n",
      "people\n",
      "go\n",
      "good\n",
      "effect\n",
      "remove\n",
      "coronavirus\n",
      "new\n",
      "week\n",
      "[num]\n",
      "time\n",
      "arm\n",
      "moderna\n",
      "booster\n",
      "work\n",
      "experience\n",
      "hour\n",
      "start\n",
      "know\n",
      "ve\n",
      "year\n",
      "take\n",
      "want\n",
      "receive\n",
      "need\n",
      "health\n",
      "pain\n",
      "bad\n",
      "help\n",
      "month\n",
      "mandate\n",
      "today\n",
      "think\n",
      "symptom\n",
      "unvaccinated\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "[NUM]\n",
      "covid\n",
      "dose\n",
      "get\n",
      "covid[NUM]\n",
      "day\n",
      "feel\n",
      "pfizer\n",
      "shot\n",
      "vaccinate\n",
      "vaccination\n",
      "m\n",
      "say\n",
      "covid[num]\n",
      "like\n",
      "second\n",
      "good\n",
      "go\n",
      "people\n",
      "remove\n",
      "coronavirus\n",
      "effect\n",
      "new\n",
      "week\n",
      "arm\n",
      "time\n",
      "moderna\n",
      "[num]\n",
      "booster\n",
      "experience\n",
      "work\n",
      "hour\n",
      "start\n",
      "know\n",
      "year\n",
      "ve\n",
      "take\n",
      "want\n",
      "receive\n",
      "health\n",
      "need\n",
      "pain\n",
      "bad\n",
      "mandate\n",
      "help\n",
      "month\n",
      "think\n",
      "today\n",
      "unvaccinated\n",
      "symptom\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "[NUM]\n",
      "covid\n",
      "dose\n",
      "get\n",
      "covid[NUM]\n",
      "day\n",
      "feel\n",
      "pfizer\n",
      "vaccinate\n",
      "shot\n",
      "vaccination\n",
      "m\n",
      "say\n",
      "like\n",
      "covid[num]\n",
      "good\n",
      "second\n",
      "go\n",
      "people\n",
      "remove\n",
      "effect\n",
      "new\n",
      "coronavirus\n",
      "week\n",
      "time\n",
      "arm\n",
      "[num]\n",
      "booster\n",
      "moderna\n",
      "work\n",
      "experience\n",
      "hour\n",
      "start\n",
      "know\n",
      "year\n",
      "ve\n",
      "take\n",
      "want\n",
      "receive\n",
      "need\n",
      "health\n",
      "pain\n",
      "bad\n",
      "help\n",
      "mandate\n",
      "month\n",
      "think\n",
      "today\n",
      "symptom\n",
      "unvaccinated\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Topic Term Distribution\n",
    "term_topics = topics.coalesce(1).toPandas()\n",
    "wordict = dict(zip(term_topics['termIndices'][0], term_topics['termWeights'][0]))\n",
    "wordict1 = dict(zip(term_topics['termIndices'][1], term_topics['termWeights'][1]))\n",
    "wordict2 = dict(zip(term_topics['termIndices'][2], term_topics['termWeights'][2]))\n",
    "wordict3 = dict(zip(term_topics['termIndices'][3], term_topics['termWeights'][3]))\n",
    "wordict4 = dict(zip(term_topics['termIndices'][4], term_topics['termWeights'][4]))\n",
    "wordict5 = dict(zip(term_topics['termIndices'][5], term_topics['termWeights'][5]))\n",
    "wordict6 = dict(zip(term_topics['termIndices'][6], term_topics['termWeights'][6]))\n",
    "df = pd.DataFrame([wordict, wordict1, wordict2, wordict3, wordict4, wordict5, wordict6  ]) # \n",
    "df.to_pickle('../../Files/models/topics/term_topics_n_7_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get doc_topic distribution\n",
    "transformed = model.transform(vectorized_tokens)\n",
    "out = transformed[['subreddit', 'score','created_utc', 'topicDistribution']]\n",
    "\n",
    "pdf = out.toPandas() # , 'topic_4', 'topic_5', 'topic_6'\n",
    "pdf[['topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6']] = pd.DataFrame(pdf['topicDistribution'].to_list(), columns=['topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6'])\n",
    "\n",
    "pdf.drop(columns='topicDistribution', inplace=True)\n",
    "pdf.to_pickle(\"../../Files/models/topics/doctop_n_7_distr_2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Get Doc Length\n",
    "countdf = tokens_df.select(size('tokens').alias('doc_len'))\n",
    "counts = countdf.toPandas()\n",
    "counts.to_pickle('../../Files/models/topics/doclen_n_7_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Vocabulary\n",
    "vocab = cv_model.vocabulary\n",
    "with open('../../Files/models/topics/n_7_vocab_2.txt', 'w') as file:\n",
    "    for item in vocab:\n",
    "        file.write(f'{item} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get term frequency\n",
    "import pyspark.sql.functions as f\n",
    "combined_df = (\n",
    "tokens_df.select(f.explode('tokens').alias('col'))\n",
    "      .select(f.collect_list('col').alias('tokens'))\n",
    ")\n",
    "counts = cv_model.transform(combined_df).select('features').collect()\n",
    "Tf = dict(zip(vocab, counts[0]['features'].values))\n",
    "Tf_v = list(Tf.values())\n",
    "with open('../../Files/models/topics/n_7_tf_2.txt', 'w') as file:\n",
    "    for item in Tf_v:\n",
    "        file.write(f'{item} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONT LOOK HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_len = tokens_df.select('tokens').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "countdf = tokens_df.select(size('tokens').alias('doc_len'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "docLen = countdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "countdf.write.csv(\"../../Files/models/doclen.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(vectorized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformed[['subreddit', 'score','created_utc', 'topicDistribution']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = (\n",
    "tokens_df.select(f.explode('tokens').alias('col'))\n",
    "      .select(f.collect_list('col').alias('tokens'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "counts = cv_model.transform(combined_df).select('features').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf = dict(zip(cv_model.vocabulary, counts[0]['features'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_v = list(Tf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf = out.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>topicDistribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>8</td>\n",
       "      <td>1586963819</td>\n",
       "      <td>[0.3334551586768651, 0.3339027337180743, 0.332...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>1</td>\n",
       "      <td>1600297040</td>\n",
       "      <td>[0.3336362057577024, 0.3326746584696878, 0.333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>1</td>\n",
       "      <td>1599356238</td>\n",
       "      <td>[0.33321316659717315, 0.333259202922333, 0.333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>1</td>\n",
       "      <td>1599130056</td>\n",
       "      <td>[0.327864080763441, 0.35364011028479353, 0.318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>1</td>\n",
       "      <td>1602832667</td>\n",
       "      <td>[0.33341178135066096, 0.3327875545975474, 0.33...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  score  created_utc  \\\n",
       "0   altnewz      8   1586963819   \n",
       "1   altnewz      1   1600297040   \n",
       "2   altnewz      1   1599356238   \n",
       "3   altnewz      1   1599130056   \n",
       "4   altnewz      1   1602832667   \n",
       "\n",
       "                                   topicDistribution  \n",
       "0  [0.3334551586768651, 0.3339027337180743, 0.332...  \n",
       "1  [0.3336362057577024, 0.3326746584696878, 0.333...  \n",
       "2  [0.33321316659717315, 0.333259202922333, 0.333...  \n",
       "3  [0.327864080763441, 0.35364011028479353, 0.318...  \n",
       "4  [0.33341178135066096, 0.3327875545975474, 0.33...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2 = pdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2 = pd.DataFrame(pdf['topicDistribution'].to_list(), columns=['topic_0', 'topic_1', 'topic_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2[['topic_0', 'topic_1', 'topic_2']] = pd.DataFrame(pdf['topicDistribution'].to_list(), columns=['topic_0', 'topic_1', 'topic_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>8</td>\n",
       "      <td>1586963819</td>\n",
       "      <td>0.333455</td>\n",
       "      <td>0.333903</td>\n",
       "      <td>0.332642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>1</td>\n",
       "      <td>1600297040</td>\n",
       "      <td>0.333636</td>\n",
       "      <td>0.332675</td>\n",
       "      <td>0.333689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>1</td>\n",
       "      <td>1599356238</td>\n",
       "      <td>0.333213</td>\n",
       "      <td>0.333259</td>\n",
       "      <td>0.333528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>1</td>\n",
       "      <td>1599130056</td>\n",
       "      <td>0.327864</td>\n",
       "      <td>0.353640</td>\n",
       "      <td>0.318496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>altnewz</td>\n",
       "      <td>1</td>\n",
       "      <td>1602832667</td>\n",
       "      <td>0.333412</td>\n",
       "      <td>0.332788</td>\n",
       "      <td>0.333801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  score  created_utc   topic_0   topic_1   topic_2\n",
       "0   altnewz      8   1586963819  0.333455  0.333903  0.332642\n",
       "1   altnewz      1   1600297040  0.333636  0.332675  0.333689\n",
       "2   altnewz      1   1599356238  0.333213  0.333259  0.333528\n",
       "3   altnewz      1   1599130056  0.327864  0.353640  0.318496\n",
       "4   altnewz      1   1602832667  0.333412  0.332788  0.333801"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2.to_pickle('../../Files/models/topics_a_3_td.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2.drop(columns='topicDistribution', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.to_pickle(\"../../Files/models/topics_n_7_distr.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "get\n",
      "time\n",
      "say\n",
      "work\n",
      "thing\n",
      "life\n",
      "good\n",
      "feel\n",
      "need\n",
      "year\n",
      "ve\n",
      "way\n",
      "come\n",
      "tell\n",
      "vaccinate\n",
      "day\n",
      "death\n",
      "right\n",
      "die\n",
      "try\n",
      "bad\n",
      "mask\n",
      "look\n",
      "world\n",
      "find\n",
      "take\n",
      "post\n",
      "believe\n",
      "url\n",
      "virus\n",
      "remove\n",
      "new\n",
      "make\n",
      "start\n",
      "person\n",
      "test\n",
      "live\n",
      "see\n",
      "num\n",
      "cause\n",
      "happen\n",
      "kill\n",
      "government\n",
      "talk\n",
      "stop\n",
      "help\n",
      "long\n",
      "mean\n",
      "man\n",
      "job\n",
      "point\n",
      "let\n",
      "ask\n",
      "child\n",
      "vaccination\n",
      "lot\n",
      "actually\n",
      "health\n",
      "shit\n",
      "end\n",
      "fuck\n",
      "family\n",
      "give\n",
      "care\n",
      "friend\n",
      "love\n",
      "human\n",
      "use\n",
      "ass\n",
      "case\n",
      "woman\n",
      "wear\n",
      "change\n",
      "big\n",
      "well\n",
      "question\n",
      "reason\n",
      "week\n",
      "guy\n",
      "shot\n",
      "kid\n",
      "leave\n",
      "hate\n",
      "month\n",
      "mandate\n",
      "pay\n",
      "medium\n",
      "doctor\n",
      "real\n",
      "state\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "people\n",
      "NUM\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "get\n",
      "time\n",
      "say\n",
      "work\n",
      "thing\n",
      "feel\n",
      "life\n",
      "good\n",
      "ve\n",
      "need\n",
      "year\n",
      "way\n",
      "come\n",
      "tell\n",
      "vaccinate\n",
      "day\n",
      "right\n",
      "try\n",
      "death\n",
      "die\n",
      "bad\n",
      "mask\n",
      "world\n",
      "look\n",
      "find\n",
      "take\n",
      "believe\n",
      "post\n",
      "url\n",
      "new\n",
      "remove\n",
      "make\n",
      "start\n",
      "virus\n",
      "person\n",
      "live\n",
      "see\n",
      "talk\n",
      "cause\n",
      "happen\n",
      "kill\n",
      "num\n",
      "help\n",
      "stop\n",
      "mean\n",
      "government\n",
      "fuck\n",
      "long\n",
      "job\n",
      "point\n",
      "man\n",
      "let\n",
      "ask\n",
      "lot\n",
      "actually\n",
      "child\n",
      "end\n",
      "health\n",
      "test\n",
      "shit\n",
      "give\n",
      "love\n",
      "family\n",
      "vaccination\n",
      "friend\n",
      "care\n",
      "use\n",
      "human\n",
      "wear\n",
      "well\n",
      "big\n",
      "change\n",
      "reason\n",
      "woman\n",
      "question\n",
      "guy\n",
      "shot\n",
      "hate\n",
      "leave\n",
      "case\n",
      "kid\n",
      "mandate\n",
      "medium\n",
      "money\n",
      "month\n",
      "hear\n",
      "week\n",
      "state\n",
      "pay\n",
      "old\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "get\n",
      "time\n",
      "say\n",
      "work\n",
      "thing\n",
      "life\n",
      "feel\n",
      "good\n",
      "need\n",
      "ve\n",
      "year\n",
      "death\n",
      "way\n",
      "come\n",
      "tell\n",
      "vaccinate\n",
      "day\n",
      "die\n",
      "try\n",
      "right\n",
      "bad\n",
      "test\n",
      "mask\n",
      "world\n",
      "look\n",
      "find\n",
      "post\n",
      "take\n",
      "believe\n",
      "remove\n",
      "new\n",
      "virus\n",
      "start\n",
      "make\n",
      "url\n",
      "person\n",
      "live\n",
      "num\n",
      "see\n",
      "kill\n",
      "cause\n",
      "happen\n",
      "fuck\n",
      "government\n",
      "help\n",
      "long\n",
      "mean\n",
      "talk\n",
      "stop\n",
      "point\n",
      "job\n",
      "man\n",
      "vaccination\n",
      "let\n",
      "ask\n",
      "health\n",
      "lot\n",
      "child\n",
      "actually\n",
      "end\n",
      "give\n",
      "shit\n",
      "family\n",
      "care\n",
      "friend\n",
      "love\n",
      "case\n",
      "use\n",
      "human\n",
      "change\n",
      "wear\n",
      "well\n",
      "big\n",
      "week\n",
      "woman\n",
      "question\n",
      "reason\n",
      "guy\n",
      "shot\n",
      "hate\n",
      "kid\n",
      "leave\n",
      "mandate\n",
      "state\n",
      "medium\n",
      "month\n",
      "hear\n",
      "pay\n",
      "d\n",
      "system\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "# extract vocabulary from CountVectorizer\n",
    "\n",
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_topics = topics.coalesce(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>termIndices</th>\n",
       "      <th>termWeights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[0.02407657465086515, 0.02137233258698163, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[0.021785314593184744, 0.02101035696233249, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[0.02578380621754275, 0.021577177569482697, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic                                        termIndices  \\\n",
       "0      0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "1      1  [1, 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "2      2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "\n",
       "                                         termWeights  \n",
       "0  [0.02407657465086515, 0.02137233258698163, 0.0...  \n",
       "1  [0.021785314593184744, 0.02101035696233249, 0....  \n",
       "2  [0.02578380621754275, 0.021577177569482697, 0....  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = pd.DataFrame(term_topics['termIndices'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces = term_topics['termIndices'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [vocab[word] for word in indeces ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordict = dict(zip([vocab[word] for word in term_topics['termIndices'][0]], term_topics['termWeights'][0]))\n",
    "wordict1 = dict(zip([vocab[word] for word in term_topics['termIndices'][1]], term_topics['termWeights'][1]))\n",
    "wordict2 = dict(zip([vocab[word] for word in term_topics['termIndices'][2]], term_topics['termWeights'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordict = dict(zip(term_topics['termIndices'][0], term_topics['termWeights'][0]))\n",
    "wordict1 = dict(zip(term_topics['termIndices'][1], term_topics['termWeights'][1]))\n",
    "wordict2 = dict(zip(term_topics['termIndices'][2], term_topics['termWeights'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([wordict, wordict1, wordict2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../../Files/models/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>95</th>\n",
       "      <th>94</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>102</th>\n",
       "      <th>101</th>\n",
       "      <th>104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024077</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>0.020059</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>0.010530</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>0.007470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021010</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>0.019893</td>\n",
       "      <td>0.012750</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.007767</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025784</td>\n",
       "      <td>0.021577</td>\n",
       "      <td>0.019976</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.010509</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>0.007616</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.001902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.024077  0.021372  0.020059  0.012459  0.010530  0.009011  0.008714   \n",
       "1  0.021010  0.021785  0.019893  0.012750  0.010856  0.009169  0.008917   \n",
       "2  0.025784  0.021577  0.019976  0.012191  0.010509  0.008990  0.008712   \n",
       "\n",
       "        7         8         9    ...       95        94        99        100  \\\n",
       "0  0.008228  0.007627  0.007470  ...  0.001962  0.001941  0.001934  0.001925   \n",
       "1  0.008264  0.007767  0.007692  ...  0.001941  0.001982       NaN       NaN   \n",
       "2  0.008391  0.007616  0.007419  ...  0.001917  0.001927       NaN       NaN   \n",
       "\n",
       "        96        97        98        102       101       104  \n",
       "0  0.001920       NaN       NaN       NaN       NaN       NaN  \n",
       "1  0.001946  0.001979  0.001957  0.001925       NaN       NaN  \n",
       "2  0.001934       NaN  0.001918       NaN  0.001904  0.001902  \n",
       "\n",
       "[3 rows x 105 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "      <td>92</td>\n",
       "      <td>95</td>\n",
       "      <td>94</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "      <td>97</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>88</td>\n",
       "      <td>96</td>\n",
       "      <td>95</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "      <td>94</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>95</td>\n",
       "      <td>101</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  90  91  92  93  94  95  96  \\\n",
       "0   0   1   2   3   4   5   6   7   8   9  ...  90  91  89  93  92  95  94   \n",
       "1   1   0   2   3   4   5   6   7   8   9  ...  90  92  94  97  93  98  88   \n",
       "2   0   1   2   3   4   5   6   7   8   9  ...  90  91  92  96  94  93  98   \n",
       "\n",
       "   97   98   99  \n",
       "0  99  100   96  \n",
       "1  96   95  102  \n",
       "2  95  101  104  \n",
       "\n",
       "[3 rows x 100 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_topics['words'] = word for words in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model4.save('../../Files/models/topic_a_7_d.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import DistributedLDAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model5 = DistributedLDAModel.load('../../Files/models/topic_a_7_d.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "topics = model5.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "NUM\n",
      "people\n",
      "vaccine\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "covid\n",
      "go\n",
      "want\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('../../Files/models/topic_a_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('../../Files/models/topic_p_n.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=7, maxIter=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('reddit_env_test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c5bfe69c2b2d435baea75ee1a7865fc7666bb526179204de658e4f2811cb086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
