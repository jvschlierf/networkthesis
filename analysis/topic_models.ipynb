{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/3147567/.ivy2/cache\n",
      "The jars for the packages stored in: /home/3147567/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b904c7fb-b336-4e0e-9b95-5310b787e05c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.1.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 288ms :: artifacts dl 83ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b904c7fb-b336-4e0e-9b95-5310b787e05c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/10ms)\n",
      "22/09/03 14:23:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"32G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.1.0\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "NeutralFile = spark.read.parquet(\"../../Files/Submissions/score/done/Neutr_vac.parquet\")\n",
    "ProFile = spark.read.parquet(\"../../Files/Submissions/score/done/Pro_vac.parquet\")\n",
    "AntiFile = spark.read.parquet(\"../../Files/Submissions/score/done/Anti_vac.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ProFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total = unionAll([NeutralFile, ProFile, AntiFile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Total.sampleBy(\"pred_1\", fractions={\n",
    "    0.0: 0.10,\n",
    "    1.0: 0.10,\n",
    "    2.0: 0.10\n",
    "}, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_p = ProFile.sample(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1057838"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_n.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base.document_assembler import DocumentAssembler\n",
    "from sparknlp.base.finisher import Finisher\n",
    "from sparknlp.annotator.stop_words_cleaner import StopWordsCleaner\n",
    "from sparknlp.annotator.normalizer import Normalizer\n",
    "from sparknlp.annotator.token import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove stopwords\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"cleanText\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"disabled\")\n",
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "# clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"cleanText\") \\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"cleanText\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        # document_assembler,\n",
    "        #     tokenizer,\n",
    "        #     normalizer,\n",
    "            # stopwords_cleaner,  \n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the pipeline\n",
    "nlp_model = nlp_pipeline.fit(sample_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pipeline to transform dataframe.\n",
    "processed_df  = nlp_model.transform(sample_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558582"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_df = processed_df.select('pred_1','cleanText')\n",
    "tokens_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"cleanText\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
    "# train the model\n",
    "cv_model = cv.fit(tokens_df)\n",
    "# transform the data. Output column name will be features.\n",
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 495:====================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -25437380.973684043\n",
      "The upper bound on perplexity: 5.620890868953256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 7\n",
    "lda = LDA(k=num_topics, maxIter=10)\n",
    "model4 = lda.fit(vectorized_tokens)\n",
    "ll = model4.logLikelihood(vectorized_tokens)\n",
    "lp = model4.logPerplexity(vectorized_tokens)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "not\n",
      "people\n",
      "like\n",
      "think\n",
      "bad\n",
      "thing\n",
      "time\n",
      "s\n",
      "life\n",
      "want\n",
      "know\n",
      "say\n",
      "m\n",
      "good\n",
      "feel\n",
      "well\n",
      "work\n",
      "way\n",
      "person\n",
      "need\n",
      "opinion\n",
      "kid\n",
      "child\n",
      "make\n",
      "world\n",
      "live\n",
      "ve\n",
      "talk\n",
      "go\n",
      "understand\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "[NUM]\n",
      "vaccine\n",
      "covid\n",
      "test\n",
      "covid[NUM]\n",
      "[num]\n",
      "coronavirus\n",
      "delete\n",
      "dose\n",
      "day\n",
      "covid[num]\n",
      "gt\n",
      "vaccinate\n",
      "week\n",
      "vaccination\n",
      "study\n",
      "case\n",
      "report\n",
      "virus\n",
      "get\n",
      "positive\n",
      "testing\n",
      "pfizer\n",
      "pillar\n",
      "month\n",
      "datum\n",
      "uk\n",
      "say\n",
      "new\n",
      "death\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "[NUM]\n",
      "not\n",
      "year\n",
      "like\n",
      "m\n",
      "people\n",
      "[num]\n",
      "good\n",
      "know\n",
      "time\n",
      "think\n",
      "company\n",
      "man\n",
      "s\n",
      "woman\n",
      "need\n",
      "stock\n",
      "look\n",
      "go\n",
      "high\n",
      "day\n",
      "pay\n",
      "feel\n",
      "remove\n",
      "market\n",
      "work\n",
      "long\n",
      "money\n",
      "ve\n",
      "job\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "remove\n",
      "[url]\n",
      "help\n",
      "post\n",
      "stock\n",
      "company\n",
      "short\n",
      "market\n",
      "look\n",
      "[NUM]\n",
      "thank\n",
      "ampx[num]b\n",
      "news\n",
      "need\n",
      "money\n",
      "good\n",
      "go\n",
      "china\n",
      "like\n",
      "great\n",
      "[num]\n",
      "retard\n",
      "dd\n",
      "people\n",
      "price\n",
      "[num]k\n",
      "see\n",
      "not\n",
      "wsb\n",
      "new\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "not\n",
      "m\n",
      "like\n",
      "shit\n",
      "go\n",
      "know\n",
      "time\n",
      "s\n",
      "want\n",
      "get\n",
      "game\n",
      "come\n",
      "good\n",
      "new\n",
      "people\n",
      "look\n",
      "ve\n",
      "try\n",
      "think\n",
      "feel\n",
      "play\n",
      "video\n",
      "right\n",
      "find\n",
      "tell\n",
      "work\n",
      "thing\n",
      "watch\n",
      "guy\n",
      "day\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "remove\n",
      "good\n",
      "gme\n",
      "robinhood\n",
      "[NUM]\n",
      "trump\n",
      "not\n",
      "s\n",
      "go\n",
      "hold\n",
      "get\n",
      "biden\n",
      "new\n",
      "question\n",
      "reddit\n",
      "stock\n",
      "think\n",
      "message\n",
      "m\n",
      "well\n",
      "meme\n",
      "like\n",
      "happen\n",
      "short\n",
      "call\n",
      "know\n",
      "buy\n",
      "test\n",
      "vote\n",
      "fuck\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "buy\n",
      "gme\n",
      "[NUM]\n",
      "amc\n",
      "hold\n",
      "share\n",
      "nan\n",
      "stock\n",
      "sell\n",
      "moon\n",
      "not\n",
      "_\n",
      "let\n",
      "price\n",
      "today\n",
      "day\n",
      "bb\n",
      "nok\n",
      "trading\n",
      "short\n",
      "go\n",
      "robinhood\n",
      "squeeze\n",
      "ape\n",
      "fund\n",
      "option\n",
      "delete\n",
      "hand\n",
      "dip\n",
      "time\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "# extract vocabulary from CountVectorizer\n",
    "vocab = cv_model.vocabulary\n",
    "topics = model4.describeTopics(maxTermsPerTopic = 30)   \n",
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../Files/models/topic_p_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('../../Files/models/topic_a_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('../../Files/models/topic_p_n.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('reddit_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7913a71c8e34b61ec22a3bd55dc7c3ae241fdb6cefe9aa5fb754c5121c0e8c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
