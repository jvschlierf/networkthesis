{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/3147567/.ivy2/cache\n",
      "The jars for the packages stored in: /home/3147567/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ea6c0acf-bf57-4082-9a37-cc1afbd7059c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.1.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 311ms :: artifacts dl 97ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ea6c0acf-bf57-4082-9a37-cc1afbd7059c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/11ms)\n",
      "22/10/23 16:02:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"32G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.1.0\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeutralFile = spark.read.parquet(\"../../Files/Submissions/score/done/Neutr_vacc.parquet\")\n",
    "ProFile = spark.read.parquet(\"../../Files/Submissions/score/done/Pro_vacc.parquet\")\n",
    "AntiFile = spark.read.parquet(\"../../Files/Submissions/score/done/Anti_vacc.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196185"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2982155"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Total.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total = unionAll([NeutralFile, ProFile, AntiFile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Total.sampleBy(\"pred_1\", fractions={\n",
    "    0.0: 0.10,\n",
    "    1.0: 0.10,\n",
    "    2.0: 0.10\n",
    "}, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_p = ProFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_n = NeutralFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = AntiFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421328"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_a.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base.document_assembler import DocumentAssembler\n",
    "from sparknlp.base.finisher import Finisher\n",
    "from sparknlp.annotator.stop_words_cleaner import StopWordsCleaner\n",
    "from sparknlp.annotator.normalizer import Normalizer\n",
    "from sparknlp.annotator.token import Tokenizer\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove stopwords\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"cleanText\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"disabled\")\n",
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "# clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\") \\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner,  \n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the pipeline\n",
    "nlp_model = nlp_pipeline.fit(sample_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pipeline to transform dataframe.\n",
    "processed_df  = nlp_model.transform(sample_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421328"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_df = processed_df.select('class_II','tokens')\n",
    "tokens_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
    "# train the model\n",
    "cv_model = cv.fit(tokens_df)\n",
    "# transform the data. Output column name will be features.\n",
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_range = [2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ''\n",
    "results += \"tyhata\"\n",
    "results += \"\\n\"\n",
    "results += \"eesdfse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tyhata\n",
      "eesdfse\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "ANTI VACCINES, NUMBER OF TOPICS: 2*************************\n",
      "The lower bound on the log likelihood of the entire corpus: -44790377.34381053\n",
      "The upper bound on perplexity: 5.791347903057161\n",
      "\n",
      "topic: 0\n",
      "*************************\n",
      "NUM\n",
      "vaccine\n",
      "people\n",
      "covid\n",
      "m\n",
      "get\n",
      "like\n",
      "think\n",
      "know\n",
      "death\n",
      "year\n",
      "time\n",
      "go\n",
      "work\n",
      "die\n",
      "vaccinate\n",
      "want\n",
      "virus\n",
      "url\n",
      "say\n",
      "test\n",
      "life\n",
      "day\n",
      "tell\n",
      "thing\n",
      "good\n",
      "come\n",
      "ve\n",
      "world\n",
      "try\n",
      "topic: 1\n",
      "*************************\n",
      "people\n",
      "like\n",
      "m\n",
      "know\n",
      "think\n",
      "want\n",
      "go\n",
      "say\n",
      "time\n",
      "thing\n",
      "NUM\n",
      "need\n",
      "feel\n",
      "way\n",
      "work\n",
      "get\n",
      "good\n",
      "right\n",
      "ve\n",
      "look\n",
      "life\n",
      "bad\n",
      "post\n",
      "remove\n",
      "man\n",
      "come\n",
      "mask\n",
      "tell\n",
      "make\n",
      "fuck\n",
      "*************************\n",
      "ANTI VACCINES, NUMBER OF TOPICS: 3*************************\n",
      "The lower bound on the log likelihood of the entire corpus: -45001099.28986275\n",
      "The upper bound on perplexity: 5.818594025388976\n",
      "\n",
      "topic: 0\n",
      "*************************\n",
      "vaccine\n",
      "NUM\n",
      "m\n",
      "covid\n",
      "people\n",
      "get\n",
      "know\n",
      "like\n",
      "work\n",
      "go\n",
      "time\n",
      "want\n",
      "year\n",
      "think\n",
      "tell\n",
      "say\n",
      "ve\n",
      "day\n",
      "feel\n",
      "vaccinate\n",
      "try\n",
      "come\n",
      "take\n",
      "life\n",
      "thing\n",
      "start\n",
      "good\n",
      "job\n",
      "die\n",
      "talk\n",
      "topic: 1\n",
      "*************************\n",
      "like\n",
      "people\n",
      "m\n",
      "know\n",
      "say\n",
      "go\n",
      "want\n",
      "time\n",
      "man\n",
      "feel\n",
      "think\n",
      "NUM\n",
      "look\n",
      "thing\n",
      "way\n",
      "need\n",
      "woman\n",
      "hate\n",
      "good\n",
      "get\n",
      "right\n",
      "remove\n",
      "shit\n",
      "ve\n",
      "kid\n",
      "life\n",
      "delete\n",
      "bad\n",
      "make\n",
      "come\n",
      "topic: 2\n",
      "*************************\n",
      "people\n",
      "NUM\n",
      "vaccine\n",
      "think\n",
      "death\n",
      "like\n",
      "mask\n",
      "covid\n",
      "virus\n",
      "know\n",
      "thing\n",
      "world\n",
      "url\n",
      "want\n",
      "die\n",
      "test\n",
      "government\n",
      "life\n",
      "vaccination\n",
      "need\n",
      "go\n",
      "cause\n",
      "vaccinate\n",
      "good\n",
      "num\n",
      "way\n",
      "health\n",
      "right\n",
      "time\n",
      "post\n",
      "*************************\n",
      "ANTI VACCINES, NUMBER OF TOPICS: 4*************************\n",
      "The lower bound on the log likelihood of the entire corpus: -44629447.394887425\n",
      "The upper bound on perplexity: 5.770539832719176\n",
      "\n",
      "topic: 0\n",
      "*************************\n",
      "m\n",
      "NUM\n",
      "like\n",
      "people\n",
      "know\n",
      "time\n",
      "go\n",
      "work\n",
      "get\n",
      "want\n",
      "think\n",
      "feel\n",
      "ve\n",
      "tell\n",
      "year\n",
      "vaccine\n",
      "thing\n",
      "life\n",
      "good\n",
      "day\n",
      "say\n",
      "try\n",
      "come\n",
      "job\n",
      "start\n",
      "talk\n",
      "covid\n",
      "way\n",
      "find\n",
      "need\n",
      "topic: 1\n",
      "*************************\n",
      "like\n",
      "people\n",
      "man\n",
      "woman\n",
      "say\n",
      "know\n",
      "kid\n",
      "NUM\n",
      "go\n",
      "m\n",
      "want\n",
      "shoot\n",
      "hate\n",
      "shit\n",
      "time\n",
      "right\n",
      "need\n",
      "game\n",
      "look\n",
      "way\n",
      "kill\n",
      "feel\n",
      "dog\n",
      "school\n",
      "think\n",
      "police\n",
      "food\n",
      "eat\n",
      "black\n",
      "make\n",
      "topic: 2\n",
      "*************************\n",
      "people\n",
      "think\n",
      "like\n",
      "mask\n",
      "NUM\n",
      "thing\n",
      "want\n",
      "know\n",
      "world\n",
      "go\n",
      "life\n",
      "vaccine\n",
      "right\n",
      "need\n",
      "good\n",
      "wear\n",
      "government\n",
      "way\n",
      "covid\n",
      "post\n",
      "death\n",
      "bad\n",
      "believe\n",
      "time\n",
      "m\n",
      "die\n",
      "medium\n",
      "fuck\n",
      "virus\n",
      "social\n",
      "topic: 3\n",
      "*************************\n",
      "vaccine\n",
      "NUM\n",
      "covid\n",
      "people\n",
      "test\n",
      "death\n",
      "url\n",
      "vaccinate\n",
      "remove\n",
      "get\n",
      "virus\n",
      "vaccination\n",
      "say\n",
      "die\n",
      "shot\n",
      "cause\n",
      "covidNUM\n",
      "num\n",
      "pfizer\n",
      "know\n",
      "case\n",
      "day\n",
      "like\n",
      "effect\n",
      "new\n",
      "report\n",
      "delete\n",
      "pillar\n",
      "dose\n",
      "covidnum\n",
      "*************************\n",
      "ANTI VACCINES, NUMBER OF TOPICS: 5*************************\n",
      "The lower bound on the log likelihood of the entire corpus: -44576744.11371156\n",
      "The upper bound on perplexity: 5.763725354810691\n",
      "\n",
      "topic: 0\n",
      "*************************\n",
      "m\n",
      "like\n",
      "NUM\n",
      "people\n",
      "know\n",
      "time\n",
      "go\n",
      "work\n",
      "want\n",
      "get\n",
      "think\n",
      "feel\n",
      "ve\n",
      "tell\n",
      "thing\n",
      "life\n",
      "year\n",
      "good\n",
      "say\n",
      "try\n",
      "day\n",
      "come\n",
      "job\n",
      "vaccine\n",
      "start\n",
      "talk\n",
      "way\n",
      "need\n",
      "find\n",
      "bad\n",
      "topic: 1\n",
      "*************************\n",
      "like\n",
      "man\n",
      "people\n",
      "woman\n",
      "say\n",
      "kid\n",
      "know\n",
      "shoot\n",
      "go\n",
      "game\n",
      "dog\n",
      "NUM\n",
      "time\n",
      "kill\n",
      "right\n",
      "want\n",
      "m\n",
      "shit\n",
      "need\n",
      "police\n",
      "look\n",
      "way\n",
      "hate\n",
      "food\n",
      "school\n",
      "black\n",
      "eat\n",
      "feel\n",
      "white\n",
      "get\n",
      "topic: 2\n",
      "*************************\n",
      "people\n",
      "think\n",
      "like\n",
      "mask\n",
      "NUM\n",
      "thing\n",
      "want\n",
      "know\n",
      "world\n",
      "life\n",
      "right\n",
      "go\n",
      "need\n",
      "good\n",
      "government\n",
      "way\n",
      "wear\n",
      "vaccine\n",
      "covid\n",
      "post\n",
      "believe\n",
      "time\n",
      "bad\n",
      "medium\n",
      "die\n",
      "fuck\n",
      "m\n",
      "social\n",
      "come\n",
      "virus\n",
      "topic: 3\n",
      "*************************\n",
      "vaccine\n",
      "covid\n",
      "people\n",
      "NUM\n",
      "get\n",
      "url\n",
      "say\n",
      "shot\n",
      "vaccinate\n",
      "virus\n",
      "cause\n",
      "die\n",
      "pfizer\n",
      "know\n",
      "like\n",
      "remove\n",
      "delete\n",
      "effect\n",
      "new\n",
      "doctor\n",
      "mandate\n",
      "vax\n",
      "take\n",
      "go\n",
      "m\n",
      "unvaccinated\n",
      "death\n",
      "work\n",
      "covidnum\n",
      "covidNUM\n",
      "topic: 4\n",
      "*************************\n",
      "NUM\n",
      "test\n",
      "death\n",
      "num\n",
      "vaccination\n",
      "remove\n",
      "vaccinate\n",
      "vaccine\n",
      "gt\n",
      "pillar\n",
      "testing\n",
      "day\n",
      "case\n",
      "people\n",
      "covid\n",
      "report\n",
      "positive\n",
      "covidNUM\n",
      "dose\n",
      "datum\n",
      "week\n",
      "url\n",
      "mask\n",
      "virus\n",
      "year\n",
      "number\n",
      "spread\n",
      "need\n",
      "study\n",
      "m\n",
      "*************************\n",
      "ANTI VACCINES, NUMBER OF TOPICS: 6*************************\n",
      "The lower bound on the log likelihood of the entire corpus: -44586722.27237547\n",
      "The upper bound on perplexity: 5.765015520057816\n",
      "\n",
      "topic: 0\n",
      "*************************\n",
      "m\n",
      "like\n",
      "NUM\n",
      "people\n",
      "time\n",
      "know\n",
      "work\n",
      "go\n",
      "feel\n",
      "want\n",
      "think\n",
      "ve\n",
      "get\n",
      "thing\n",
      "life\n",
      "good\n",
      "tell\n",
      "year\n",
      "say\n",
      "try\n",
      "day\n",
      "come\n",
      "job\n",
      "way\n",
      "start\n",
      "talk\n",
      "need\n",
      "find\n",
      "bad\n",
      "ask\n",
      "topic: 1\n",
      "*************************\n",
      "man\n",
      "woman\n",
      "like\n",
      "people\n",
      "kid\n",
      "say\n",
      "shoot\n",
      "kill\n",
      "game\n",
      "shit\n",
      "dog\n",
      "go\n",
      "police\n",
      "NUM\n",
      "black\n",
      "right\n",
      "school\n",
      "time\n",
      "want\n",
      "ban\n",
      "white\n",
      "food\n",
      "know\n",
      "m\n",
      "need\n",
      "remove\n",
      "sex\n",
      "word\n",
      "comment\n",
      "make\n",
      "topic: 2\n",
      "*************************\n",
      "people\n",
      "think\n",
      "like\n",
      "mask\n",
      "thing\n",
      "want\n",
      "NUM\n",
      "know\n",
      "world\n",
      "life\n",
      "right\n",
      "wear\n",
      "go\n",
      "good\n",
      "way\n",
      "bad\n",
      "fuck\n",
      "need\n",
      "die\n",
      "government\n",
      "believe\n",
      "time\n",
      "medium\n",
      "post\n",
      "social\n",
      "death\n",
      "m\n",
      "mean\n",
      "make\n",
      "covid\n",
      "topic: 3\n",
      "*************************\n",
      "vaccine\n",
      "people\n",
      "NUM\n",
      "remove\n",
      "covid\n",
      "shot\n",
      "get\n",
      "delete\n",
      "die\n",
      "cause\n",
      "say\n",
      "vax\n",
      "pfizer\n",
      "url\n",
      "doctor\n",
      "take\n",
      "death\n",
      "hate\n",
      "like\n",
      "heart\n",
      "vaccinate\n",
      "thank\n",
      "drug\n",
      "day\n",
      "unvaccinated\n",
      "big\n",
      "look\n",
      "blood\n",
      "jab\n",
      "cancer\n",
      "topic: 4\n",
      "*************************\n",
      "NUM\n",
      "test\n",
      "death\n",
      "num\n",
      "vaccination\n",
      "gt\n",
      "pillar\n",
      "vaccine\n",
      "testing\n",
      "vaccinate\n",
      "case\n",
      "report\n",
      "covidNUM\n",
      "day\n",
      "positive\n",
      "people\n",
      "dose\n",
      "url\n",
      "datum\n",
      "covid\n",
      "remove\n",
      "week\n",
      "virus\n",
      "study\n",
      "number\n",
      "ampxnumb\n",
      "year\n",
      "uk\n",
      "process\n",
      "include\n",
      "topic: 5\n",
      "*************************\n",
      "vaccine\n",
      "covid\n",
      "people\n",
      "NUM\n",
      "know\n",
      "m\n",
      "vaccinate\n",
      "get\n",
      "like\n",
      "virus\n",
      "say\n",
      "go\n",
      "think\n",
      "mandate\n",
      "url\n",
      "want\n",
      "new\n",
      "effect\n",
      "ve\n",
      "government\n",
      "need\n",
      "come\n",
      "work\n",
      "time\n",
      "post\n",
      "conspiracy\n",
      "health\n",
      "year\n",
      "vaccination\n",
      "right\n",
      "*************************\n",
      "ANTI VACCINES, NUMBER OF TOPICS: 7*************************\n",
      "The lower bound on the log likelihood of the entire corpus: -44619422.914905086\n",
      "The upper bound on perplexity: 5.769243678175101\n",
      "\n",
      "topic: 0\n",
      "*************************\n",
      "m\n",
      "like\n",
      "NUM\n",
      "people\n",
      "time\n",
      "know\n",
      "work\n",
      "feel\n",
      "go\n",
      "want\n",
      "think\n",
      "life\n",
      "ve\n",
      "thing\n",
      "good\n",
      "get\n",
      "year\n",
      "tell\n",
      "try\n",
      "say\n",
      "job\n",
      "day\n",
      "way\n",
      "come\n",
      "start\n",
      "talk\n",
      "need\n",
      "bad\n",
      "find\n",
      "take\n",
      "topic: 1\n",
      "*************************\n",
      "man\n",
      "woman\n",
      "kid\n",
      "shoot\n",
      "like\n",
      "people\n",
      "say\n",
      "dog\n",
      "police\n",
      "kill\n",
      "black\n",
      "school\n",
      "shit\n",
      "NUM\n",
      "right\n",
      "go\n",
      "white\n",
      "time\n",
      "want\n",
      "food\n",
      "sex\n",
      "ban\n",
      "know\n",
      "word\n",
      "need\n",
      "m\n",
      "child\n",
      "get\n",
      "pain\n",
      "life\n",
      "topic: 2\n",
      "*************************\n",
      "people\n",
      "think\n",
      "like\n",
      "mask\n",
      "thing\n",
      "want\n",
      "world\n",
      "know\n",
      "NUM\n",
      "life\n",
      "right\n",
      "die\n",
      "bad\n",
      "good\n",
      "way\n",
      "government\n",
      "need\n",
      "go\n",
      "fuck\n",
      "wear\n",
      "medium\n",
      "believe\n",
      "social\n",
      "time\n",
      "death\n",
      "money\n",
      "make\n",
      "human\n",
      "mean\n",
      "care\n",
      "topic: 3\n",
      "*************************\n",
      "vaccine\n",
      "remove\n",
      "people\n",
      "delete\n",
      "NUM\n",
      "shot\n",
      "covid\n",
      "get\n",
      "die\n",
      "hate\n",
      "say\n",
      "vax\n",
      "cause\n",
      "doctor\n",
      "like\n",
      "pfizer\n",
      "take\n",
      "thank\n",
      "death\n",
      "heart\n",
      "look\n",
      "cancer\n",
      "big\n",
      "url\n",
      "vaccinate\n",
      "unvaccinated\n",
      "blood\n",
      "tell\n",
      "good\n",
      "go\n",
      "topic: 4\n",
      "*************************\n",
      "NUM\n",
      "post\n",
      "mask\n",
      "game\n",
      "m\n",
      "death\n",
      "day\n",
      "trump\n",
      "go\n",
      "sub\n",
      "stop\n",
      "like\n",
      "ampxnumb\n",
      "people\n",
      "say\n",
      "know\n",
      "comment\n",
      "think\n",
      "come\n",
      "wear\n",
      "url\n",
      "fake\n",
      "want\n",
      "time\n",
      "look\n",
      "tell\n",
      "get\n",
      "work\n",
      "play\n",
      "news\n",
      "topic: 5\n",
      "*************************\n",
      "vaccine\n",
      "covid\n",
      "m\n",
      "know\n",
      "people\n",
      "get\n",
      "like\n",
      "say\n",
      "vaccinate\n",
      "go\n",
      "NUM\n",
      "think\n",
      "want\n",
      "mandate\n",
      "ve\n",
      "conspiracy\n",
      "come\n",
      "government\n",
      "effect\n",
      "new\n",
      "need\n",
      "thing\n",
      "work\n",
      "post\n",
      "tell\n",
      "right\n",
      "amp\n",
      "virus\n",
      "year\n",
      "time\n",
      "topic: 6\n",
      "*************************\n",
      "NUM\n",
      "vaccine\n",
      "test\n",
      "covid\n",
      "people\n",
      "death\n",
      "virus\n",
      "vaccinate\n",
      "url\n",
      "vaccination\n",
      "num\n",
      "covidNUM\n",
      "case\n",
      "pillar\n",
      "testing\n",
      "report\n",
      "health\n",
      "gt\n",
      "die\n",
      "datum\n",
      "positive\n",
      "year\n",
      "dose\n",
      "study\n",
      "cause\n",
      "day\n",
      "disease\n",
      "covidnum\n",
      "risk\n",
      "time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 752:===================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/3147567/Thesis/github/analysis/topic_models.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/github/analysis/topic_models.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model4 \u001b[39m=\u001b[39m lda\u001b[39m.\u001b[39mfit(vectorized_tokens)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/github/analysis/topic_models.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ll \u001b[39m=\u001b[39m model4\u001b[39m.\u001b[39mlogLikelihood(vectorized_tokens)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/github/analysis/topic_models.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m lp \u001b[39m=\u001b[39m model4\u001b[39m.\u001b[39;49mlogPerplexity(vectorized_tokens)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/github/analysis/topic_models.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m results \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m25\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/github/analysis/topic_models.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m results \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/ml/clustering.py:1239\u001b[0m, in \u001b[0;36mLDAModel.logPerplexity\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogPerplexity\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[1;32m   1231\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[39m    Calculate an upper bound on perplexity.  (Lower is better.)\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[39m    See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[39m        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_java(\u001b[39m\"\u001b[39;49m\u001b[39mlogPerplexity\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataset)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/ml/wrapper.py:54\u001b[0m, in \u001b[0;36mJavaWrapper._call_java\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     52\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[1;32m     53\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, m(\u001b[39m*\u001b[39;49mjava_args))\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/py4j/java_gateway.py:1308\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1303\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1308\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1309\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1310\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1312\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/py4j/java_gateway.py:1205\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError while sending\u001b[39m\u001b[39m\"\u001b[39m, e, proto\u001b[39m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m   1206\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m   1207\u001b[0m     \u001b[39mif\u001b[39;00m answer\u001b[39m.\u001b[39mstartswith(proto\u001b[39m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = ''\n",
    "for topic_n in topic_range:\n",
    "    num_topics = topic_n\n",
    "    lda = LDA(k=num_topics, maxIter=10)\n",
    "    model4 = lda.fit(vectorized_tokens)\n",
    "    ll = model4.logLikelihood(vectorized_tokens)\n",
    "    lp = model4.logPerplexity(vectorized_tokens)\n",
    "    results += \"*\"*25\n",
    "    results += \"\\n\"\n",
    "    results += f\"ANTI VACCINES, NUMBER OF TOPICS: {num_topics}\"\n",
    "    results += \"*\"*25\n",
    "    results += \"\\n\"\n",
    "    results += (\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "    results += \"\\n\"\n",
    "    results +=(\"The upper bound on perplexity: \" + str(lp))\n",
    "    results += \"\\n\"\n",
    "    results += \"\\n\"\n",
    "    vocab = cv_model.vocabulary\n",
    "    topics = model4.describeTopics(maxTermsPerTopic = 30)   \n",
    "    topics_rdd = topics.rdd\n",
    "    topics_words = topics_rdd\\\n",
    "        .map(lambda row: row['termIndices'])\\\n",
    "        .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "        .collect()\n",
    "    for idx, topic in enumerate(topics_words):\n",
    "        results += (\"topic: {}\".format(idx))\n",
    "        results += \"\\n\"\n",
    "        results += (\"*\"*25)\n",
    "        results += \"\\n\"\n",
    "        for word in topic:\n",
    "            results += word\n",
    "            results += \"\\n\"\n",
    "            # results += (\"*\"*25)\n",
    "            # results += \"\\n\"\n",
    "#write results to file\n",
    "with open(\"../../Files/models/topic_an.txt\", \"w\") as output:\n",
    "    output.write(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../Files/models/topic_an.txt\", \"w\") as output:\n",
    "    output.write(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "vaccine\n",
      "covidnum\n",
      "NUM\n",
      "covid\n",
      "remove\n",
      "booster\n",
      "pfizer\n",
      "say\n",
      "vaccination\n",
      "million\n",
      "dose\n",
      "vaccinate\n",
      "shot\n",
      "coronavirus\n",
      "biden\n",
      "moderna\n",
      "get\n",
      "antivaxxer\n",
      "jab\n",
      "delete\n",
      "uk\n",
      "covidNUM\n",
      "eu\n",
      "trump\n",
      "fda\n",
      "americans\n",
      "mandatory\n",
      "india\n",
      "plan\n",
      "company\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "vaccine\n",
      "covidNUM\n",
      "vaccination\n",
      "world\n",
      "information\n",
      "health\n",
      "NUM\n",
      "yes\n",
      "join\n",
      "global\n",
      "canada\n",
      "daily\n",
      "passport\n",
      "rule\n",
      "num\n",
      "ask\n",
      "question\n",
      "case\n",
      "ampnbsp\n",
      "discussion\n",
      "cdc\n",
      "appointment\n",
      "find\n",
      "united\n",
      "report\n",
      "answer\n",
      "texas\n",
      "concern\n",
      "approve\n",
      "proof\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "NUM\n",
      "day\n",
      "feel\n",
      "m\n",
      "like\n",
      "get\n",
      "go\n",
      "vaccine\n",
      "time\n",
      "work\n",
      "shot\n",
      "week\n",
      "experience\n",
      "ve\n",
      "know\n",
      "take\n",
      "hour\n",
      "dose\n",
      "think\n",
      "num\n",
      "bad\n",
      "symptom\n",
      "start\n",
      "want\n",
      "help\n",
      "pain\n",
      "second\n",
      "good\n",
      "well\n",
      "thank\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "NUM\n",
      "vaccine\n",
      "new\n",
      "year\n",
      "covid\n",
      "dose\n",
      "love\n",
      "great\n",
      "cute\n",
      "get\n",
      "amazing\n",
      "beautiful\n",
      "m\n",
      "vaccinate\n",
      "old\n",
      "immunity\n",
      "start\n",
      "day\n",
      "say\n",
      "shot\n",
      "time\n",
      "people\n",
      "pfizer\n",
      "free\n",
      "month\n",
      "school\n",
      "week\n",
      "york\n",
      "coronavirus\n",
      "gift\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "vaccinate\n",
      "good\n",
      "vaccine\n",
      "unvaccinated\n",
      "want\n",
      "covid\n",
      "say\n",
      "people\n",
      "travel\n",
      "smile\n",
      "time\n",
      "worker\n",
      "look\n",
      "country\n",
      "new\n",
      "hope\n",
      "make\n",
      "fully\n",
      "china\n",
      "hospital\n",
      "guy\n",
      "come\n",
      "require\n",
      "get\n",
      "antivax\n",
      "know\n",
      "enjoy\n",
      "support\n",
      "fauci\n",
      "help\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "dose\n",
      "get\n",
      "NUM\n",
      "vaccine\n",
      "feel\n",
      "second\n",
      "pfizer\n",
      "arm\n",
      "shot\n",
      "day\n",
      "effect\n",
      "m\n",
      "sore\n",
      "hour\n",
      "covid\n",
      "moderna\n",
      "experience\n",
      "today\n",
      "go\n",
      "shoot\n",
      "pain\n",
      "like\n",
      "NUMnd\n",
      "bad\n",
      "receive\n",
      "week\n",
      "covidNUM\n",
      "headache\n",
      "fever\n",
      "yesterday\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "vaccine\n",
      "NUM\n",
      "covid\n",
      "covidNUM\n",
      "coronavirus\n",
      "people\n",
      "vaccinate\n",
      "trial\n",
      "vaccination\n",
      "study\n",
      "url\n",
      "dose\n",
      "rate\n",
      "say\n",
      "pfizer\n",
      "effective\n",
      "need\n",
      "johnson\n",
      "variant\n",
      "mandate\n",
      "test\n",
      "uk\n",
      "state\n",
      "case\n",
      "infection\n",
      "shot\n",
      "amp\n",
      "num\n",
      "month\n",
      "find\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# extract vocabulary from CountVectorizer\n",
    "vocab = cv_model.vocabulary\n",
    "topics = model4.describeTopics(maxTermsPerTopic = 30)   \n",
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.save('../../Files/models/topic_a_7.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('../../Files/models/topic_a_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('../../Files/models/topic_p_n.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('reddit_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7913a71c8e34b61ec22a3bd55dc7c3ae241fdb6cefe9aa5fb754c5121c0e8c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
