{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import spacy\n",
    "import numpy as np\n",
    "import sklearn as skl \n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'label'\n",
    "input_column = 'cleanTitle'\n",
    "\n",
    "train_data = pd.read_pickle('../../../Files/Submissions/train/train_split_submission.pickle') \n",
    "valid_data = pd.read_pickle('../../../Files/Submissions/train/val_split_submission.pickle')\n",
    "test_data = pd.read_pickle('../../../Files/Submissions/train/test_split_submission.pickle')\n",
    "\n",
    "train_data = train_data[[target, input_column]]\n",
    "valid_data = valid_data[[target, input_column]]\n",
    "test_data = test_data[[target, input_column]]\n",
    "\n",
    "data = pd.concat([train_data, valid_data, test_data])\n",
    "\n",
    "\n",
    "train_instances = train_data[input_column].apply(str).apply(str.split)\n",
    "train_labels = train_data[target]\n",
    "\n",
    "# collect known word tokens and tags\n",
    "wordset, labelset = set(), set()\n",
    "\n",
    "# collect tags from all data, to prevent unseen labels\n",
    "labelset.update(set(data[target]))\n",
    "\n",
    "# get the vocabulary\n",
    "for words in train_instances:\n",
    "    wordset.update(set(words))\n",
    "\n",
    "# map words and tags into ints\n",
    "PAD = '-PAD-'\n",
    "UNK = '-UNK-'\n",
    "word2int = {word: i + 2 for i, word in enumerate(sorted(wordset))}\n",
    "word2int[PAD] = 0  # special token for padding\n",
    "word2int[UNK] = 1  # special token for unknown words\n",
    " \n",
    "label2int = {label: i for i, label in enumerate(sorted(labelset))}\n",
    "# inverted index to translate it back\n",
    "int2label = {i:label for label, i in label2int.items()}\n",
    "\n",
    "\n",
    "def convert2ints(instances):\n",
    "    \"\"\"\n",
    "    function to apply the mapping to all words\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for words in instances:\n",
    "        # replace words with int, 1 for unknown words\n",
    "        word_ints = [word2int.get(word, 1) for word in words]\n",
    "        result.append(word_ints)\n",
    "    return result\n",
    "                          \n",
    "train_instances_int = convert2ints(train_instances)\n",
    "train_labels_int = [label2int[label] for label in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = test_data[input_column].apply(str).apply(str.split)\n",
    "test_labels = test_data[target]\n",
    "\n",
    "test_instances_int = convert2ints(test_instances)\n",
    "test_labels_int = [label2int[label] for label in test_labels]\n",
    "\n",
    "# convert dev data\n",
    "val_instances = valid_data[input_column].apply(str).apply(str.split)\n",
    "val_labels = valid_data[target]\n",
    "\n",
    "val_instances_int = convert2ints(val_instances)\n",
    "val_labels_int = [label2int[label] for label in val_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_1hot = to_categorical(train_labels_int, len(label2int))\n",
    "test_labels_1hot = to_categorical(test_labels_int, len(label2int))\n",
    "val_labels_1hot = to_categorical(val_labels_int, len(label2int))\n",
    "\n",
    "train_labels_1hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "['shill', 'organization'] 2\n",
      "[22655 17709     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0] 15\n"
     ]
    }
   ],
   "source": [
    "# compute 95th percentile of training sentence lengths\n",
    "L = sorted(map(len, train_instances))\n",
    "MAX_LENGTH = L[int(len(L)*0.95)]\n",
    "print(MAX_LENGTH)\n",
    "\n",
    "# apply padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_instances_int = pad_sequences(train_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
    "test_instances_int = pad_sequences(test_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
    "val_instances_int = pad_sequences(val_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
    "\n",
    "print(train_instances[0], len(train_instances[0]))\n",
    "print(train_instances_int[0], len(train_instances_int[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_instances_int)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_instances_int, label=train_labels_int)\n",
    "dval = xgb.DMatrix(val_instances_int, label=val_labels_int)\n",
    "dtest = xgb.DMatrix(test_instances_int, label=test_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 8, 'eta': 1, 'objective': 'multi:softmax', 'num_class': 3, 'eval_metric': ['auc', 'ams@0'], 'nthread': 4, 'silent': 1}\n",
    "\n",
    "evallist = [(dval, 'eval'), (dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:44:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:44:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:44:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:44:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:44:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_round = 500\n",
    "seed = 42\n",
    "cv_results = xgb.cv(\n",
    "    param, \n",
    "    dtrain, \n",
    "    num_boost_round=num_round,\n",
    "    seed=seed, \n",
    "    nfold=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>train-ams@0-mean</th>\n",
       "      <th>train-ams@0-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "      <th>test-ams@0-mean</th>\n",
       "      <th>test-ams@0-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.759845</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>218.330054</td>\n",
       "      <td>0.676217</td>\n",
       "      <td>0.733313</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>109.108588</td>\n",
       "      <td>1.326160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.815736</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>218.333337</td>\n",
       "      <td>0.671118</td>\n",
       "      <td>0.783347</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>109.116321</td>\n",
       "      <td>1.333643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.850129</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>218.337195</td>\n",
       "      <td>0.673382</td>\n",
       "      <td>0.812958</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>109.114368</td>\n",
       "      <td>1.335949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.871339</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>218.337015</td>\n",
       "      <td>0.673368</td>\n",
       "      <td>0.829098</td>\n",
       "      <td>0.005645</td>\n",
       "      <td>109.124072</td>\n",
       "      <td>1.341030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.887703</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>218.336182</td>\n",
       "      <td>0.669097</td>\n",
       "      <td>0.841703</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>109.114505</td>\n",
       "      <td>1.333645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.999699</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>218.330725</td>\n",
       "      <td>0.675186</td>\n",
       "      <td>0.918024</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>218.329904</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>0.918009</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.999701</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>218.329904</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>0.918017</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>218.329904</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>218.329904</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>0.918010</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-auc-mean  train-auc-std  train-ams@0-mean  train-ams@0-std  \\\n",
       "0          0.759845       0.001814        218.330054         0.676217   \n",
       "1          0.815736       0.002401        218.333337         0.671118   \n",
       "2          0.850129       0.001867        218.337195         0.673382   \n",
       "3          0.871339       0.004394        218.337015         0.673368   \n",
       "4          0.887703       0.005562        218.336182         0.669097   \n",
       "..              ...            ...               ...              ...   \n",
       "495        0.999699       0.000006        218.330725         0.675186   \n",
       "496        0.999700       0.000006        218.329904         0.675854   \n",
       "497        0.999701       0.000006        218.329904         0.675854   \n",
       "498        0.999702       0.000007        218.329904         0.675854   \n",
       "499        0.999702       0.000007        218.329904         0.675854   \n",
       "\n",
       "     test-auc-mean  test-auc-std  test-ams@0-mean  test-ams@0-std  \n",
       "0         0.733313      0.004254       109.108588        1.326160  \n",
       "1         0.783347      0.004543       109.116321        1.333643  \n",
       "2         0.812958      0.003231       109.114368        1.335949  \n",
       "3         0.829098      0.005645       109.124072        1.341030  \n",
       "4         0.841703      0.007221       109.114505        1.333645  \n",
       "..             ...           ...              ...             ...  \n",
       "495       0.918024      0.001223       109.115831        1.328617  \n",
       "496       0.918009      0.001208       109.115831        1.328617  \n",
       "497       0.918017      0.001216       109.115831        1.328617  \n",
       "498       0.918000      0.001231       109.115831        1.328617  \n",
       "499       0.918010      0.001227       109.115831        1.328617  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('reddit_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7913a71c8e34b61ec22a3bd55dc7c3ae241fdb6cefe9aa5fb754c5121c0e8c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
