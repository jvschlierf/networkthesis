{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import spacy\n",
    "import numpy as np\n",
    "import sklearn as skl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'label'\n",
    "input_column = 'cleanTitle'\n",
    "\n",
    "train_data = pd.read_pickle('../../../Files/Submissions/train/train_split_submission.pickle') \n",
    "valid_data = pd.read_pickle('../../../Files/Submissions/train/val_split_submission.pickle')\n",
    "test_data = pd.read_pickle('../../../Files/Submissions/train/test_split_submission.pickle')\n",
    "\n",
    "train_data = train_data[[target, input_column]]\n",
    "valid_data = valid_data[[target, input_column]]\n",
    "test_data = test_data[[target, input_column]]\n",
    "\n",
    "data = pd.concat([train_data, valid_data, test_data])\n",
    "\n",
    "\n",
    "train_instances = train_data[input_column].apply(str).apply(str.split)\n",
    "train_labels = train_data[target]\n",
    "\n",
    "# collect known word tokens and tags\n",
    "wordset, labelset = set(), set()\n",
    "\n",
    "# collect tags from all data, to prevent unseen labels\n",
    "labelset.update(set(data[target]))\n",
    "\n",
    "# get the vocabulary\n",
    "for words in train_instances:\n",
    "    wordset.update(set(words))\n",
    "\n",
    "# map words and tags into ints\n",
    "PAD = '-PAD-'\n",
    "UNK = '-UNK-'\n",
    "word2int = {word: i + 2 for i, word in enumerate(sorted(wordset))}\n",
    "word2int[PAD] = 0  # special token for padding\n",
    "word2int[UNK] = 1  # special token for unknown words\n",
    " \n",
    "label2int = {label: i for i, label in enumerate(sorted(labelset))}\n",
    "# inverted index to translate it back\n",
    "int2label = {i:label for label, i in label2int.items()}\n",
    "\n",
    "\n",
    "def convert2ints(instances):\n",
    "    \"\"\"\n",
    "    function to apply the mapping to all words\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for words in instances:\n",
    "        # replace words with int, 1 for unknown words\n",
    "        word_ints = [word2int.get(word, 1) for word in words]\n",
    "        result.append(word_ints)\n",
    "    return result\n",
    "                          \n",
    "train_instances_int = convert2ints(train_instances)\n",
    "train_labels_int = [label2int[label] for label in train_labels]\n",
    "\n",
    "test_instances = test_data[input_column].apply(str).apply(str.split)\n",
    "test_labels = test_data[target]\n",
    "\n",
    "test_instances_int = convert2ints(test_instances)\n",
    "test_labels_int = [label2int[label] for label in test_labels]\n",
    "\n",
    "# convert dev data\n",
    "val_instances = valid_data[input_column].apply(str).apply(str.split)\n",
    "val_labels = valid_data[target]\n",
    "\n",
    "val_instances_int = convert2ints(val_instances)\n",
    "val_labels_int = [label2int[label] for label in val_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute 95th percentile of training sentence lengths\n",
    "L = sorted(map(len, train_instances))\n",
    "MAX_LENGTH = L[int(len(L)*0.95)]\n",
    "\n",
    "# apply padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_instances_int = pad_sequences(train_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
    "test_instances_int = pad_sequences(test_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
    "val_instances_int = pad_sequences(val_instances_int, padding='post', maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_instances_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrf.fit(train_instances_int, train_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrf.classes_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('reddit_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7913a71c8e34b61ec22a3bd55dc7c3ae241fdb6cefe9aa5fb754c5121c0e8c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
