{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import spacy\n",
    "import numpy as np\n",
    "import sklearn as skl \n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'label'\n",
    "input_column = 'cleanTitle'\n",
    "\n",
    "train_data = pd.read_pickle('../../../Files/Submissions/train/train_split_submission.pickle') \n",
    "valid_data = pd.read_pickle('../../../Files/Submissions/train/val_split_submission.pickle')\n",
    "test_data = pd.read_pickle('../../../Files/Submissions/train/test_split_submission.pickle')\n",
    "\n",
    "train_data = train_data[[target, input_column]]\n",
    "valid_data = valid_data[[target, input_column]]\n",
    "test_data = test_data[[target, input_column]]\n",
    "\n",
    "data = pd.concat([train_data, valid_data, test_data])\n",
    "\n",
    "\n",
    "train_instances = train_data[input_column].apply(str).apply(str.split)\n",
    "train_labels = train_data[target]\n",
    "\n",
    "# collect known word tokens and tags\n",
    "wordset, labelset = set(), set()\n",
    "\n",
    "# collect tags from all data, to prevent unseen labels\n",
    "labelset.update(set(data[target]))\n",
    "\n",
    "# get the vocabulary\n",
    "for words in train_instances:\n",
    "    wordset.update(set(words))\n",
    "\n",
    "# map words and tags into ints\n",
    "PAD = '-PAD-'\n",
    "UNK = '-UNK-'\n",
    "word2int = {word: i + 2 for i, word in enumerate(sorted(wordset))}\n",
    "word2int[PAD] = 0  # special token for padding\n",
    "word2int[UNK] = 1  # special token for unknown words\n",
    " \n",
    "label2int = {label: i for i, label in enumerate(sorted(labelset))}\n",
    "# inverted index to translate it back\n",
    "int2label = {i:label for label, i in label2int.items()}\n",
    "\n",
    "\n",
    "def convert2ints(instances):\n",
    "    \"\"\"\n",
    "    function to apply the mapping to all words\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for words in instances:\n",
    "        # replace words with int, 1 for unknown words\n",
    "        word_ints = [word2int.get(word, 1) for word in words]\n",
    "        result.append(word_ints)\n",
    "    return result\n",
    "                          \n",
    "train_instances_int = convert2ints(train_instances)\n",
    "train_labels_int = [label2int[label] for label in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = test_data[input_column].apply(str).apply(str.split)\n",
    "test_labels = test_data[target]\n",
    "\n",
    "test_instances_int = convert2ints(test_instances)\n",
    "test_labels_int = [label2int[label] for label in test_labels]\n",
    "\n",
    "# convert dev data\n",
    "val_instances = valid_data[input_column].apply(str).apply(str.split)\n",
    "val_labels = valid_data[target]\n",
    "\n",
    "val_instances_int = convert2ints(val_instances)\n",
    "val_labels_int = [label2int[label] for label in val_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_1hot = to_categorical(train_labels_int, len(label2int))\n",
    "test_labels_1hot = to_categorical(test_labels_int, len(label2int))\n",
    "val_labels_1hot = to_categorical(val_labels_int, len(label2int))\n",
    "\n",
    "train_labels_1hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "['shill', 'organization'] 2\n",
      "[22655 17709     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0] 15\n"
     ]
    }
   ],
   "source": [
    "# compute 95th percentile of training sentence lengths\n",
    "L = sorted(map(len, train_instances))\n",
    "MAX_LENGTH = L[int(len(L)*0.95)]\n",
    "print(MAX_LENGTH)\n",
    "\n",
    "# apply padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_instances_int = pad_sequences(train_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
    "test_instances_int = pad_sequences(test_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
    "val_instances_int = pad_sequences(val_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
    "\n",
    "print(train_instances[0], len(train_instances[0]))\n",
    "print(train_instances_int[0], len(train_instances_int[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_instances_int)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_instances_int, label=train_labels_int)\n",
    "dval = xgb.DMatrix(val_instances_int, label=val_labels_int)\n",
    "dtest = xgb.DMatrix(test_instances_int, label=test_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 8, 'eta': 1, 'objective': 'multi:softmax', 'num_class': 3, 'eval_metric': ['auc', 'ams@0'], 'nthread': 4, 'silent': 1}\n",
    "\n",
    "evallist = [(dval, 'eval'), (dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:44:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:44:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:44:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:44:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:44:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_round = 500\n",
    "seed = 42\n",
    "cv_results = xgb.cv(\n",
    "    param, \n",
    "    dtrain, \n",
    "    num_boost_round=num_round,\n",
    "    seed=seed, \n",
    "    nfold=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>train-ams@0-mean</th>\n",
       "      <th>train-ams@0-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "      <th>test-ams@0-mean</th>\n",
       "      <th>test-ams@0-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.759845</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>218.330054</td>\n",
       "      <td>0.676217</td>\n",
       "      <td>0.733313</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>109.108588</td>\n",
       "      <td>1.326160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.815736</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>218.333337</td>\n",
       "      <td>0.671118</td>\n",
       "      <td>0.783347</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>109.116321</td>\n",
       "      <td>1.333643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.850129</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>218.337195</td>\n",
       "      <td>0.673382</td>\n",
       "      <td>0.812958</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>109.114368</td>\n",
       "      <td>1.335949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.871339</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>218.337015</td>\n",
       "      <td>0.673368</td>\n",
       "      <td>0.829098</td>\n",
       "      <td>0.005645</td>\n",
       "      <td>109.124072</td>\n",
       "      <td>1.341030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.887703</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>218.336182</td>\n",
       "      <td>0.669097</td>\n",
       "      <td>0.841703</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>109.114505</td>\n",
       "      <td>1.333645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.999699</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>218.330725</td>\n",
       "      <td>0.675186</td>\n",
       "      <td>0.918024</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>218.329904</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>0.918009</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.999701</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>218.329904</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>0.918017</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>218.329904</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>218.329904</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>0.918010</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>109.115831</td>\n",
       "      <td>1.328617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-auc-mean  train-auc-std  train-ams@0-mean  train-ams@0-std  \\\n",
       "0          0.759845       0.001814        218.330054         0.676217   \n",
       "1          0.815736       0.002401        218.333337         0.671118   \n",
       "2          0.850129       0.001867        218.337195         0.673382   \n",
       "3          0.871339       0.004394        218.337015         0.673368   \n",
       "4          0.887703       0.005562        218.336182         0.669097   \n",
       "..              ...            ...               ...              ...   \n",
       "495        0.999699       0.000006        218.330725         0.675186   \n",
       "496        0.999700       0.000006        218.329904         0.675854   \n",
       "497        0.999701       0.000006        218.329904         0.675854   \n",
       "498        0.999702       0.000007        218.329904         0.675854   \n",
       "499        0.999702       0.000007        218.329904         0.675854   \n",
       "\n",
       "     test-auc-mean  test-auc-std  test-ams@0-mean  test-ams@0-std  \n",
       "0         0.733313      0.004254       109.108588        1.326160  \n",
       "1         0.783347      0.004543       109.116321        1.333643  \n",
       "2         0.812958      0.003231       109.114368        1.335949  \n",
       "3         0.829098      0.005645       109.124072        1.341030  \n",
       "4         0.841703      0.007221       109.114505        1.333645  \n",
       "..             ...           ...              ...             ...  \n",
       "495       0.918024      0.001223       109.115831        1.328617  \n",
       "496       0.918009      0.001208       109.115831        1.328617  \n",
       "497       0.918017      0.001216       109.115831        1.328617  \n",
       "498       0.918000      0.001231       109.115831        1.328617  \n",
       "499       0.918010      0.001227       109.115831        1.328617  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = { 'mad_depth' : [3, 5, 6, 8, 10],\n",
    "        'learning_rate' : [0.01, 0.05, 0.1],\n",
    "        'n_estimators' : [100, 500, 1000],\n",
    "        'objective': ['multi:softmax', 'multi:softprob'],\n",
    "        'colsample_bytree': [0.5, 0.7, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator = xgbc,\n",
    "                param_grid = params,\n",
    "                scoring='roc_auc_ovo',\n",
    "                verbose = 1\n",
    "            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/3147567/Thesis/github/classifier/explainable/xgboost.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/github/classifier/explainable/xgboost.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(train_instances_int, train_labels_1hot)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m         clone(base_estimator),\n\u001b[1;32m    841\u001b[0m         X,\n\u001b[1;32m    842\u001b[0m         y,\n\u001b[1;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    849\u001b[0m     )\n\u001b[1;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    852\u001b[0m     )\n\u001b[1;32m    853\u001b[0m )\n\u001b[1;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    679\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    682\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/xgboost/sklearn.py:1382\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mnum_class\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_\n\u001b[1;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(\n\u001b[1;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1381\u001b[0m )\n\u001b[0;32m-> 1382\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1383\u001b[0m     missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[1;32m   1384\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   1385\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   1386\u001b[0m     group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1387\u001b[0m     qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1388\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1389\u001b[0m     base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m   1390\u001b[0m     feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m   1391\u001b[0m     eval_set\u001b[39m=\u001b[39;49meval_set,\n\u001b[1;32m   1392\u001b[0m     sample_weight_eval_set\u001b[39m=\u001b[39;49msample_weight_eval_set,\n\u001b[1;32m   1393\u001b[0m     base_margin_eval_set\u001b[39m=\u001b[39;49mbase_margin_eval_set,\n\u001b[1;32m   1394\u001b[0m     eval_group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1395\u001b[0m     eval_qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1396\u001b[0m     create_dmatrix\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs: DMatrix(nthread\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m   1397\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menable_categorical,\n\u001b[1;32m   1398\u001b[0m )\n\u001b[1;32m   1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1401\u001b[0m     params,\n\u001b[1;32m   1402\u001b[0m     train_dmatrix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m   1412\u001b[0m )\n\u001b[1;32m   1414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/xgboost/sklearn.py:401\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    382\u001b[0m     missing: \u001b[39mfloat\u001b[39m,\n\u001b[1;32m    383\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     enable_categorical: \u001b[39mbool\u001b[39m,\n\u001b[1;32m    397\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[39mstr\u001b[39m]]]:\n\u001b[1;32m    398\u001b[0m     \u001b[39m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the way.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     train_dmatrix \u001b[39m=\u001b[39m create_dmatrix(\n\u001b[1;32m    402\u001b[0m         data\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    403\u001b[0m         label\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    404\u001b[0m         group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m    405\u001b[0m         qid\u001b[39m=\u001b[39;49mqid,\n\u001b[1;32m    406\u001b[0m         weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    407\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    408\u001b[0m         feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    409\u001b[0m         missing\u001b[39m=\u001b[39;49mmissing,\n\u001b[1;32m    410\u001b[0m         enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    413\u001b[0m     n_validation \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m eval_set \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(eval_set)\n\u001b[1;32m    415\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/xgboost/sklearn.py:1396\u001b[0m, in \u001b[0;36mXGBClassifier.fit.<locals>.<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mnum_class\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_\n\u001b[1;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(\n\u001b[1;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1381\u001b[0m )\n\u001b[1;32m   1382\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1383\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1384\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   1385\u001b[0m     y\u001b[39m=\u001b[39my,\n\u001b[1;32m   1386\u001b[0m     group\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1387\u001b[0m     qid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1388\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[1;32m   1389\u001b[0m     base_margin\u001b[39m=\u001b[39mbase_margin,\n\u001b[1;32m   1390\u001b[0m     feature_weights\u001b[39m=\u001b[39mfeature_weights,\n\u001b[1;32m   1391\u001b[0m     eval_set\u001b[39m=\u001b[39meval_set,\n\u001b[1;32m   1392\u001b[0m     sample_weight_eval_set\u001b[39m=\u001b[39msample_weight_eval_set,\n\u001b[1;32m   1393\u001b[0m     base_margin_eval_set\u001b[39m=\u001b[39mbase_margin_eval_set,\n\u001b[1;32m   1394\u001b[0m     eval_group\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1395\u001b[0m     eval_qid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m-> 1396\u001b[0m     create_dmatrix\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: DMatrix(nthread\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m   1397\u001b[0m     enable_categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_categorical,\n\u001b[1;32m   1398\u001b[0m )\n\u001b[1;32m   1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1401\u001b[0m     params,\n\u001b[1;32m   1402\u001b[0m     train_dmatrix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m   1412\u001b[0m )\n\u001b[1;32m   1414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/xgboost/core.py:643\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m handle, feature_names, feature_types \u001b[39m=\u001b[39m dispatch_data_backend(\n\u001b[1;32m    644\u001b[0m     data,\n\u001b[1;32m    645\u001b[0m     missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[1;32m    646\u001b[0m     threads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnthread,\n\u001b[1;32m    647\u001b[0m     feature_names\u001b[39m=\u001b[39;49mfeature_names,\n\u001b[1;32m    648\u001b[0m     feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[1;32m    649\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    651\u001b[0m \u001b[39massert\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m handle\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/xgboost/data.py:887\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    883\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_scipy_csr(\n\u001b[1;32m    884\u001b[0m         data\u001b[39m.\u001b[39mtocsr(), missing, threads, feature_names, feature_types\n\u001b[1;32m    885\u001b[0m     )\n\u001b[1;32m    886\u001b[0m \u001b[39mif\u001b[39;00m _is_numpy_array(data):\n\u001b[0;32m--> 887\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_numpy_array(data, missing, threads, feature_names,\n\u001b[1;32m    888\u001b[0m                              feature_types)\n\u001b[1;32m    889\u001b[0m \u001b[39mif\u001b[39;00m _is_uri(data):\n\u001b[1;32m    890\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_uri(data, missing, feature_names, feature_types)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/xgboost/data.py:185\u001b[0m, in \u001b[0;36m_from_numpy_array\u001b[0;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    179\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m    180\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmissing\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(missing),\n\u001b[1;32m    181\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnthread\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mint\u001b[39m(nthread),\n\u001b[1;32m    182\u001b[0m }\n\u001b[1;32m    183\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39m(json\u001b[39m.\u001b[39mdumps(args), \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m _check_call(\n\u001b[0;32m--> 185\u001b[0m     _LIB\u001b[39m.\u001b[39;49mXGDMatrixCreateFromDense(\n\u001b[1;32m    186\u001b[0m         _array_interface(data),\n\u001b[1;32m    187\u001b[0m         config,\n\u001b[1;32m    188\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(handle),\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[39mreturn\u001b[39;00m handle, feature_names, feature_types\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf.fit(train_instances_int, train_labels_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridsearch = load('xgbCVGridSearch.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgboost.sklearn.XGBClassifier"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 1000, 'num_class': 3, 'objective': 'multi:softprob'}\n",
      "Highest AUC:  0.9309728292375551\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters:\", Gridsearch.best_params_)\n",
    "print(\"Highest AUC: \", (Gridsearch.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(Gridesearch.cv_results_)\n",
    "# df1.to_csv('xgboost_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_num_class</th>\n",
       "      <th>param_objective</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>80.091854</td>\n",
       "      <td>8.578351</td>\n",
       "      <td>0.422838</td>\n",
       "      <td>0.032511</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.928852</td>\n",
       "      <td>0.929915</td>\n",
       "      <td>0.932881</td>\n",
       "      <td>0.929763</td>\n",
       "      <td>0.933452</td>\n",
       "      <td>0.930973</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>61.540091</td>\n",
       "      <td>6.199461</td>\n",
       "      <td>0.324311</td>\n",
       "      <td>0.029329</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.927215</td>\n",
       "      <td>0.928086</td>\n",
       "      <td>0.931972</td>\n",
       "      <td>0.927805</td>\n",
       "      <td>0.931833</td>\n",
       "      <td>0.929382</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>84.471425</td>\n",
       "      <td>9.330232</td>\n",
       "      <td>0.453256</td>\n",
       "      <td>0.045889</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.925553</td>\n",
       "      <td>0.927085</td>\n",
       "      <td>0.930634</td>\n",
       "      <td>0.926786</td>\n",
       "      <td>0.930606</td>\n",
       "      <td>0.928133</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>92.961855</td>\n",
       "      <td>7.584533</td>\n",
       "      <td>0.471483</td>\n",
       "      <td>0.070459</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.1...</td>\n",
       "      <td>0.926387</td>\n",
       "      <td>0.926844</td>\n",
       "      <td>0.930113</td>\n",
       "      <td>0.926399</td>\n",
       "      <td>0.930192</td>\n",
       "      <td>0.927987</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>45.372425</td>\n",
       "      <td>1.339221</td>\n",
       "      <td>0.232754</td>\n",
       "      <td>0.016557</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.925062</td>\n",
       "      <td>0.926473</td>\n",
       "      <td>0.929667</td>\n",
       "      <td>0.926238</td>\n",
       "      <td>0.930366</td>\n",
       "      <td>0.927561</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "44      80.091854      8.578351         0.422838        0.032511   \n",
       "41      61.540091      6.199461         0.324311        0.029329   \n",
       "29      84.471425      9.330232         0.453256        0.045889   \n",
       "89      92.961855      7.584533         0.471483        0.070459   \n",
       "43      45.372425      1.339221         0.232754        0.016557   \n",
       "\n",
       "   param_colsample_bytree param_learning_rate param_max_depth  \\\n",
       "44                    0.5                 0.1              10   \n",
       "41                    0.5                 0.1               8   \n",
       "29                    0.5                0.05              10   \n",
       "89                    0.7                 0.1              10   \n",
       "43                    0.5                 0.1              10   \n",
       "\n",
       "   param_n_estimators param_num_class param_objective  \\\n",
       "44               1000               3  multi:softprob   \n",
       "41               1000               3  multi:softprob   \n",
       "29               1000               3  multi:softprob   \n",
       "89               1000               3  multi:softprob   \n",
       "43                500               3  multi:softprob   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "44  {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.928852   \n",
       "41  {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.927215   \n",
       "29  {'colsample_bytree': 0.5, 'learning_rate': 0.0...           0.925553   \n",
       "89  {'colsample_bytree': 0.7, 'learning_rate': 0.1...           0.926387   \n",
       "43  {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.925062   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "44           0.929915           0.932881           0.929763   \n",
       "41           0.928086           0.931972           0.927805   \n",
       "29           0.927085           0.930634           0.926786   \n",
       "89           0.926844           0.930113           0.926399   \n",
       "43           0.926473           0.929667           0.926238   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "44           0.933452         0.930973        0.001837                1  \n",
       "41           0.931833         0.929382        0.002077                2  \n",
       "29           0.930606         0.928133        0.002095                3  \n",
       "89           0.930192         0.927987        0.001776                4  \n",
       "43           0.930366         0.927561        0.002073                5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sort_values(['rank_test_score']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('xgboost_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(test_instances_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(test_instances_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba.tofile('../../../Files/models/xgb_proba.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tofile('../../../Files/models/xgb_test.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('reddit_env_test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c5bfe69c2b2d435baea75ee1a7865fc7666bb526179204de658e4f2811cb086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
