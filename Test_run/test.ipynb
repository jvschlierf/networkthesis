{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import psaw\n",
    "import numpy\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import simplejson as json\n",
    "from pmaw import PushshiftAPI\n",
    "import pyspark\n",
    "import datetime\n",
    "import os\n",
    "from itertools import chain\n",
    "import csv\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL-divergence(box_1 || box_2): 0.057 \n",
      "KL-divergence(box_2 || box_1): 0.056 \n",
      "KL-divergence(box_1 || box_1): 0.000 \n",
      "Using Scipy rel_entr function\n",
      "KL-divergence(box_1 || box_2): 0.057 \n",
      "KL-divergence(box_2 || box_1): 0.056 \n",
      "KL-divergence(box_1 || box_1): 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fh/yf9jwszj6d5_1_xhfz_l6k000000gn/T/ipykernel_24124/3709783919.py:9: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(a[i] * np.log(a[i]/b[i]) for i in range(len(a)))\n"
     ]
    }
   ],
   "source": [
    "# box =[P(green),P(blue),P(red),P(yellow)]\n",
    "box_1 = [0.25, 0.33, 0.23, 0.19]\n",
    "box_2 = [0.21, 0.21, 0.32, 0.26]\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "def kl_divergence(a, b):\n",
    "\treturn sum(a[i] * np.log(a[i]/b[i]) for i in range(len(a)))\n",
    "\t\n",
    "print('KL-divergence(box_1 || box_2): %.3f ' % kl_divergence(box_1,box_2))\n",
    "print('KL-divergence(box_2 || box_1): %.3f ' % kl_divergence(box_2,box_1))\n",
    "\n",
    "# D( p || p) =0\n",
    "print('KL-divergence(box_1 || box_1): %.3f ' % kl_divergence(box_1,box_1))\n",
    "\n",
    "print(\"Using Scipy rel_entr function\")\n",
    "box_1 = np.array(box_1)\n",
    "box_2 = np.array(box_2)\n",
    "\n",
    "print('KL-divergence(box_1 || box_2): %.3f ' % sum(rel_entr(box_1,box_2)))\n",
    "print('KL-divergence(box_2 || box_1): %.3f ' % sum(rel_entr(box_2,box_1)))\n",
    "print('KL-divergence(box_1 || box_1): %.3f ' % sum(rel_entr(box_1,box_1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confidences = sorted(list(set(loc_pics['confidence'])))[20:-20]\n",
    "\n",
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL = dict()\n",
    "for c in confidences:\n",
    "    array_1 = loc_pics.loc[loc_pics['confidence'] >= c, 'iou'].sort_values().to_list()\n",
    "    array_2 = loc_pics.loc[loc_pics['confidence'] < c, 'iou'].sort_values().to_list()\n",
    "        \n",
    "    if len(array_1) < len(array_2):\n",
    "        array_2 = np.random.choice(array_2, len(array_1))\n",
    "    \n",
    "    else:\n",
    "        array_1 = np.random.choice(array_1, len(array_2))\n",
    "    \n",
    "    KL[c] = mutual_info_score(array_1,array_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(KL, key=KL.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/Users/jakobschlierf/Downloads/Voluntarist_meme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join('/Users/jakobschlierf/Downloads/Voluntarist_meme', file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1607639850.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1643205900.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1583017200.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1613111550.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1610375700.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1593960600.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1604904000.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1640470050.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1626790800.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1637734200.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1624054950.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1634998350.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1599432300.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1618583250.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1596696450.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1585753050.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1615847400.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1602168150.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1588488900.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1621319100.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1591224750.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1632262500.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1629526650.pickle',\n",
       " '/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes-1645941750.pickle']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_splits(files, subreddit, ptype): # merges the files down\n",
    "    merge_candidates = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            if os.path.getsize(file) > 459:\n",
    "                if file[:-18] == subreddit:\n",
    "                    merge_candidates.append(file)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    merge_candidates.sort()\n",
    "    if len(merge_candidates) == 24:\n",
    "        df = pd.concat([pd.read_pickle(candidate) for candidate in merge_candidates])\n",
    "        df.to_pickle('/Users/akobschlierf/Desktop/Master/Thesis/Files/Comments/VoluntaristMemes.pickle')\n",
    "        print(f'{subreddit} merged')\n",
    "        for i in merge_candidates:\n",
    "            os.remove(i)\n",
    "    else:\n",
    "        print(f'{subreddit} has only {len(merge_candidates)} files to merge. Merging not possible.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_candidates = []\n",
    "for file in files:\n",
    "    try:\n",
    "        if os.path.getsize(file) > 459:\n",
    "            if file[:-18] == 'VoluntaristMemes':\n",
    "                merge_candidates.append(file)\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799062\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "164789\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "1617779\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "215004\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "400714\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "2304693\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "1006853\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "107911\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "344347\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "179874\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "316471\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "148441\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "668999\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "235911\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "1627944\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "2607191\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "152984\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "747781\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "3091790\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "616197\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "2381569\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "387131\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "1331889\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n",
      "29869\n",
      "/Users/jakobschlierf/Downloads/Voluntarist_meme/VoluntaristMemes\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print(os.path.getsize(file))\n",
    "    print(file[:-18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_pickle(candidate) for candidate in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63699"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('/Users/jakobschlierf/Desktop/Master/Thesis/Files/Comments/VoluntaristMemes.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Preprocessing/subreddits.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    subreddits = list(reader)\n",
    "\n",
    "subreddits = list(chain.from_iterable(subreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_missing_files(dir, files):\n",
    "    missing_files = []\n",
    "    for file in files:\n",
    "        if not os.path.exists(os.path.join(dir, file)) or os.path.getsize(os.path.join(dir, file)) <= 459:\n",
    "            missing_files.append(file)\n",
    "    return missing_files\n",
    "\n",
    "\n",
    "def splittimeframe(subreddit, start, end, split): # splits the time into a series of smaller files to pull\n",
    "    split_list = []\n",
    "    step = (end - start) / split\n",
    "    for i in range(split):\n",
    "            s = int(start + i * step)\n",
    "            e = int((start + (i + 1) * step) - 1)\n",
    "            if i == split - 1:\n",
    "                    e += 86400\n",
    "            split_list.append(f'{subreddit}-{s}.pickle')\n",
    "\n",
    "    return split_list\n",
    "\n",
    "def find_existing_pulls(type, subreddits): #remove existing pulls from subreddits list\n",
    "    done = os.listdir(f'../../Files/{type}/')\n",
    "    for i in done:\n",
    "        done[done.index(i)] = i[:-7]\n",
    "    res = [i for i in subreddits if i not in done]\n",
    "    return res\n",
    "\n",
    "\n",
    "subreddits_not_completed = find_existing_pulls('Submissions', subreddits)\n",
    "\n",
    "start = int(datetime.datetime(2020, 3, 1).timestamp())\n",
    "end = int(datetime.datetime(2022, 3, 31).timestamp())\n",
    "split = 24\n",
    "missing_parts = []\n",
    "for subreddit in subreddits_not_completed:\n",
    "    \n",
    "    subreddit_parts = splittimeframe(subreddit, start, end, split)\n",
    "    missing = identify_missing_files(f'../../Files/Submissions/temp/', subreddit_parts)\n",
    "    for m in missing:\n",
    "        missing_parts.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4559"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_not_completed = find_existing_pulls('Submissions', subreddits)\n",
    "\n",
    "start = int(datetime.datetime(2020, 3, 1).timestamp())\n",
    "end = int(datetime.datetime(2022, 3, 31).timestamp())\n",
    "split = 24\n",
    "missing_parts = []\n",
    "for subreddit in subreddits_not_completed:\n",
    "    \n",
    "    subreddit_parts = splittimeframe(subreddit, start, end, split)\n",
    "    missing = identify_missing_files(f'../../Files/Submissions/temp/', subreddit_parts)\n",
    "    for m in missing:\n",
    "        missing_parts.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2385"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_parts.index('PoliticalCompassMemes-1618583250.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../../Files/Submissions/temp/missing.csv', \"w\")\n",
    "writer = csv.writer(file, delimiter = \"\\n\")\n",
    "for list_ in missing_parts:\n",
    "     writer.writerow([list_])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dankmemes 1583017200\n"
     ]
    }
   ],
   "source": [
    "print(missing_parts[0][:-18], missing_parts[0][-17:-7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(datetime.datetime(2020, 3, 1).timestamp())\n",
    "end = int(datetime.datetime(2022, 3, 31).timestamp())\n",
    "split = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splittimeframe(subreddit, start, end, split):\n",
    "    split_list = []\n",
    "    step = (end - start) / split\n",
    "    for i in range(split):\n",
    "            s = int(start + i * step)\n",
    "            e = int((start + (i + 1) * step) - 1)\n",
    "            if i == split - 1:\n",
    "                    e += 86400\n",
    "            split_list.append([subreddit, s, e])\n",
    "\n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ivermectin 03/01/2020, 00:00:00 end 04/01/2020, 16:57:29\n",
      "2 ivermectin 04/01/2020, 16:57:30 end 05/03/2020, 08:54:59\n",
      "3 ivermectin 05/03/2020, 08:55:00 end 06/04/2020, 00:52:29\n",
      "4 ivermectin 06/04/2020, 00:52:30 end 07/05/2020, 16:49:59\n",
      "5 ivermectin 07/05/2020, 16:50:00 end 08/06/2020, 08:47:29\n",
      "6 ivermectin 08/06/2020, 08:47:30 end 09/07/2020, 00:44:59\n",
      "7 ivermectin 09/07/2020, 00:45:00 end 10/08/2020, 16:42:29\n",
      "8 ivermectin 10/08/2020, 16:42:30 end 11/09/2020, 07:39:59\n",
      "9 ivermectin 11/09/2020, 07:40:00 end 12/10/2020, 23:37:29\n",
      "10 ivermectin 12/10/2020, 23:37:30 end 01/11/2021, 15:34:59\n",
      "11 ivermectin 01/11/2021, 15:35:00 end 02/12/2021, 07:32:29\n",
      "12 ivermectin 02/12/2021, 07:32:30 end 03/15/2021, 23:29:59\n",
      "13 ivermectin 03/15/2021, 23:30:00 end 04/16/2021, 16:27:29\n",
      "14 ivermectin 04/16/2021, 16:27:30 end 05/18/2021, 08:24:59\n",
      "15 ivermectin 05/18/2021, 08:25:00 end 06/19/2021, 00:22:29\n",
      "16 ivermectin 06/19/2021, 00:22:30 end 07/20/2021, 16:19:59\n",
      "17 ivermectin 07/20/2021, 16:20:00 end 08/21/2021, 08:17:29\n",
      "18 ivermectin 08/21/2021, 08:17:30 end 09/22/2021, 00:14:59\n",
      "19 ivermectin 09/22/2021, 00:15:00 end 10/23/2021, 16:12:29\n",
      "20 ivermectin 10/23/2021, 16:12:30 end 11/24/2021, 07:09:59\n",
      "21 ivermectin 11/24/2021, 07:10:00 end 12/25/2021, 23:07:29\n",
      "22 ivermectin 12/25/2021, 23:07:30 end 01/26/2022, 15:04:59\n",
      "23 ivermectin 01/26/2022, 15:05:00 end 02/27/2022, 07:02:29\n",
      "24 ivermectin 02/27/2022, 07:02:30 end 03/31/2022, 23:59:59\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for sub, start, end in l:\n",
    "    print(i, sub, datetime.datetime.fromtimestamp(start).strftime(\"%m/%d/%Y, %H:%M:%S\"), 'end' ,  datetime.datetime.fromtimestamp(end).strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ivermectin', 1583017200, 1585753049],\n",
       " ['ivermectin', 1585753050, 1588488899],\n",
       " ['ivermectin', 1588488900, 1591224749],\n",
       " ['ivermectin', 1591224750, 1593960599],\n",
       " ['ivermectin', 1593960600, 1596696449],\n",
       " ['ivermectin', 1596696450, 1599432299],\n",
       " ['ivermectin', 1599432300, 1602168149],\n",
       " ['ivermectin', 1602168150, 1604903999],\n",
       " ['ivermectin', 1604904000, 1607639849],\n",
       " ['ivermectin', 1607639850, 1610375699],\n",
       " ['ivermectin', 1610375700, 1613111549],\n",
       " ['ivermectin', 1613111550, 1615847399],\n",
       " ['ivermectin', 1615847400, 1618583249],\n",
       " ['ivermectin', 1618583250, 1621319099],\n",
       " ['ivermectin', 1621319100, 1624054949],\n",
       " ['ivermectin', 1624054950, 1626790799],\n",
       " ['ivermectin', 1626790800, 1629526649],\n",
       " ['ivermectin', 1629526650, 1632262499],\n",
       " ['ivermectin', 1632262500, 1634998349],\n",
       " ['ivermectin', 1634998350, 1637734199],\n",
       " ['ivermectin', 1637734200, 1640470049],\n",
       " ['ivermectin', 1640470050, 1643205899],\n",
       " ['ivermectin', 1643205900, 1645941749],\n",
       " ['ivermectin', 1645941750, 1648763999]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(f'../../Files/{ptype}/temp/')\n",
    "files.pop(files.index('missing.csv'))\n",
    "\n",
    "t = [file[:-18] for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aww-1585753050.pickle',\n",
       " 'aww-1618583250.pickle',\n",
       " 'aww-1596696450.pickle',\n",
       " 'aww-1599432300.pickle',\n",
       " 'aww-1634998350.pickle',\n",
       " 'aww-1624054950.pickle',\n",
       " 'aww-1645941750.pickle',\n",
       " 'aww-1629526650.pickle',\n",
       " 'aww-1632262500.pickle',\n",
       " 'aww-1621319100.pickle',\n",
       " 'aww-1591224750.pickle',\n",
       " 'aww-1615847400.pickle',\n",
       " 'aww-1602168150.pickle',\n",
       " 'aww-1588488900.pickle',\n",
       " 'aww-1604904000.pickle',\n",
       " 'aww-1593960600.pickle',\n",
       " 'aww-1610375700.pickle',\n",
       " 'aww-1613111550.pickle',\n",
       " 'aww-1583017200.pickle',\n",
       " 'aww-1643205900.pickle',\n",
       " 'aww-1607639850.pickle',\n",
       " 'aww-1637734200.pickle',\n",
       " 'aww-1626790800.pickle',\n",
       " 'aww-1640470050.pickle']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('../../TESTMERGE/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [file[:-18] for file in files]\n",
    "\n",
    "subreddits = []\n",
    "for i in t:\n",
    "    if i not in subreddits:\n",
    "        subreddits.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_splits(files, subreddit, ptype):\n",
    "    merge_candidates = []\n",
    "    for file in files:\n",
    "        if os.path.getsize(f'../../Files/{ptype}/temp/{file}') > 459:\n",
    "            if file[:-18] == subreddit:\n",
    "                # merge_candidates.append(f'../../Files/{ptype}/temp/{file}')\n",
    "                merge_candidates.append(f'../../TESTMERGE/{file}')\n",
    "\n",
    "    merge_candidates.sort()\n",
    "    if len(merge_candidates) == 24:\n",
    "        df = pd.concat([pd.read_pickle(candidate) for candidate in merge_candidates])\n",
    "        df.to_pickle(f'../../Files/{ptype}/{subreddit}.pickle')\n",
    "       \n",
    "        # for candidate in merge_candidates:\n",
    "        #     os.remove(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'aww'\n",
    "ptype = 'Submissions'\n",
    "merge_candidates = []\n",
    "for file in files:\n",
    "    if os.path.getsize(f'../../Files/{ptype}/temp/{file}') > 459:\n",
    "        if file[:-18] == subreddit:\n",
    "            # merge_candidates.append(f'../../Files/{ptype}/temp/{file}')\n",
    "            merge_candidates.append(f'../../TESTMERGE/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_pickle(candidate) for candidate in merge_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1251041"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribute Pulls test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Validating Pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import chain\n",
    "with open('../Preprocessing/subreddits.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    subreddits = list(reader)\n",
    "subreddits = list(chain.from_iterable(subreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aspiememes',\n",
       " 'sciencememes',\n",
       " 'Anarcho_Capitalism',\n",
       " 'ConservativeMemes',\n",
       " 'CoronavirusCirclejerk',\n",
       " 'JustUnsubbed',\n",
       " 'theyknew',\n",
       " 'introvert',\n",
       " 'southafrica',\n",
       " 'SurprisingNoOne',\n",
       " 'AnythingGoesNews',\n",
       " 'GoldandBlack',\n",
       " 'PositiveNewsCovid19',\n",
       " 'RuralNewsNetwork',\n",
       " 'Anticonsumption',\n",
       " 'GreenAndPleasant',\n",
       " 'January6',\n",
       " 'LandlordLove',\n",
       " 'LateStageImperialism',\n",
       " 'OliverMarkusMalloy',\n",
       " 'Palestine',\n",
       " 'lebanon',\n",
       " 'tech',\n",
       " 'wholesome',\n",
       " '196',\n",
       " 'ToiletPaperUSA',\n",
       " 'TrollFaceIncident',\n",
       " '2meirl42meirl4meirl',\n",
       " 'wholesomegreentext',\n",
       " 'ANormalDayInAmerica',\n",
       " 'AOC',\n",
       " 'TheFightThatMatters',\n",
       " 'BeansInThings',\n",
       " 'WalmartCelebrities',\n",
       " 'WtWBBot',\n",
       " 'thanksihateit',\n",
       " 'BadChoicesGoodStories',\n",
       " 'InsaneProtestors',\n",
       " 'SocialJusticeInAction',\n",
       " 'TheNarcoblog701',\n",
       " 'alberta',\n",
       " 'islam',\n",
       " 'worldnewsvideo',\n",
       " 'AmericanFascism2020',\n",
       " 'AreTheStraightsOK',\n",
       " 'Astuff',\n",
       " 'AskWomenOfColorOver30',\n",
       " 'anime_titties',\n",
       " 'AutisticPride',\n",
       " 'Moronavirus',\n",
       " 'BoycottChina',\n",
       " 'BreadTube',\n",
       " 'thedavidpakmanshow',\n",
       " 'COMPLETEANARCHY',\n",
       " 'CanadaPolitics',\n",
       " 'CapitolConsequences',\n",
       " 'ClassPoliticsTwitter',\n",
       " 'lostgeneration',\n",
       " 'DeSantis',\n",
       " 'JordanPeterson',\n",
       " 'LouderWithCrowder',\n",
       " 'Republican',\n",
       " 'AmericanPandemics',\n",
       " 'CoronavirusVariants',\n",
       " 'CoronavirusUS',\n",
       " 'NoLockdownsNoMasks',\n",
       " 'OpenTheSchools',\n",
       " 'ParentingWithoutFear',\n",
       " 'ShitPoliticsSays',\n",
       " 'NoNewNormalBan',\n",
       " 'CoronavirusPandemic',\n",
       " 'CoronaVirus_2019_nCoV',\n",
       " 'Cringetopia',\n",
       " 'CuratedTumblr',\n",
       " 'collapse',\n",
       " 'DankLeft',\n",
       " 'DemocraticSocialism',\n",
       " 'DescentIntoTyranny',\n",
       " 'TheHallOfShame',\n",
       " 'Brand_Insider',\n",
       " 'composernews',\n",
       " 'Edmonton',\n",
       " 'onguardforthee',\n",
       " 'EndlessWar',\n",
       " 'EntitledBitch',\n",
       " 'EverythingScience',\n",
       " 'AIandRobotics',\n",
       " 'regenerate',\n",
       " 'FoxFiction',\n",
       " 'ActualFreakouts',\n",
       " 'NotoQanon',\n",
       " 'entitledparentsmemes',\n",
       " 'karens',\n",
       " 'Fuckthealtright',\n",
       " 'ForUnitedStates',\n",
       " 'GenZedong',\n",
       " 'ShitLiberalsSay',\n",
       " 'CommunismMemes',\n",
       " 'GreenAndFriendly',\n",
       " 'GreyAndUnpleasant',\n",
       " 'Labour',\n",
       " 'LabourUK',\n",
       " 'Politsturm',\n",
       " 'WikiLeaks',\n",
       " 'gay_irl',\n",
       " 'israelexposed',\n",
       " 'ukpolitics',\n",
       " 'TheRightCantMeme',\n",
       " 'fragilecommunism',\n",
       " 'NotFunnyButHilarious',\n",
       " 'HomeDepot',\n",
       " 'IndianCountry',\n",
       " 'InterestingVideoClips',\n",
       " 'Incredibly',\n",
       " 'WMAWCBF',\n",
       " 'brexit',\n",
       " 'kotakuinaction2',\n",
       " 'northernireland',\n",
       " 'vaxxhappened',\n",
       " 'LockdownCriticalLeft',\n",
       " 'LockdownSkepticism',\n",
       " 'TrueAnon',\n",
       " 'stupidpol',\n",
       " 'NoNoNewNormal',\n",
       " 'criticalblunder',\n",
       " 'medizzy',\n",
       " 'Makesmybloodboil',\n",
       " 'Maps',\n",
       " 'megafaunarewilding',\n",
       " 'oldmaps',\n",
       " 'Netherlands',\n",
       " 'Slovakia',\n",
       " 'mapporncirclejerk',\n",
       " 'MarchAgainstNazis',\n",
       " 'RepublicanValues',\n",
       " 'MurderedByAOC',\n",
       " 'MxRMods',\n",
       " 'NewDealAmerica',\n",
       " 'NoNewNormal',\n",
       " 'ConspiracyUltra',\n",
       " 'CovidIsACult',\n",
       " 'altnewz',\n",
       " 'antimaskers',\n",
       " 'conspiracies',\n",
       " 'conspiracyUK',\n",
       " 'conspiracyhub',\n",
       " 'conspiro',\n",
       " 'theculling',\n",
       " 'NonPoliticalTwitter',\n",
       " 'ParlerWatch',\n",
       " 'PoliticalVideo',\n",
       " 'Political_Revolution',\n",
       " 'ItHadToBeBrazil',\n",
       " 'PublicFreakoutX',\n",
       " 'Qult_Headquarters',\n",
       " 'houston',\n",
       " 'texas',\n",
       " 'wordington',\n",
       " 'yourmomshousepodcast',\n",
       " 'conspiracy_commons',\n",
       " 'karen',\n",
       " 'newyorkcity',\n",
       " 'sydney',\n",
       " 'Trumpgret',\n",
       " 'maskfreakouts',\n",
       " 'QAnonCasualties',\n",
       " 'QanonKaren',\n",
       " 'InsaneParler',\n",
       " 'MeWeTrolling',\n",
       " 'skeptic',\n",
       " 'Rakingthemuck',\n",
       " 'ReallyAmerican',\n",
       " 'SapphoAndHerFriend',\n",
       " 'Scotland',\n",
       " 'ShitAmericansSay',\n",
       " 'Sino',\n",
       " 'SocialistRA',\n",
       " 'StallmanWasRight',\n",
       " 'TikTokHumor',\n",
       " 'TimDillon',\n",
       " 'DankMemesFromSite19',\n",
       " 'SpeedOfLobsters',\n",
       " 'TOTALLYREALTWEETS',\n",
       " 'VaushV',\n",
       " 'YUROP',\n",
       " 'TopMindsOfReddit',\n",
       " 'Trumpvirus',\n",
       " 'EcoNewsNetwork',\n",
       " 'Green_News',\n",
       " 'LGBTnews',\n",
       " 'TheHealingEarth',\n",
       " 'WestVirginia',\n",
       " 'Positive_News',\n",
       " 'WatchDogsWoofInside',\n",
       " 'premiuminternet',\n",
       " 'WayOfTheBern',\n",
       " 'NEWPOLITIC',\n",
       " 'WhatTheTwitter',\n",
       " 'wokekids',\n",
       " 'alltheleft',\n",
       " 'union',\n",
       " 'EUnews',\n",
       " 'PBS_NewsHour',\n",
       " 'climatesolutions',\n",
       " 'europes',\n",
       " 'geopolitics',\n",
       " 'inthenews',\n",
       " 'japan',\n",
       " 'korea',\n",
       " 'nationofeurope',\n",
       " 'neocentrism',\n",
       " 'taiwan',\n",
       " 'uninsurable',\n",
       " 'worldevents',\n",
       " 'artmemes',\n",
       " 'INTP',\n",
       " 'aaaaaaacccccccce',\n",
       " 'depression_memes',\n",
       " 'furry_irl',\n",
       " 'goodanimemes',\n",
       " 'infp',\n",
       " 'socialanxiety',\n",
       " 'Calgary',\n",
       " 'CrimeMugshots',\n",
       " 'Kashmiri',\n",
       " 'The_Mueller',\n",
       " 'myanmar',\n",
       " 'BabyBumps',\n",
       " 'BorderCollie',\n",
       " 'ArchitecturalRevival',\n",
       " 'BrazilianPainting',\n",
       " 'HUEstation',\n",
       " 'covidskepticscanada',\n",
       " 'canadaleft',\n",
       " 'catsaysmao',\n",
       " 'chomsky',\n",
       " 'collapze',\n",
       " 'xrmed',\n",
       " 'InfowarriorRides',\n",
       " 'dontyouknowwhoiam',\n",
       " 'stupidpeoplefacebook',\n",
       " 'DebateVaccine',\n",
       " 'TrueAntiVaccination',\n",
       " 'conspiracyNOPOL',\n",
       " 'conspiracyfact',\n",
       " 'noagenda',\n",
       " 'vacci_nation',\n",
       " 'walkaway',\n",
       " 'whatsreallygoinon',\n",
       " 'cursedmemes',\n",
       " 'oklahoma',\n",
       " 'USA_de',\n",
       " 'berlin',\n",
       " 'germany',\n",
       " 'CovIdiots',\n",
       " 'fucktheccp',\n",
       " 'Boomerhumour',\n",
       " 'punnychalkboards',\n",
       " 'houstonmusic',\n",
       " 'discworld',\n",
       " 'labor',\n",
       " 'CPUSA',\n",
       " 'ndp',\n",
       " 'libertarianmeme',\n",
       " 'Jreg',\n",
       " 'BreakingPointsNews',\n",
       " 'Communalists',\n",
       " 'InformedTankie',\n",
       " 'InternationalLeft',\n",
       " 'KomradeInfo',\n",
       " 'sendinthetanks',\n",
       " 'urbanplanning',\n",
       " 'publix',\n",
       " 'washingtondc',\n",
       " 'malaysia',\n",
       " 'FuckThatsViral',\n",
       " 'Oklahomatoday',\n",
       " 'normanok',\n",
       " 'okdemocrats',\n",
       " 'BadCopNoTimbit',\n",
       " 'BhutilaKarpoche',\n",
       " 'Canada_Politics',\n",
       " 'ClimateCrisisCanada',\n",
       " 'NovaScotia',\n",
       " 'fordnation',\n",
       " 'halifax',\n",
       " 'ottawa',\n",
       " 'vancouver',\n",
       " 'pointlesslygendered',\n",
       " 'Virginia',\n",
       " 'conspiracytheories',\n",
       " 'Trumpet_of_Discord',\n",
       " 'CoronavirusRelief',\n",
       " 'religiousfruitcake',\n",
       " 'CoV2Canada',\n",
       " 'TheBeliefInstinct',\n",
       " 'Virology',\n",
       " 'seculartalk',\n",
       " 'socialism',\n",
       " 'technews',\n",
       " 'NoShitSherlock',\n",
       " 'techdetects',\n",
       " 'CorpusChristi',\n",
       " 'ViralTexas',\n",
       " 'sanantonio',\n",
       " 'mildlyvagina',\n",
       " 'stupidloopholes',\n",
       " 'Libernadian',\n",
       " 'Etobicoke',\n",
       " 'Scarborough',\n",
       " 'TTC',\n",
       " 'usa',\n",
       " 'FuckTheAltWrong',\n",
       " 'donaldtrump',\n",
       " 'AskThe_Donald',\n",
       " 'DeclineIntoCensorship',\n",
       " 'FreeSpeech',\n",
       " 'The_Chocker',\n",
       " 'Hasan_Piker',\n",
       " 'UpvoteBecauseButt',\n",
       " 'AnarchoChristian',\n",
       " 'AntiComAction',\n",
       " 'Firearms',\n",
       " 'Government_is_lame',\n",
       " 'GuardiansOfLiberty',\n",
       " 'Shitstatistssay',\n",
       " 'TimPool',\n",
       " 'VoluntaristMemes',\n",
       " 'WallStreetbetsELITE',\n",
       " 'Wallstreetsilver',\n",
       " 'ConservativesOnly',\n",
       " 'uspolitics',\n",
       " 'IntellectualDarkWeb',\n",
       " 'Jordan_Peterson_Memes',\n",
       " 'Jung',\n",
       " 'The_Ultimate',\n",
       " 'WesternCivilisation',\n",
       " 'enoughpetersonspam',\n",
       " 'Blessed_Images',\n",
       " 'FridayNightFunkin',\n",
       " 'Offensivejokes',\n",
       " 'TumblrInAction',\n",
       " 'averageredditor',\n",
       " 'redditmoment',\n",
       " 'Conservativelifestyle',\n",
       " 'Defund_NPR_and_PBS',\n",
       " 'EXDemocrats2021',\n",
       " 'climateskeptics',\n",
       " 'conservatives',\n",
       " 'tucker_carlson',\n",
       " 'AntifascistsofReddit',\n",
       " 'WomenInNews',\n",
       " 'asia',\n",
       " 'AntiLibertarianCringe',\n",
       " 'EnoughCommieSpam',\n",
       " 'LoveForLandlords',\n",
       " 'basicincomejapan',\n",
       " 'Destiny',\n",
       " 'EnoughIDWspam',\n",
       " 'Israel',\n",
       " 'Jewish',\n",
       " 'SocialDemocracy',\n",
       " 'TheMajorityReport',\n",
       " 'daverubin',\n",
       " 'sanepolitics',\n",
       " 'UFCW',\n",
       " 'redsports',\n",
       " 'didnthappen',\n",
       " 'CoronavirusWA',\n",
       " 'VirginiaPolitics',\n",
       " 'nova',\n",
       " 'rva',\n",
       " 'pregnant',\n",
       " 'ChurchOfCOVID',\n",
       " 'CovidSheepWatch',\n",
       " 'LatinoPeopleTwitter',\n",
       " 'VideojuegosMX',\n",
       " 'AgainstTheIlluminati',\n",
       " 'AlternativeHistory',\n",
       " 'AwakenTheSheep',\n",
       " 'DebateVaccines',\n",
       " 'LevelArchive',\n",
       " 'DevelEire',\n",
       " 'GAA',\n",
       " 'VanLife',\n",
       " 'SydneyNews',\n",
       " 'EnoughAntifaSpam',\n",
       " 'TheDonaldTrump2024',\n",
       " 'TheShitTankiesSay',\n",
       " 'benshapiro',\n",
       " 'Ohio',\n",
       " 'HillaryForPrison',\n",
       " 'CoronavirusNewYork',\n",
       " 'AgainstHateSubreddits',\n",
       " 'CoronaBumpers',\n",
       " 'Covid19VaccineRats',\n",
       " 'CovidVaccinated',\n",
       " 'CovidVaccineInjury',\n",
       " 'VACCINES',\n",
       " 'VaccineDiscussion',\n",
       " 'modernavaccine',\n",
       " 'AncientTruehistory',\n",
       " 'CoronaVaccines',\n",
       " 'FloridaCoronavirus',\n",
       " 'conservativecartoons',\n",
       " 'FragrantHarbour',\n",
       " 'FUCKCHINAVIRUS',\n",
       " 'FreedomofSpeech',\n",
       " 'TheTrumpZone',\n",
       " 'ronpaul',\n",
       " 'Liberal',\n",
       " 'COVID',\n",
       " 'CoronavirusOntario',\n",
       " 'CoronavirusToronto',\n",
       " 'PeoplesPartyofCanada',\n",
       " 'Liberty_4_Canada',\n",
       " 'CollegeRepublicans',\n",
       " 'Deplatformed_',\n",
       " 'TexasConservatives',\n",
       " 'ProblemSolvingMatrix',\n",
       " 'TakeTheJab',\n",
       " 'VaccineEnvy',\n",
       " 'vaccinesideeffects',\n",
       " 'antivax',\n",
       " 'RussiaLago',\n",
       " 'IntelligenceNews',\n",
       " 'BadGOP_NoDoughnuts',\n",
       " 'GQP',\n",
       " 'Impeach_Trump',\n",
       " 'VoteDEM',\n",
       " 'AntiTrumpAlliance',\n",
       " 'CorporateMisconduct',\n",
       " 'EnoughTrumpSpam',\n",
       " 'Fuck45',\n",
       " 'GunsAreCool',\n",
       " 'Intelligence',\n",
       " 'JoeBiden',\n",
       " 'Keep_Track',\n",
       " 'LincolnProject',\n",
       " 'MAGAs',\n",
       " 'Michigan',\n",
       " 'POTUSWatch',\n",
       " 'Republican_misdeeds',\n",
       " 'TexasPolitics',\n",
       " 'VeganLibs',\n",
       " 'VoteBlue',\n",
       " 'WhatBidenHasDone',\n",
       " 'democrats',\n",
       " 'esist',\n",
       " 'farming',\n",
       " 'foreignpolicy',\n",
       " 'progressive',\n",
       " 'Africa',\n",
       " 'internationalpolitics',\n",
       " 'International',\n",
       " 'Milk_Tea_Alliance',\n",
       " 'YouWillSurviveIt',\n",
       " 'COVID19',\n",
       " 'USGovernment',\n",
       " 'USNews2',\n",
       " 'usanews',\n",
       " 'BidenCoalition',\n",
       " 'DemocracyNow',\n",
       " 'DemocratsforDiversity',\n",
       " 'uspolicy',\n",
       " 'world',\n",
       " 'worldnews2',\n",
       " 'worldnews247',\n",
       " 'OntarioCanada',\n",
       " 'LockdownSkepticismCAN',\n",
       " 'waterloo',\n",
       " 'ModernPropaganda',\n",
       " 'EndTheLockdowns',\n",
       " 'ConspiracyII',\n",
       " 'ConspiracyPastPresent',\n",
       " 'censoredreality',\n",
       " 'ivermectin',\n",
       " 'IVMScience',\n",
       " 'CoronavirusMa',\n",
       " 'Backcountry',\n",
       " 'WaterlooWxRecords',\n",
       " 'worldpolitics2',\n",
       " 'coveryourselfinoil',\n",
       " 'trollface',\n",
       " '2meirl4meirl',\n",
       " 'dankmemes',\n",
       " 'ABoringDystopia',\n",
       " 'FuckYouKaren',\n",
       " 'awfuleverything',\n",
       " 'trippinthroughtime',\n",
       " 'meirl',\n",
       " 'meme',\n",
       " 'HolUp',\n",
       " 'memes',\n",
       " 'MakeMeSuffer',\n",
       " 'funnysigns',\n",
       " 'privacy',\n",
       " 'teenagers',\n",
       " 'me_irl',\n",
       " 'worldnews',\n",
       " 'UpliftingNews',\n",
       " 'conspiracy',\n",
       " 'BeAmazed',\n",
       " 'CryptoCurrency',\n",
       " 'FunnyandSad',\n",
       " 'Futurology',\n",
       " 'HumansBeingBros',\n",
       " 'MadeMeSmile',\n",
       " 'MurderedByWords',\n",
       " 'PoliticalHumor',\n",
       " 'SelfAwarewolves',\n",
       " 'ThatsInsane',\n",
       " 'TikTokCringe',\n",
       " 'UrbanHell',\n",
       " 'assholedesign',\n",
       " 'coolguides',\n",
       " 'facepalm',\n",
       " 'india',\n",
       " 'interestingasfuck',\n",
       " 'mildlyinfuriating',\n",
       " 'mildlyinteresting',\n",
       " 'nottheonion',\n",
       " 'science',\n",
       " 'todayilearned',\n",
       " 'tumblr',\n",
       " 'videos',\n",
       " 'wallstreetbets',\n",
       " 'wholesomememes',\n",
       " '4chan',\n",
       " 'greentext',\n",
       " 'ANormalDayInRussia',\n",
       " 'PublicFreakout',\n",
       " 'ATBGE',\n",
       " 'DiWHY',\n",
       " 'ofcoursethatsathing',\n",
       " 'TIHI',\n",
       " 'dontputyourdickinthat',\n",
       " 'ActualPublicFreakouts',\n",
       " 'IdiotsInCars',\n",
       " 'LosAngeles',\n",
       " 'WinStupidPrizes',\n",
       " 'nextfuckinglevel',\n",
       " 'trashy',\n",
       " 'AmItheAsshole',\n",
       " 'confidentlyincorrect',\n",
       " 'Art',\n",
       " 'pics',\n",
       " 'AskReddit',\n",
       " 'Bad_Cop_No_Donut',\n",
       " 'BikiniBottomTwitter',\n",
       " 'BlackPeopleTwitter',\n",
       " 'LeopardsAteMyFace',\n",
       " 'Justfuckmyshitup',\n",
       " 'CasualUK',\n",
       " 'ChoosingBeggars',\n",
       " 'Conservative',\n",
       " 'Coronavirus',\n",
       " 'dataisbeautiful',\n",
       " 'funny',\n",
       " 'movies',\n",
       " 'nba',\n",
       " 'news',\n",
       " 'CrappyDesign',\n",
       " 'CrazyFuckingVideos',\n",
       " 'Damnthatsinteresting',\n",
       " 'aww',\n",
       " 'vandwellers',\n",
       " 'DidntKnowIWantedThat',\n",
       " 'Documentaries',\n",
       " 'Economics',\n",
       " 'FellowKids',\n",
       " 'JusticeServed',\n",
       " 'insaneparents',\n",
       " 'instant_regret',\n",
       " 'quityourbullshit',\n",
       " 'technology',\n",
       " 'GetMotivated',\n",
       " 'LateStageCapitalism',\n",
       " 'WhitePeopleTwitter',\n",
       " 'antiwork',\n",
       " 'europe',\n",
       " 'unitedkingdom',\n",
       " 'Health',\n",
       " 'HistoryMemes',\n",
       " 'ireland',\n",
       " 'HistoryPorn',\n",
       " 'HydroHomies',\n",
       " 'PornhubComments',\n",
       " 'Unexpected',\n",
       " 'blursedimages',\n",
       " 'cursedcomments',\n",
       " 'maybemaybemaybe',\n",
       " 'okbuddyretard',\n",
       " 'shitposting',\n",
       " 'HongKong',\n",
       " 'Instagramreality',\n",
       " 'agedlikemilk',\n",
       " 'atheism',\n",
       " 'byebyejob',\n",
       " 'entertainment',\n",
       " 'iamatotalpieceofshit',\n",
       " 'insanepeoplefacebook',\n",
       " 'sadcringe',\n",
       " 'oddlysatisfying',\n",
       " 'WTF',\n",
       " 'WatchPeopleDieInside',\n",
       " 'comedyheaven',\n",
       " 'forbiddensnacks',\n",
       " 'holdmyfeedingtube',\n",
       " 'natureismetal',\n",
       " 'MapPorn',\n",
       " 'NoahGetTheBoat',\n",
       " 'NotMyJob',\n",
       " 'PoliticalCompassMemes',\n",
       " 'politics',\n",
       " 'AbruptChaos',\n",
       " 'Whatcouldgowrong',\n",
       " 'donthelpjustfilm',\n",
       " 'fightporn',\n",
       " 'instantkarma',\n",
       " 'peopleofwalmart',\n",
       " 'PupliftingNews',\n",
       " 'SandersForPresident',\n",
       " 'Satisfyingasfuck',\n",
       " 'ShitPostCrusaders',\n",
       " 'Superstonk',\n",
       " 'ThatLookedExpensive',\n",
       " 'Wellthatsucks',\n",
       " 'perfectlycutscreams',\n",
       " 'tifu',\n",
       " 'Wallstreetbetsnew',\n",
       " 'therewasanattempt',\n",
       " 'yesyesyesyesno',\n",
       " 'youseeingthisshit',\n",
       " 'FUCKYOUINPARTICULAR',\n",
       " 'technicallythetruth',\n",
       " 'Philippines',\n",
       " 'canada',\n",
       " 'BrandNewSentence',\n",
       " 'anime_irl',\n",
       " 'comics',\n",
       " 'starterpacks',\n",
       " 'australia',\n",
       " 'cringepics',\n",
       " 'environment',\n",
       " 'NatureIsFuckingLit',\n",
       " 'gifs',\n",
       " 'CityPorn',\n",
       " 'business',\n",
       " 'clevercomebacks',\n",
       " 'iamverysmart',\n",
       " 'woooosh',\n",
       " 'economy',\n",
       " 'AteTheOnion',\n",
       " 'gatekeeping',\n",
       " 'im14andthisisdeep',\n",
       " 'skiing',\n",
       " 'TheLastAirbender',\n",
       " 'oddlyterrifying',\n",
       " 'itookapicture',\n",
       " 'labrats',\n",
       " 'madlads',\n",
       " 'offbeat',\n",
       " 'Bossfight',\n",
       " 'UNBGBBIIVCHIDCTIICBG',\n",
       " 'toptalent',\n",
       " 'oddlyspecific',\n",
       " 'ontario',\n",
       " 'toronto',\n",
       " 'philosophy',\n",
       " 'psychology',\n",
       " 'rareinsults',\n",
       " 'specializedtools',\n",
       " 'suspiciouslyspecific',\n",
       " 'singapore',\n",
       " 'TheWayWeWere',\n",
       " 'AccidentalComedy',\n",
       " 'BadDesigns',\n",
       " 'KidsAreFuckingStupid',\n",
       " 'confusing_perspective',\n",
       " 'crappyoffbrands',\n",
       " 'dontdeadopeninside',\n",
       " 'engrish',\n",
       " 'onejob',\n",
       " 'shittyfoodporn',\n",
       " 'SweatyPalms',\n",
       " 'JoeRogan',\n",
       " 'Libertarian',\n",
       " 'tattoos',\n",
       " 'GoCommitDie',\n",
       " 'PewdiepieSubmissions',\n",
       " 'blackmagicfuckery',\n",
       " 'blessedimages',\n",
       " 'childfree',\n",
       " 'ihadastroke',\n",
       " 'unpopularopinion',\n",
       " 'youngpeopleyoutube',\n",
       " 'bestof',\n",
       " 'sports',\n",
       " 'Awwducational',\n",
       " 'nyc',\n",
       " 'woahdude',\n",
       " 'popping',\n",
       " 'terriblefacebookmemes',\n",
       " 'DataHoarder']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = 'Submissions'\n",
    "done = os.listdir(f'../../Files/{type}/')\n",
    "for i in done:\n",
    "    done[done.index(i)] = i[:-7]\n",
    "res = [i for i in subreddits if i not in done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705 23\n"
     ]
    }
   ],
   "source": [
    "print(len(subreddits), len(done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [i for i in subreddits if i not in done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n"
     ]
    }
   ],
   "source": [
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for Identifying subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df): #aggregate over the subreddits so that we get a list with subreddit - crosspost from and counts\n",
    "    \n",
    "    df2 = df.groupby(['subreddit','crosspost_parent']).agg({'subreddit_subscribers': 'mean', 'crosspost_parent_subs': 'mean' , 'author' : 'count', 'crosspost_parent_num': 'sum'}).reset_index()\n",
    "    df2.rename(columns = {'author':'count',}, inplace = True)\n",
    "    df2['crosspost_parent_subs'] = df2['crosspost_parent_subs'].fillna(0)\n",
    "    df2['crosspost_parent_subs'] = df2['crosspost_parent_subs'].astype(int)\n",
    "\n",
    "    imp = df2.groupby(['crosspost_parent']).agg({'subreddit': 'count', 'count': 'sum'})\n",
    "    imp = imp.rename(columns ={'count': 'total'}) \n",
    "    imp.drop(['subreddit'], axis=1, inplace=True)\n",
    "    df3 = df2.merge(imp, on='crosspost_parent', how='left')\n",
    "\n",
    "    print('aggregated')\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crosspost_child(df, outfile, depth_lim): #find the crosspost children of posts from the observed subreddits\n",
    "    # api = PushshiftAPI()\n",
    "    # df = df[df['num_crossposts'] > 0] # We do this by looking for posts that have been crossposted\n",
    "    # t = df.groupby(['url']).agg({'num_crossposts': 'sum', 'id':  'max'}).reset_index()\n",
    "    # urls = list(t['url']) # then collect the url of posts / attached links / pictures\n",
    "\n",
    "    # df = pd.DataFrame(columns=['id', 'url', 'title', 'subreddit', 'selftext', 'subreddit_subscribers',\n",
    "    #     'num_crossposts', 'crosspost_parent', 'created_utc', 'author', 'num_comments', 'score'])\n",
    "\n",
    "    # for j in urls:\n",
    "    #     results2 = api.search_submissions(\n",
    "    #         url = j, # and search for them using the pushshift api\n",
    "    #         filter=[ 'id', 'url', 'title', 'subreddit', 'selftext', 'subreddit_subscribers',\n",
    "    #         'num_crossposts', 'crosspost_parent', 'created_utc', 'author', 'num_comments', 'score']) \n",
    "    #     temp = pd.DataFrame([thing for thing in results2])\n",
    "    #     df = pd.concat([df, temp])\n",
    "\n",
    "    # df.to_pickle(f'../../Files/{outfile}_raw_child_{depth_lim}.pickle')\n",
    "    df2 = df[df['num_crossposts'] > 0].reset_index(drop=True) #split into parent posts (number of crossposts > 0 )\n",
    "    df3 = df[df['num_crossposts'] == 0].reset_index(drop=True) # and child posts (number of crossposts == 0)\n",
    "    df3 = df3[df3['crosspost_parent'].notna()] # Drop all children where field for parent is empty\n",
    "    df3['crosspost_parent']  = df3['crosspost_parent'].apply(lambda x: x[3:])\n",
    "    df2.drop('crosspost_parent', axis=1, inplace=True)\n",
    "    df2 = df2.rename(columns ={'subreddit':'crosspost_parent', 'subreddit_subscribers': 'crosspost_parent_subs', 'num_crossposts': 'crosspost_parent_num'})\n",
    "    df4 = df2.merge(df3, left_on='id', right_on='crosspost_parent', suffixes= ('','_y'))\n",
    "\n",
    "    df4 = df4[(df4['crosspost_parent'].str.startswith('u_') == False) & (df4['subreddit'].str.startswith('u_') == False)] # get rid off users as subreddit\n",
    "    df5 = aggregate(df4)\n",
    "    \n",
    "    df\n",
    "    print(f'identified {len(df4)} children')\n",
    "    df5.to_pickle(f'../../Files/{outfile}_cross_child_{depth_lim}.pickle') # save file to avoid straining the API\n",
    "    return df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/Users/jakobschlierf/Desktop/Master/Thesis/Files/test_0613_raw_child_2.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n6yugl</td>\n",
       "      <td>https://pdf.math.dev/</td>\n",
       "      <td>Beautiful PDFs from HTML</td>\n",
       "      <td>u_dweeed</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_n6yjsj</td>\n",
       "      <td>1620395121</td>\n",
       "      <td>dweeed</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n6yjsj</td>\n",
       "      <td>https://pdf.math.dev/</td>\n",
       "      <td>Beautiful PDFs from HTML</td>\n",
       "      <td>webdev</td>\n",
       "      <td></td>\n",
       "      <td>821591</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1620394253</td>\n",
       "      <td>speckz</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n6yjm4</td>\n",
       "      <td>https://pdf.math.dev/</td>\n",
       "      <td>Beautiful PDFs from HTML</td>\n",
       "      <td>web_design</td>\n",
       "      <td></td>\n",
       "      <td>536815</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1620394240</td>\n",
       "      <td>speckz</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n6yjju</td>\n",
       "      <td>https://pdf.math.dev/</td>\n",
       "      <td>Beautiful PDFs from HTML</td>\n",
       "      <td>Frontend</td>\n",
       "      <td></td>\n",
       "      <td>135880</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1620394235</td>\n",
       "      <td>speckz</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mli1c7</td>\n",
       "      <td>https://pdf.math.dev</td>\n",
       "      <td>Guide (and example code) to producing beautifu...</td>\n",
       "      <td>css</td>\n",
       "      <td></td>\n",
       "      <td>82004</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1617733009</td>\n",
       "      <td>RentGreat8009</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                    url  \\\n",
       "0  n6yugl  https://pdf.math.dev/   \n",
       "1  n6yjsj  https://pdf.math.dev/   \n",
       "2  n6yjm4  https://pdf.math.dev/   \n",
       "3  n6yjju  https://pdf.math.dev/   \n",
       "4  mli1c7   https://pdf.math.dev   \n",
       "\n",
       "                                               title   subreddit selftext  \\\n",
       "0                           Beautiful PDFs from HTML    u_dweeed            \n",
       "1                           Beautiful PDFs from HTML      webdev            \n",
       "2                           Beautiful PDFs from HTML  web_design            \n",
       "3                           Beautiful PDFs from HTML    Frontend            \n",
       "4  Guide (and example code) to producing beautifu...         css            \n",
       "\n",
       "  subreddit_subscribers num_crossposts crosspost_parent created_utc  \\\n",
       "0                     0              0        t3_n6yjsj  1620395121   \n",
       "1                821591              0              NaN  1620394253   \n",
       "2                536815              0              NaN  1620394240   \n",
       "3                135880              0              NaN  1620394235   \n",
       "4                 82004              0              NaN  1617733009   \n",
       "\n",
       "          author num_comments score  \n",
       "0         dweeed            0     1  \n",
       "1         speckz           19     1  \n",
       "2         speckz            6     1  \n",
       "3         speckz            0     1  \n",
       "4  RentGreat8009            2     1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregated\n",
      "identified 15950 children\n"
     ]
    }
   ],
   "source": [
    "df2 = get_crosspost_child(df, 'serverfix', 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>crosspost_parent_subs</th>\n",
       "      <th>count</th>\n",
       "      <th>crosspost_parent_num</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>interestingasfuck</td>\n",
       "      <td>9076.0</td>\n",
       "      <td>7595870</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111bookmarks</td>\n",
       "      <td>ATBGE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1418326</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200isfineIGUESSugh</td>\n",
       "      <td>peopleofwalmart</td>\n",
       "      <td>50073.0</td>\n",
       "      <td>409263</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13or30</td>\n",
       "      <td>HumansBeingBros</td>\n",
       "      <td>406092.0</td>\n",
       "      <td>2215179</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13or30</td>\n",
       "      <td>sadcringe</td>\n",
       "      <td>404388.0</td>\n",
       "      <td>924672</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit   crosspost_parent  subreddit_subscribers  \\\n",
       "0         0sanitymemes  interestingasfuck                 9076.0   \n",
       "1         111bookmarks              ATBGE                    1.0   \n",
       "2  1200isfineIGUESSugh    peopleofwalmart                50073.0   \n",
       "3               13or30    HumansBeingBros               406092.0   \n",
       "4               13or30          sadcringe               404388.0   \n",
       "\n",
       "   crosspost_parent_subs  count crosspost_parent_num  total  \n",
       "0                7595870      1                   16    541  \n",
       "1                1418326      1                    2    481  \n",
       "2                 409263      1                    1     13  \n",
       "3                2215179      1                    7    144  \n",
       "4                 924672      1                    4     63  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_pickle('../../Files/test_0613_cross_parent_2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>crosspost_parent_subs</th>\n",
       "      <th>count</th>\n",
       "      <th>crosspost_parent_num</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6thForm</td>\n",
       "      <td>2meirl4meirl</td>\n",
       "      <td>35791.0</td>\n",
       "      <td>1268402</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6thForm</td>\n",
       "      <td>6thForm</td>\n",
       "      <td>37984.0</td>\n",
       "      <td>37984</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6thForm</td>\n",
       "      <td>Advice</td>\n",
       "      <td>37341.0</td>\n",
       "      <td>512881</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6thForm</td>\n",
       "      <td>AlevelGeog</td>\n",
       "      <td>41873.0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6thForm</td>\n",
       "      <td>ApplyingToCollege</td>\n",
       "      <td>38328.5</td>\n",
       "      <td>325892</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit   crosspost_parent  subreddit_subscribers  crosspost_parent_subs  \\\n",
       "0   6thForm       2meirl4meirl                35791.0                1268402   \n",
       "1   6thForm            6thForm                37984.0                  37984   \n",
       "2   6thForm             Advice                37341.0                 512881   \n",
       "3   6thForm         AlevelGeog                41873.0                    159   \n",
       "4   6thForm  ApplyingToCollege                38328.5                 325892   \n",
       "\n",
       "   count  crosspost_parent_num  total  \n",
       "0      1                     3    111  \n",
       "1      4                     8      4  \n",
       "2      1                     1     11  \n",
       "3      1                     1      1  \n",
       "4      2                     2      6  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.concat([df3, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8863, 7)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30629, 7)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_pickle('../../Files/test_0613_cross.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = '5FpjYNUrv93rDqflbNE-6w'\n",
    "CLIENT_SECRET = '5ZITWnmNd6_qCbPXDB90xmOaIMo9_w'\n",
    "USER_NAME = 'theonejay96'\n",
    "SCRIPT_NAME = 'Data\\ Science\\ Thesis'\n",
    "LOGIN_PASSWORD = 'vejkyz-hyBfez-4cinte'\n",
    "\n",
    "reddit =  praw.Reddit(\n",
    "    client_id= CLIENT_ID ,\n",
    "    client_secret= CLIENT_SECRET ,\n",
    "    user_agent= SCRIPT_NAME,\n",
    "    username= USER_NAME,\n",
    "    password= LOGIN_PASSWORD)\n",
    "\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David_McTravord555</td>\n",
       "      <td>1654625979</td>\n",
       "      <td>v73j9f</td>\n",
       "      <td>0</td>\n",
       "      <td>worldbuilding</td>\n",
       "      <td>t5_2rd6n</td>\n",
       "      <td>905089</td>\n",
       "      <td>In my universe where magical creatures (like v...</td>\n",
       "      <td>https://www.reddit.com/r/worldbuilding/comment...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antdryem_</td>\n",
       "      <td>1654625979</td>\n",
       "      <td>v73j9d</td>\n",
       "      <td>0</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>t5_2r1hz</td>\n",
       "      <td>76494</td>\n",
       "      <td>Getting to live in Dublin</td>\n",
       "      <td>https://www.reddit.com/r/Dublin/comments/v73j9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spnkmeimbd</td>\n",
       "      <td>1654625979</td>\n",
       "      <td>v73j9a</td>\n",
       "      <td>0</td>\n",
       "      <td>sideboobandass</td>\n",
       "      <td>t5_2pam8b</td>\n",
       "      <td>3271</td>\n",
       "      <td>Come keep me warm</td>\n",
       "      <td>https://i.redd.it/y7heujw9uy391.jpg</td>\n",
       "      <td>t3_v5yhln</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>johnevepierrot</td>\n",
       "      <td>1654625979</td>\n",
       "      <td>v73j99</td>\n",
       "      <td>0</td>\n",
       "      <td>Bromance2</td>\n",
       "      <td>t5_3h6m2l</td>\n",
       "      <td>383</td>\n",
       "      <td>Looking for Connections</td>\n",
       "      <td>https://www.reddit.com/r/Bromance2/comments/v7...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pirl_opden</td>\n",
       "      <td>1654625979</td>\n",
       "      <td>v73j97</td>\n",
       "      <td>0</td>\n",
       "      <td>HiTMAN</td>\n",
       "      <td>t5_2rn0x</td>\n",
       "      <td>134437</td>\n",
       "      <td>Another JIRA ticket,another set of funerals</td>\n",
       "      <td>https://v.redd.it/w7e46i2bo8491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author  created_utc      id  num_crossposts       subreddit  \\\n",
       "0  David_McTravord555   1654625979  v73j9f               0   worldbuilding   \n",
       "1           Antdryem_   1654625979  v73j9d               0          Dublin   \n",
       "2          Spnkmeimbd   1654625979  v73j9a               0  sideboobandass   \n",
       "3      johnevepierrot   1654625979  v73j99               0       Bromance2   \n",
       "4          pirl_opden   1654625979  v73j97               0          HiTMAN   \n",
       "\n",
       "  subreddit_id  subreddit_subscribers  \\\n",
       "0     t5_2rd6n                 905089   \n",
       "1     t5_2r1hz                  76494   \n",
       "2    t5_2pam8b                   3271   \n",
       "3    t5_3h6m2l                    383   \n",
       "4     t5_2rn0x                 134437   \n",
       "\n",
       "                                               title  \\\n",
       "0  In my universe where magical creatures (like v...   \n",
       "1                          Getting to live in Dublin   \n",
       "2                                  Come keep me warm   \n",
       "3                            Looking for Connections   \n",
       "4        Another JIRA ticket,another set of funerals   \n",
       "\n",
       "                                                 url crosspost_parent  \\\n",
       "0  https://www.reddit.com/r/worldbuilding/comment...              NaN   \n",
       "1  https://www.reddit.com/r/Dublin/comments/v73j9...              NaN   \n",
       "2                https://i.redd.it/y7heujw9uy391.jpg        t3_v5yhln   \n",
       "3  https://www.reddit.com/r/Bromance2/comments/v7...              NaN   \n",
       "4                    https://v.redd.it/w7e46i2bo8491              NaN   \n",
       "\n",
       "                               crosspost_parent_list  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  [{'all_awardings': [], 'allow_live_comments': ...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen2 = api.search_submissions(\n",
    "    filter=[ 'id', 'url', 'title', 'subreddit', 'subreddit_id', 'subreddit_subscribers',\n",
    "        'num_crossposts', 'crosspost_parent', 'crosspost_parent_list', 'created_utc', 'author'],\n",
    "    limit= 100)\n",
    "\n",
    "temp = pd.DataFrame([thing for thing in gen2])\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [ting for ting in gen2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp = pd.DataFrame([thing for thing in gen2])\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/Users/jakobschlierf/Desktop/Master/Thesis/Files/test0606__cross.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_id</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>crosspost_parent_subs</th>\n",
       "      <th>count</th>\n",
       "      <th>crosspost_parent_num</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CovidVaccinated</td>\n",
       "      <td>t5_3gppqs</td>\n",
       "      <td>MapPorn</td>\n",
       "      <td>t5_2si92</td>\n",
       "      <td>4351.0</td>\n",
       "      <td>1252559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CovidVaccinated</td>\n",
       "      <td>t5_3gppqs</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "      <td>4196.0</td>\n",
       "      <td>15347693</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DebateVaccines</td>\n",
       "      <td>t5_3jwf7</td>\n",
       "      <td>AntiVaxxers</td>\n",
       "      <td>t5_384mz</td>\n",
       "      <td>2354.5</td>\n",
       "      <td>27397</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DebateVaccines</td>\n",
       "      <td>t5_3jwf7</td>\n",
       "      <td>ConspiracyUltra</td>\n",
       "      <td>t5_3flq3y</td>\n",
       "      <td>2711.869565</td>\n",
       "      <td>236</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DebateVaccines</td>\n",
       "      <td>t5_3jwf7</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>t5_2x4yx</td>\n",
       "      <td>2585.5</td>\n",
       "      <td>2365057</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subreddit subreddit_id crosspost_parent crosspost_parent_id  \\\n",
       "0  CovidVaccinated    t5_3gppqs          MapPorn            t5_2si92   \n",
       "1  CovidVaccinated    t5_3gppqs  dataisbeautiful            t5_2tk95   \n",
       "2   DebateVaccines     t5_3jwf7      AntiVaxxers            t5_384mz   \n",
       "3   DebateVaccines     t5_3jwf7  ConspiracyUltra           t5_3flq3y   \n",
       "4   DebateVaccines     t5_3jwf7      Coronavirus            t5_2x4yx   \n",
       "\n",
       "  subreddit_subscribers crosspost_parent_subs count crosspost_parent_num total  \n",
       "0                4351.0               1252559     1                    1     1  \n",
       "1                4196.0              15347693     1                    2     1  \n",
       "2                2354.5                 27397     2                   26     2  \n",
       "3           2711.869565                   236    23                   23    23  \n",
       "4                2585.5               2365057     4                    5    12  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_child</th>\n",
       "      <th>total</th>\n",
       "      <th>crosspost_parent_%</th>\n",
       "      <th>crosspost_child_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>857</td>\n",
       "      <td>10</td>\n",
       "      <td>2393</td>\n",
       "      <td>0.358128</td>\n",
       "      <td>0.004179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATBGE</td>\n",
       "      <td>14</td>\n",
       "      <td>408</td>\n",
       "      <td>2400</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ActualPublicFreakouts</td>\n",
       "      <td>823</td>\n",
       "      <td>49</td>\n",
       "      <td>2382</td>\n",
       "      <td>0.345508</td>\n",
       "      <td>0.020571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AgainstHateSubreddits</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1908</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.003669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlternativeHealth</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               subreddit  crosspost_parent  crosspost_child  total  \\\n",
       "0        ABoringDystopia               857               10   2393   \n",
       "1                  ATBGE                14              408   2400   \n",
       "2  ActualPublicFreakouts               823               49   2382   \n",
       "3  AgainstHateSubreddits                 1                7   1908   \n",
       "4      AlternativeHealth                42                0    245   \n",
       "\n",
       "   crosspost_parent_%  crosspost_child_%  \n",
       "0            0.358128           0.004179  \n",
       "1            0.005833           0.170000  \n",
       "2            0.345508           0.020571  \n",
       "3            0.000524           0.003669  \n",
       "4            0.171429           0.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('/Users/jakobschlierf/Desktop/Master/Thesis/Files/test0606__ratio_temp2')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_id</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>crosspost_parent_subs</th>\n",
       "      <th>count</th>\n",
       "      <th>crosspost_parent_num</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13or30</td>\n",
       "      <td>t5_3mzz4</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>t5_2qh6p</td>\n",
       "      <td>406093.0</td>\n",
       "      <td>717882</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1950s</td>\n",
       "      <td>t5_2suyd</td>\n",
       "      <td>interestingasfuck</td>\n",
       "      <td>t5_2qhsa</td>\n",
       "      <td>6798.0</td>\n",
       "      <td>7474756</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197</td>\n",
       "      <td>t5_3enhu</td>\n",
       "      <td>DankLeft</td>\n",
       "      <td>t5_11ga7z</td>\n",
       "      <td>16225.0</td>\n",
       "      <td>157790</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007scape</td>\n",
       "      <td>t5_2wbww</td>\n",
       "      <td>ThatsInsane</td>\n",
       "      <td>t5_swxxz</td>\n",
       "      <td>557307.0</td>\n",
       "      <td>862697</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007scape</td>\n",
       "      <td>t5_2wbww</td>\n",
       "      <td>funny</td>\n",
       "      <td>t5_2qh33</td>\n",
       "      <td>550208.0</td>\n",
       "      <td>35286028</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit subreddit_id   crosspost_parent crosspost_parent_id  \\\n",
       "0     13or30     t5_3mzz4       Conservative            t5_2qh6p   \n",
       "1      1950s     t5_2suyd  interestingasfuck            t5_2qhsa   \n",
       "2        197     t5_3enhu           DankLeft           t5_11ga7z   \n",
       "3  2007scape     t5_2wbww        ThatsInsane            t5_swxxz   \n",
       "4  2007scape     t5_2wbww              funny            t5_2qh33   \n",
       "\n",
       "   subreddit_subscribers  crosspost_parent_subs  count crosspost_parent_num  \\\n",
       "0               406093.0                 717882      1                    2   \n",
       "1                 6798.0                7474756      1                    3   \n",
       "2                16225.0                 157790      5                    5   \n",
       "3               557307.0                 862697      1                   22   \n",
       "4               550208.0               35286028      1                   11   \n",
       "\n",
       "   total  \n",
       "0     89  \n",
       "1    781  \n",
       "2     63  \n",
       "3    164  \n",
       "4    146  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('/Users/jakobschlierf/Desktop/Master/Thesis/Files/test0606__cross_child.pickle')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4881, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4 = df4[(df4['crosspost_parent'].str.contains('u_') == False)]\n",
    "# df2 = df[(df['crosspost_parent'].str.startswith('u_') == False)]\n",
    "\n",
    "df2 = df[(df['crosspost_parent'].str.startswith('u_') == False) & (df['subreddit'].str.startswith('u_') == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4102, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['crosspost_parent_list'].notna()].reset_index()\n",
    "df2 = df2[df2['crosspost_parent_list'].str.len() != 0] #sometimes, this field contains an empty list\n",
    "\n",
    "df2['t'] = df2['crosspost_parent_list'].apply(lambda x: dict(x[0])) # Pull crosspost_from information from field 'crosspost_parent_list' (which is in a json format)\n",
    "df2['crosspost_parent'] = df2['t'].apply(lambda x: x['subreddit'])  # Using the field has the advantage that we can deal with deleted posts which we could not find using the reddit API.\n",
    "df2['crosspost_parent_id'] = df2['t'].apply(lambda x: x['subreddit_id'])\n",
    "df2['crosspost_parent_subs'] = df2['t'].apply(lambda x: x['subreddit_subscribers'] )\n",
    "df2['crosspost_parent_num'] = df2['t'].apply(lambda x: x['num_crossposts'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>t</th>\n",
       "      <th>crosspost_parent_id</th>\n",
       "      <th>crosspost_parent_subs</th>\n",
       "      <th>crosspost_parent_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>l9ncqc</td>\n",
       "      <td>https://i.redd.it/i214kiu1jqe61.jpg</td>\n",
       "      <td>Whats better than freezing to death</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514355</td>\n",
       "      <td>WhitePeopleTwitter</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612130867</td>\n",
       "      <td>doctornowzaradan</td>\n",
       "      <td>1.612124e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>t5_35n7t</td>\n",
       "      <td>2011831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>l9m64s</td>\n",
       "      <td>https://v.redd.it/28r4g0xudne61</td>\n",
       "      <td>How are those $2000 relief checks coming along?</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514354</td>\n",
       "      <td>NewDealAmerica</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'GLOBAL...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612127619</td>\n",
       "      <td>ProctorZeuss</td>\n",
       "      <td>1.612120e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'GLOBAL'...</td>\n",
       "      <td>t5_2xl4f0</td>\n",
       "      <td>43387</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>l9kxsi</td>\n",
       "      <td>https://i.redd.it/d0u3bosh3me61.jpg</td>\n",
       "      <td>Jimmy is a fucking badass</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514347</td>\n",
       "      <td>nextfuckinglevel</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'PREMIU...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612124273</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1.612117e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'PREMIUM...</td>\n",
       "      <td>t5_m0bnr</td>\n",
       "      <td>3622511</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>l9kltk</td>\n",
       "      <td>https://i.redd.it/kbybo0r1ppe61.jpg</td>\n",
       "      <td>School staff is one of the most important peop...</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514347</td>\n",
       "      <td>MadeMeSmile</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'GLOBAL...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612123409</td>\n",
       "      <td>AnoonymouseChocobo</td>\n",
       "      <td>1.612116e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'GLOBAL'...</td>\n",
       "      <td>t5_2uqcm</td>\n",
       "      <td>2852714</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>l9kepr</td>\n",
       "      <td>https://i.redd.it/k7scw0fiqpe61.jpg</td>\n",
       "      <td>Wtf children are not fined for late loans in m...</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514346</td>\n",
       "      <td>wholesomememes</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'GLOBAL...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612122905</td>\n",
       "      <td>Damroth</td>\n",
       "      <td>1.612116e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'GLOBAL'...</td>\n",
       "      <td>t5_3gcwj</td>\n",
       "      <td>8871710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      id                                  url  \\\n",
       "0      3  l9ncqc  https://i.redd.it/i214kiu1jqe61.jpg   \n",
       "1      5  l9m64s      https://v.redd.it/28r4g0xudne61   \n",
       "2      6  l9kxsi  https://i.redd.it/d0u3bosh3me61.jpg   \n",
       "3      7  l9kltk  https://i.redd.it/kbybo0r1ppe61.jpg   \n",
       "4      9  l9kepr  https://i.redd.it/k7scw0fiqpe61.jpg   \n",
       "\n",
       "                                               title        subreddit  \\\n",
       "0               Whats better than freezing to death  ABoringDystopia   \n",
       "1    How are those $2000 relief checks coming along?  ABoringDystopia   \n",
       "2                          Jimmy is a fucking badass  ABoringDystopia   \n",
       "3  School staff is one of the most important peop...  ABoringDystopia   \n",
       "4  Wtf children are not fined for late loans in m...  ABoringDystopia   \n",
       "\n",
       "  subreddit_id subreddit_subscribers    crosspost_parent  \\\n",
       "0     t5_3c6l1                514355  WhitePeopleTwitter   \n",
       "1     t5_3c6l1                514354      NewDealAmerica   \n",
       "2     t5_3c6l1                514347    nextfuckinglevel   \n",
       "3     t5_3c6l1                514347         MadeMeSmile   \n",
       "4     t5_3c6l1                514346      wholesomememes   \n",
       "\n",
       "                               crosspost_parent_list num_crossposts  \\\n",
       "0  [{'all_awardings': [], 'allow_live_comments': ...              0   \n",
       "1  [{'all_awardings': [{'award_sub_type': 'GLOBAL...              0   \n",
       "2  [{'all_awardings': [{'award_sub_type': 'PREMIU...              0   \n",
       "3  [{'all_awardings': [{'award_sub_type': 'GLOBAL...              0   \n",
       "4  [{'all_awardings': [{'award_sub_type': 'GLOBAL...              0   \n",
       "\n",
       "  created_utc              author       created  \\\n",
       "0  1612130867    doctornowzaradan  1.612124e+09   \n",
       "1  1612127619        ProctorZeuss  1.612120e+09   \n",
       "2  1612124273           [deleted]  1.612117e+09   \n",
       "3  1612123409  AnoonymouseChocobo  1.612116e+09   \n",
       "4  1612122905             Damroth  1.612116e+09   \n",
       "\n",
       "                                                   t crosspost_parent_id  \\\n",
       "0  {'all_awardings': [], 'allow_live_comments': F...            t5_35n7t   \n",
       "1  {'all_awardings': [{'award_sub_type': 'GLOBAL'...           t5_2xl4f0   \n",
       "2  {'all_awardings': [{'award_sub_type': 'PREMIUM...            t5_m0bnr   \n",
       "3  {'all_awardings': [{'award_sub_type': 'GLOBAL'...            t5_2uqcm   \n",
       "4  {'all_awardings': [{'award_sub_type': 'GLOBAL'...            t5_3gcwj   \n",
       "\n",
       "   crosspost_parent_subs  crosspost_parent_num  \n",
       "0                2011831                     1  \n",
       "1                  43387                     6  \n",
       "2                3622511                    25  \n",
       "3                2852714                     3  \n",
       "4                8871710                     1  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>t</th>\n",
       "      <th>crosspost_parent_id</th>\n",
       "      <th>crosspost_parent_subs</th>\n",
       "      <th>crosspost_parent_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>l9ncqc</td>\n",
       "      <td>https://i.redd.it/i214kiu1jqe61.jpg</td>\n",
       "      <td>Whats better than freezing to death</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514355</td>\n",
       "      <td>WhitePeopleTwitter</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612130867</td>\n",
       "      <td>doctornowzaradan</td>\n",
       "      <td>1.612124e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>t5_35n7t</td>\n",
       "      <td>2011831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>l9m64s</td>\n",
       "      <td>https://v.redd.it/28r4g0xudne61</td>\n",
       "      <td>How are those $2000 relief checks coming along?</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514354</td>\n",
       "      <td>NewDealAmerica</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'GLOBAL...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612127619</td>\n",
       "      <td>ProctorZeuss</td>\n",
       "      <td>1.612120e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'GLOBAL'...</td>\n",
       "      <td>t5_2xl4f0</td>\n",
       "      <td>43387</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>l9kxsi</td>\n",
       "      <td>https://i.redd.it/d0u3bosh3me61.jpg</td>\n",
       "      <td>Jimmy is a fucking badass</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514347</td>\n",
       "      <td>nextfuckinglevel</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'PREMIU...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612124273</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1.612117e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'PREMIUM...</td>\n",
       "      <td>t5_m0bnr</td>\n",
       "      <td>3622511</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>l9kltk</td>\n",
       "      <td>https://i.redd.it/kbybo0r1ppe61.jpg</td>\n",
       "      <td>School staff is one of the most important peop...</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514347</td>\n",
       "      <td>MadeMeSmile</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'GLOBAL...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612123409</td>\n",
       "      <td>AnoonymouseChocobo</td>\n",
       "      <td>1.612116e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'GLOBAL'...</td>\n",
       "      <td>t5_2uqcm</td>\n",
       "      <td>2852714</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>l9kepr</td>\n",
       "      <td>https://i.redd.it/k7scw0fiqpe61.jpg</td>\n",
       "      <td>Wtf children are not fined for late loans in m...</td>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>t5_3c6l1</td>\n",
       "      <td>514346</td>\n",
       "      <td>wholesomememes</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'GLOBAL...</td>\n",
       "      <td>0</td>\n",
       "      <td>1612122905</td>\n",
       "      <td>Damroth</td>\n",
       "      <td>1.612116e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'GLOBAL'...</td>\n",
       "      <td>t5_3gcwj</td>\n",
       "      <td>8871710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8787</th>\n",
       "      <td>95</td>\n",
       "      <td>ltoqoj</td>\n",
       "      <td>https://i.redd.it/tb78uxank0k61.jpg</td>\n",
       "      <td>Please join us #Condemns_the_terrorist_activit...</td>\n",
       "      <td>UnitedNations</td>\n",
       "      <td>t5_2s0h9</td>\n",
       "      <td>6426</td>\n",
       "      <td>myanmar</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1614434546</td>\n",
       "      <td>memelgi</td>\n",
       "      <td>1.614427e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>t5_2rt95</td>\n",
       "      <td>8938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8788</th>\n",
       "      <td>96</td>\n",
       "      <td>lto7a1</td>\n",
       "      <td>https://v.redd.it/scqr8x6vyyj61</td>\n",
       "      <td>It's no longer safe for us. She is just a frui...</td>\n",
       "      <td>UnitedNations</td>\n",
       "      <td>t5_2s0h9</td>\n",
       "      <td>6425</td>\n",
       "      <td>myanmar</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1614432753</td>\n",
       "      <td>memelgi</td>\n",
       "      <td>1.614426e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>t5_2rt95</td>\n",
       "      <td>8937</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8789</th>\n",
       "      <td>97</td>\n",
       "      <td>lto68o</td>\n",
       "      <td>https://i.redd.it/wi6qozresyj61.jpg</td>\n",
       "      <td>A pregnant woman was arrested by myanmar brutu...</td>\n",
       "      <td>UnitedNations</td>\n",
       "      <td>t5_2s0h9</td>\n",
       "      <td>6425</td>\n",
       "      <td>myanmar</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1614432653</td>\n",
       "      <td>memelgi</td>\n",
       "      <td>1.614425e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>t5_2rt95</td>\n",
       "      <td>8937</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8790</th>\n",
       "      <td>98</td>\n",
       "      <td>m5cvc5</td>\n",
       "      <td>/r/LibertarianPartyUSA/comments/m5cpqj/liberta...</td>\n",
       "      <td>Libertarian startup city Prspera has broken g...</td>\n",
       "      <td>GoldandBlack</td>\n",
       "      <td>t5_3ffci</td>\n",
       "      <td>79304</td>\n",
       "      <td>LibertarianPartyUSA</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1615783183</td>\n",
       "      <td>nixfu</td>\n",
       "      <td>1.615776e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>t5_3epv4</td>\n",
       "      <td>7285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8791</th>\n",
       "      <td>154</td>\n",
       "      <td>m3mte3</td>\n",
       "      <td>https://www.bloomberg.com/news/features/2021-0...</td>\n",
       "      <td>Libertarians Want to Make New Hampshire a Flyi...</td>\n",
       "      <td>GoldandBlack</td>\n",
       "      <td>t5_3ffci</td>\n",
       "      <td>79144</td>\n",
       "      <td>Libertyinourlifetime</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1615571010</td>\n",
       "      <td>JobDestroyer</td>\n",
       "      <td>1.615564e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>t5_iowve</td>\n",
       "      <td>2081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8698 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      id                                                url  \\\n",
       "0         3  l9ncqc                https://i.redd.it/i214kiu1jqe61.jpg   \n",
       "1         5  l9m64s                    https://v.redd.it/28r4g0xudne61   \n",
       "2         6  l9kxsi                https://i.redd.it/d0u3bosh3me61.jpg   \n",
       "3         7  l9kltk                https://i.redd.it/kbybo0r1ppe61.jpg   \n",
       "4         9  l9kepr                https://i.redd.it/k7scw0fiqpe61.jpg   \n",
       "...     ...     ...                                                ...   \n",
       "8787     95  ltoqoj                https://i.redd.it/tb78uxank0k61.jpg   \n",
       "8788     96  lto7a1                    https://v.redd.it/scqr8x6vyyj61   \n",
       "8789     97  lto68o                https://i.redd.it/wi6qozresyj61.jpg   \n",
       "8790     98  m5cvc5  /r/LibertarianPartyUSA/comments/m5cpqj/liberta...   \n",
       "8791    154  m3mte3  https://www.bloomberg.com/news/features/2021-0...   \n",
       "\n",
       "                                                  title        subreddit  \\\n",
       "0                  Whats better than freezing to death  ABoringDystopia   \n",
       "1       How are those $2000 relief checks coming along?  ABoringDystopia   \n",
       "2                             Jimmy is a fucking badass  ABoringDystopia   \n",
       "3     School staff is one of the most important peop...  ABoringDystopia   \n",
       "4     Wtf children are not fined for late loans in m...  ABoringDystopia   \n",
       "...                                                 ...              ...   \n",
       "8787  Please join us #Condemns_the_terrorist_activit...    UnitedNations   \n",
       "8788  It's no longer safe for us. She is just a frui...    UnitedNations   \n",
       "8789  A pregnant woman was arrested by myanmar brutu...    UnitedNations   \n",
       "8790  Libertarian startup city Prspera has broken g...     GoldandBlack   \n",
       "8791  Libertarians Want to Make New Hampshire a Flyi...     GoldandBlack   \n",
       "\n",
       "     subreddit_id subreddit_subscribers      crosspost_parent  \\\n",
       "0        t5_3c6l1                514355    WhitePeopleTwitter   \n",
       "1        t5_3c6l1                514354        NewDealAmerica   \n",
       "2        t5_3c6l1                514347      nextfuckinglevel   \n",
       "3        t5_3c6l1                514347           MadeMeSmile   \n",
       "4        t5_3c6l1                514346        wholesomememes   \n",
       "...           ...                   ...                   ...   \n",
       "8787     t5_2s0h9                  6426               myanmar   \n",
       "8788     t5_2s0h9                  6425               myanmar   \n",
       "8789     t5_2s0h9                  6425               myanmar   \n",
       "8790     t5_3ffci                 79304   LibertarianPartyUSA   \n",
       "8791     t5_3ffci                 79144  Libertyinourlifetime   \n",
       "\n",
       "                                  crosspost_parent_list num_crossposts  \\\n",
       "0     [{'all_awardings': [], 'allow_live_comments': ...              0   \n",
       "1     [{'all_awardings': [{'award_sub_type': 'GLOBAL...              0   \n",
       "2     [{'all_awardings': [{'award_sub_type': 'PREMIU...              0   \n",
       "3     [{'all_awardings': [{'award_sub_type': 'GLOBAL...              0   \n",
       "4     [{'all_awardings': [{'award_sub_type': 'GLOBAL...              0   \n",
       "...                                                 ...            ...   \n",
       "8787  [{'all_awardings': [], 'allow_live_comments': ...              0   \n",
       "8788  [{'all_awardings': [], 'allow_live_comments': ...              0   \n",
       "8789  [{'all_awardings': [], 'allow_live_comments': ...              0   \n",
       "8790  [{'all_awardings': [], 'allow_live_comments': ...              0   \n",
       "8791  [{'all_awardings': [], 'allow_live_comments': ...              0   \n",
       "\n",
       "     created_utc              author       created  \\\n",
       "0     1612130867    doctornowzaradan  1.612124e+09   \n",
       "1     1612127619        ProctorZeuss  1.612120e+09   \n",
       "2     1612124273           [deleted]  1.612117e+09   \n",
       "3     1612123409  AnoonymouseChocobo  1.612116e+09   \n",
       "4     1612122905             Damroth  1.612116e+09   \n",
       "...          ...                 ...           ...   \n",
       "8787  1614434546             memelgi  1.614427e+09   \n",
       "8788  1614432753             memelgi  1.614426e+09   \n",
       "8789  1614432653             memelgi  1.614425e+09   \n",
       "8790  1615783183               nixfu  1.615776e+09   \n",
       "8791  1615571010        JobDestroyer  1.615564e+09   \n",
       "\n",
       "                                                      t crosspost_parent_id  \\\n",
       "0     {'all_awardings': [], 'allow_live_comments': F...            t5_35n7t   \n",
       "1     {'all_awardings': [{'award_sub_type': 'GLOBAL'...           t5_2xl4f0   \n",
       "2     {'all_awardings': [{'award_sub_type': 'PREMIUM...            t5_m0bnr   \n",
       "3     {'all_awardings': [{'award_sub_type': 'GLOBAL'...            t5_2uqcm   \n",
       "4     {'all_awardings': [{'award_sub_type': 'GLOBAL'...            t5_3gcwj   \n",
       "...                                                 ...                 ...   \n",
       "8787  {'all_awardings': [], 'allow_live_comments': F...            t5_2rt95   \n",
       "8788  {'all_awardings': [], 'allow_live_comments': F...            t5_2rt95   \n",
       "8789  {'all_awardings': [], 'allow_live_comments': F...            t5_2rt95   \n",
       "8790  {'all_awardings': [], 'allow_live_comments': F...            t5_3epv4   \n",
       "8791  {'all_awardings': [], 'allow_live_comments': F...            t5_iowve   \n",
       "\n",
       "      crosspost_parent_subs  crosspost_parent_num  \n",
       "0                   2011831                     1  \n",
       "1                     43387                     6  \n",
       "2                   3622511                    25  \n",
       "3                   2852714                     3  \n",
       "4                   8871710                     1  \n",
       "...                     ...                   ...  \n",
       "8787                   8938                     1  \n",
       "8788                   8937                     1  \n",
       "8789                   8937                     1  \n",
       "8790                   7285                     1  \n",
       "8791                   2081                     1  \n",
       "\n",
       "[8698 rows x 17 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2[(df2['crosspost_parent'].str.contains('u_') == False)]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2['crosspost_parent'] != 'u_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.iloc[3]['crosspost_parent_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wh'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]['subreddit'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mcrosspost_parent_list\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39mdict\u001b[39;49m(x[\u001b[39m0\u001b[39;49m]))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4322'>4323</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4323'>4324</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4324'>4325</a>\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4327'>4328</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4328'>4329</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4329'>4330</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4330'>4331</a>\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4331'>4332</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4430'>4431</a>\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4431'>4432</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4432'>4433</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1077'>1078</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1078'>1079</a>\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1079'>1080</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1081'>1082</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1130'>1131</a>\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1131'>1132</a>\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1132'>1133</a>\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1133'>1134</a>\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1134'>1135</a>\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1135'>1136</a>\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1136'>1137</a>\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1137'>1138</a>\u001b[0m             values,\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1138'>1139</a>\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1139'>1140</a>\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1140'>1141</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1142'>1143</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1143'>1144</a>\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1144'>1145</a>\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1145'>1146</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb Cell 10'\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcrosspost_parent_list\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mdict\u001b[39m(x[\u001b[39m0\u001b[39;49m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "df['t'] = df['crosspost_parent_list'].apply(lambda x: dict(x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df23 = df[df['crosspost_parent'].notna()]\n",
    "# df23 = df23[df23['crosspost_parent'].apply(lambda x: x[0:1]) != 't3']\n",
    "# df23.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8792, 12)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df23.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000043?line=0'>1</a>\u001b[0m df4 \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39;49m\u001b[39mcrosspost_parent\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: x[\u001b[39m0\u001b[39;49m:\u001b[39m2\u001b[39;49m]) \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mu_\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000043?line=1'>2</a>\u001b[0m df4\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4322'>4323</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4323'>4324</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4324'>4325</a>\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4327'>4328</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4328'>4329</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4329'>4330</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4330'>4331</a>\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4331'>4332</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4430'>4431</a>\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4431'>4432</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/series.py?line=4432'>4433</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1077'>1078</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1078'>1079</a>\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1079'>1080</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1081'>1082</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1130'>1131</a>\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1131'>1132</a>\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1132'>1133</a>\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1133'>1134</a>\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1134'>1135</a>\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1135'>1136</a>\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1136'>1137</a>\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1137'>1138</a>\u001b[0m             values,\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1138'>1139</a>\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1139'>1140</a>\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1140'>1141</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1142'>1143</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1143'>1144</a>\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1144'>1145</a>\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/pandas/core/apply.py?line=1145'>1146</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb Cell 11'\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000043?line=0'>1</a>\u001b[0m df4 \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mcrosspost_parent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39;49m:\u001b[39m2\u001b[39;49m]) \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mu_\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000043?line=1'>2</a>\u001b[0m df4\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "df4 = df[df['crosspost_parent'].apply(lambda x: x[0:2]) != 'u_']\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3 = df2.groupby('subreddit').agg({'crosspost_parent': 'count', 'num_crossposts': lambda x: np.count_nonzero(x), 'subreddit_subscribers' : 'count'}).reset_index()\n",
    "df3 = df3.rename(columns ={'subreddit_subscribers':'total', 'num_crossposts': 'crosspost_child'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_child</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>u_rklokh</td>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Astuff</td>\n",
       "      <td>503</td>\n",
       "      <td>9</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>covidskepticscanada</td>\n",
       "      <td>347</td>\n",
       "      <td>15</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABoringDystopia</td>\n",
       "      <td>325</td>\n",
       "      <td>9</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>NEWPOLITIC</td>\n",
       "      <td>311</td>\n",
       "      <td>22</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>montreal</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>privacy</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>pcgaming</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>IAmA</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               subreddit  crosspost_parent  crosspost_child  total\n",
       "135             u_rklokh               900                0    900\n",
       "9                 Astuff               503                9    755\n",
       "103  covidskepticscanada               347               15    603\n",
       "0        ABoringDystopia               325                9    900\n",
       "57            NEWPOLITIC               311               22    900\n",
       "..                   ...               ...              ...    ...\n",
       "115             montreal                 0                3    897\n",
       "122              privacy                 0               13    891\n",
       "116                 news                 0               19    900\n",
       "120             pcgaming                 0                4    900\n",
       "47                  IAmA                 0               11    899\n",
       "\n",
       "[149 rows x 4 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.sort_values('crosspost_parent',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['crosspost_parent_%'] = df3['crosspost_parent'] / df3['total']\n",
    "df3['crosspost_child_%'] = df3['crosspost_child'] / df3['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20008680636389625"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['crosspost_parent_%'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019490443440403512"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['crosspost_child_%'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m urls:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=3'>4</a>\u001b[0m     gen2 \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39msearch_submissions(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=4'>5</a>\u001b[0m     url \u001b[39m=\u001b[39m i ,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=5'>6</a>\u001b[0m     \u001b[39mfilter\u001b[39m\u001b[39m=\u001b[39m[ \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubreddit\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubreddit_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubreddit_subscribers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=6'>7</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnum_crossposts\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcrosspost_parent\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcreated_utc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauthor\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=7'>8</a>\u001b[0m     limit \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m) \u001b[39m#limit to avoid going over the rate limit.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=8'>9</a>\u001b[0m     results2 \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(gen2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=9'>10</a>\u001b[0m     temp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([thing\u001b[39m.\u001b[39md_ \u001b[39mfor\u001b[39;00m thing \u001b[39min\u001b[39;00m results2])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000046?line=10'>11</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df, temp])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py:238\u001b[0m, in \u001b[0;36mPushshiftAPIMinimal._search\u001b[0;34m(self, kind, stop_condition, return_batch, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=235'>236</a>\u001b[0m endpoint \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{dataset}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{kind}\u001b[39;00m\u001b[39m/search\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(dataset\u001b[39m=\u001b[39mdataset, kind\u001b[39m=\u001b[39mkind)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=236'>237</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url\u001b[39m.\u001b[39mformat(endpoint\u001b[39m=\u001b[39mendpoint)\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=237'>238</a>\u001b[0m \u001b[39mfor\u001b[39;00m response \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_paging(url):\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=238'>239</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maggs\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m response:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=239'>240</a>\u001b[0m         \u001b[39myield\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39maggs\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py:215\u001b[0m, in \u001b[0;36mPushshiftAPIMinimal._handle_paging\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=211'>212</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(err_msg\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_results_per_request))\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=212'>213</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_nec_args(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpayload)\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=214'>215</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(url, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpayload)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=215'>216</a>\u001b[0m \u001b[39myield\u001b[39;00m data\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=216'>217</a>\u001b[0m \u001b[39mif\u001b[39;00m limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py:184\u001b[0m, in \u001b[0;36mPushshiftAPIMinimal._get\u001b[0;34m(self, url, payload)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=181'>182</a>\u001b[0m i\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=182'>183</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=183'>184</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, params\u001b[39m=\u001b[39;49mpayload, proxies\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproxies)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=184'>185</a>\u001b[0m     log\u001b[39m.\u001b[39minfo(response\u001b[39m.\u001b[39murl)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/psaw/PushshiftAPI.py?line=185'>186</a>\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mResponse status code: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m response\u001b[39m.\u001b[39mstatus_code)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py:75\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=64'>65</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=65'>66</a>\u001b[0m \n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=66'>67</a>\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=71'>72</a>\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=72'>73</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=74'>75</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m'\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=56'>57</a>\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=57'>58</a>\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=58'>59</a>\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=59'>60</a>\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/api.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=523'>524</a>\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=524'>525</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=525'>526</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=526'>527</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=527'>528</a>\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=528'>529</a>\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=530'>531</a>\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=641'>642</a>\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=643'>644</a>\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=644'>645</a>\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=646'>647</a>\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/sessions.py?line=647'>648</a>\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=437'>438</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=438'>439</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=439'>440</a>\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=440'>441</a>\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=441'>442</a>\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=442'>443</a>\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=443'>444</a>\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=444'>445</a>\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=445'>446</a>\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=446'>447</a>\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=447'>448</a>\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=448'>449</a>\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=449'>450</a>\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=450'>451</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=452'>453</a>\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=453'>454</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/requests/adapters.py?line=454'>455</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=699'>700</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=701'>702</a>\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=702'>703</a>\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=703'>704</a>\u001b[0m     conn,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=704'>705</a>\u001b[0m     method,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=705'>706</a>\u001b[0m     url,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=706'>707</a>\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=707'>708</a>\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=708'>709</a>\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=709'>710</a>\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=710'>711</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=712'>713</a>\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=713'>714</a>\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=714'>715</a>\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=715'>716</a>\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=716'>717</a>\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=443'>444</a>\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=444'>445</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=445'>446</a>\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=446'>447</a>\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=447'>448</a>\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=448'>449</a>\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=449'>450</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=450'>451</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=440'>441</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=441'>442</a>\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=442'>443</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=443'>444</a>\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=444'>445</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=445'>446</a>\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=446'>447</a>\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=447'>448</a>\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/site-packages/urllib3/connectionpool.py?line=448'>449</a>\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=1374'>1375</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=1375'>1376</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=1376'>1377</a>\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=1377'>1378</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=1378'>1379</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=317'>318</a>\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=318'>319</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=319'>320</a>\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=320'>321</a>\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=321'>322</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=279'>280</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=280'>281</a>\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=281'>282</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/http/client.py?line=282'>283</a>\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/socket.py?line=701'>702</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/socket.py?line=702'>703</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/socket.py?line=703'>704</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/socket.py?line=704'>705</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/socket.py?line=705'>706</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1236'>1237</a>\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1237'>1238</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1238'>1239</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1239'>1240</a>\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1240'>1241</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1241'>1242</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1242'>1243</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1096'>1097</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1097'>1098</a>\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1098'>1099</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1099'>1100</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/ssl.py?line=1100'>1101</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['id', 'url', 'title','subreddit', 'subreddit_id', 'subreddit_subscribers',\n",
    "        'num_crossposts', 'crosspost_parent', 'created_utc', 'author'])\n",
    "for i in urls:\n",
    "    gen2 = api.search_submissions(\n",
    "    url = i ,\n",
    "    filter=[ 'id', 'url', 'title', 'subreddit', 'subreddit_id', 'subreddit_subscribers',\n",
    "        'num_crossposts', 'crosspost_parent', 'created_utc', 'author'],\n",
    "    limit = 100) #limit to avoid going over the rate limit.\n",
    "    results2 = list(gen2)\n",
    "    temp = pd.DataFrame([thing.d_ for thing in results2])\n",
    "    df = pd.concat([df, temp])\n",
    "\n",
    "df2 = df[df['num_crossposts'] > 0].reset_index(drop=True)\n",
    "df3 = df[df['num_crossposts'] == 0].reset_index(drop=True)\n",
    "df3 = df3[df3['crosspost_parent'].notna()]\n",
    "df3['crosspost_parent']  = df3['crosspost_parent'].apply(lambda x: x[3:])\n",
    "df2.drop('crosspost_parent', axis=1, inplace=True)\n",
    "df2 = df2.rename(columns ={'subreddit':'crosspost_parent','subreddit_id': 'crosspost_parent_id', 'subreddit_subscribers': 'crosspost_parent_subs', 'num_crossposts': 'crosspost_parent_num'})\n",
    "df4 = df2.merge(df3, left_on='id', right_on='crosspost_parent', suffixes= ('','_y'))\n",
    "df5 = df4.groupby(['subreddit','subreddit_id','crosspost_parent', 'crosspost_parent_id']).agg({'subreddit_subscribers': 'mean', 'crosspost_parent_subs': 'mean' , 'author' : 'count', 'crosspost_parent_num': 'sum'}).reset_index()\n",
    "df5.rename(columns = {'author':'count',}, inplace = True)\n",
    "df5['crosspost_parent_subs'] = df5['crosspost_parent_subs'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4.to_pickle('../../Files/merge_test.pickle')\n",
    "df4 = pd.read_pickle('../../Files/test_child.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()\n",
    "df = df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['num_crossposts'] > 0].reset_index(drop=True)\n",
    "df3 = df[df['num_crossposts'] == 0].reset_index(drop=True)\n",
    "df3 = df3[df3['crosspost_parent'].notna()]\n",
    "df3['crosspost_parent']  = df3['crosspost_parent'].apply(lambda x: x[3:])\n",
    "df2.drop('crosspost_parent', axis=1, inplace=True)\n",
    "df2 = df2.rename(columns ={'subreddit':'crosspost_parent','subreddit_id': 'crosspost_parent_id', 'subreddit_subscribers': 'crosspost_parent_subs', 'num_crossposts': 'crosspost_parent_num'})\n",
    "df4 = df2.merge(df3, left_on='id', right_on='crosspost_parent', suffixes= ('','_y'))\n",
    "df5 = df4.groupby(['subreddit','subreddit_id','crosspost_parent', 'crosspost_parent_id']).agg({'subreddit_subscribers': 'mean', 'crosspost_parent_subs': 'mean' , 'author' : 'count', 'crosspost_parent_num': 'sum'}).reset_index()\n",
    "df5.rename(columns = {'author':'count',}, inplace = True)\n",
    "df5['crosspost_parent_subs'] = df5['crosspost_parent_subs'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df2.merge(df3, left_on='id', right_on='crosspost_parent', suffixes= ('','_y'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.groupby(['subreddit','subreddit_id','crosspost_parent', 'crosspost_parent_id']).agg({'subreddit_subscribers': 'mean', 'crosspost_parents_subs': 'mean' , 'author' : 'count', 'crosspost_parent_num': 'sum'}).reset_index()\n",
    "# df3.rename(columns = {'author':'count',}, inplace = True)\n",
    "# df3['crosspost_from_subs'] = df3['crosspost_from_subs'].astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit\n",
      "subreddit_id\n",
      "crosspost_parent\n",
      "crosspost_parent_id\n",
      "subreddit_subscribers\n",
      "crosspost_parent_subs\n",
      "count\n",
      "crosspost_parent_num\n"
     ]
    }
   ],
   "source": [
    "k = df5.columns\n",
    "for j in k:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fh/yf9jwszj6d5_1_xhfz_l6k000000gn/T/ipykernel_28294/726648991.py:2: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s.append(df5['crosspost_parent'].drop_duplicates())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         ABoringDystopia\n",
       "2            AconspiracyA\n",
       "3       AlternativeHealth\n",
       "5      Anarcho_Capitalism\n",
       "7        AnythingGoesNews\n",
       "              ...        \n",
       "163          vacci_nation\n",
       "164           vandwellers\n",
       "165              walkaway\n",
       "166     whatsreallygoinon\n",
       "167             worldnews\n",
       "Name: subreddit, Length: 126, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df5['subreddit'].drop_duplicates()\n",
    "s.append(df5['crosspost_parent'].drop_duplicates())\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABoringDystopia\n",
      "AconspiracyA\n",
      "AlternativeHealth\n",
      "Anarcho_Capitalism\n",
      "AnythingGoesNews\n",
      "Astuff\n",
      "BidenLies\n",
      "COVID\n",
      "CeticosRespostaCovid\n",
      "ChurchOfCOVID\n",
      "CoVaxSkeptical\n",
      "ConservativeMemes\n",
      "ConspiracyUltra\n",
      "CoronaKritiker\n",
      "CoronaNC\n",
      "CoronavirusCirclejerk\n",
      "CoronavirusColorado\n",
      "CoronavirusOntario\n",
      "CoronavirusOregon\n",
      "CoronavirusToronto\n",
      "CoronavirusUK\n",
      "CoronavirusUS\n",
      "CoronavirusVariants\n",
      "CovidIsACult\n",
      "CovidVaccinated\n",
      "DankLeft\n",
      "DebateVaccine\n",
      "DebateVaccines\n",
      "DescentIntoTyranny\n",
      "DrFauci\n",
      "EndTheLockdowns\n",
      "EnoughAntifaSpam\n",
      "FUCKCHINAVIRUS\n",
      "FuckFuckMasks\n",
      "GreatReject\n",
      "HillaryForPrison\n",
      "JustUnsubbed\n",
      "LeopardsAteMyFace\n",
      "LibertarianLGBTQ\n",
      "LockdownCriticalLeft\n",
      "LockdownSkepticismCAN\n",
      "LoveForDronePilots\n",
      "Mujico\n",
      "NEWPOLITIC\n",
      "NoNewNormal\n",
      "NoNewNormalBan\n",
      "NoNoNewNormal\n",
      "NoShitSherlock\n",
      "NolibsWatch\n",
      "OpenTheSchools\n",
      "PeoplesPartyofCanada\n",
      "Republican\n",
      "RepublicanValues\n",
      "RuralNewsNetwork\n",
      "SatanicRitualAbuse\n",
      "Slovakia\n",
      "SurprisingNoOne\n",
      "Thailand\n",
      "TheBeliefInstinct\n",
      "TheFightThatMatters\n",
      "TheGreatResistance\n",
      "ThePeoplesPower\n",
      "TopConspiracy\n",
      "TrueAntiVaccination\n",
      "TruthLeaks\n",
      "UnbiasedCanada\n",
      "Unite99percent\n",
      "UpliftingNews\n",
      "VaccineDiscussion\n",
      "VaccinePsychology\n",
      "WikiLeaks\n",
      "altnewz\n",
      "antimask\n",
      "antivaccine\n",
      "betternews\n",
      "bioethics\n",
      "brasilivre\n",
      "cnn\n",
      "conservatives\n",
      "conspiracies\n",
      "conspiracy\n",
      "conspiracy_commons\n",
      "conspiracyhub\n",
      "conspiro\n",
      "coronabr\n",
      "corruptscience\n",
      "covidmx\n",
      "covidskepticscanada\n",
      "czech\n",
      "de\n",
      "europe\n",
      "femacamps\n",
      "free_space\n",
      "indianews\n",
      "newsokuexp\n",
      "noagenda\n",
      "nottheonion\n",
      "politics\n",
      "prohealth\n",
      "sciencememes\n",
      "taiwan\n",
      "technology\n",
      "theculling\n",
      "u_Additional-Let-5102\n",
      "u_Blackthorn319\n",
      "u_Coletinho\n",
      "u_Death5talker451968\n",
      "u_Dyckia_Dude\n",
      "u_Groundbreaking_Depth\n",
      "u_Jdiaz1887\n",
      "u_MigueBoga\n",
      "u_Sevensevenseve7\n",
      "u_approvedppe\n",
      "u_axiom0083\n",
      "u_margretbullsworth\n",
      "u_potchichi\n",
      "u_rklokh\n",
      "u_theindependentonline\n",
      "u_upinthetrees1989\n",
      "uknews\n",
      "uwaterloo\n",
      "vacci_nation\n",
      "vandwellers\n",
      "walkaway\n",
      "whatsreallygoinon\n",
      "worldnews\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in s:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"../../Files/depth1.csv\", \n",
    "           s,\n",
    "           delimiter =\", \", \n",
    "           fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df5['subreddit'].drop_duplicates().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'drop_duplicates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000014?line=0'>1</a>\u001b[0m t\u001b[39m.\u001b[39mextend(df5[\u001b[39m'\u001b[39m\u001b[39mcrosspost_parent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdrop_duplicates()\u001b[39m.\u001b[39mto_list())\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000014?line=2'>3</a>\u001b[0m t\u001b[39m.\u001b[39;49mdrop_duplicates()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'drop_duplicates'"
     ]
    }
   ],
   "source": [
    "t = df5['subreddit'].drop_duplicates().to_list()\n",
    "t.extend(df5['crosspost_parent'].drop_duplicates().to_list())\n",
    "res = []\n",
    "for i in t:\n",
    "    if i not in res and i not in subreddits:\n",
    "        res.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subreddits = ['DebateVaccines', 'CovidVaccinated', 'Vaccine', 'Coronavirus', 'LockdownSkepticism', 'HermanCainAward', 'NoNewNormal']\n",
    "res = []\n",
    "for i in t:\n",
    "    if i not in res and i not in subreddits:\n",
    "        res.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 129\n"
     ]
    }
   ],
   "source": [
    "print(len(t), len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABoringDystopia',\n",
       " 'AconspiracyA',\n",
       " 'AlternativeHealth',\n",
       " 'Anarcho_Capitalism',\n",
       " 'AnythingGoesNews',\n",
       " 'Astuff',\n",
       " 'BidenLies',\n",
       " 'COVID',\n",
       " 'COVID19_support',\n",
       " 'CeticosRespostaCovid',\n",
       " 'ChurchOfCOVID',\n",
       " 'CoVaxSkeptical',\n",
       " 'ConservativeMemes',\n",
       " 'ConspiracyUltra',\n",
       " 'CoronaKritiker',\n",
       " 'CoronaNC',\n",
       " 'CoronavirusCirclejerk',\n",
       " 'CoronavirusColorado',\n",
       " 'CoronavirusOntario',\n",
       " 'CoronavirusOregon',\n",
       " 'CoronavirusToronto',\n",
       " 'CoronavirusUK',\n",
       " 'CoronavirusUS',\n",
       " 'CoronavirusVariants',\n",
       " 'CovidIsACult',\n",
       " 'DankLeft',\n",
       " 'DebateVaccine',\n",
       " 'DescentIntoTyranny',\n",
       " 'DrFauci',\n",
       " 'EndTheLockdowns',\n",
       " 'EnoughAntifaSpam',\n",
       " 'FUCKCHINAVIRUS',\n",
       " 'FuckFuckMasks',\n",
       " 'GoldandBlack',\n",
       " 'GreatReject',\n",
       " 'Health',\n",
       " 'HillaryForPrison',\n",
       " 'JustUnsubbed',\n",
       " 'LeopardsAteMyFace',\n",
       " 'Liberate_Canada',\n",
       " 'LibertarianLGBTQ',\n",
       " 'LockdownCriticalLeft',\n",
       " 'LockdownSkepticismCAN',\n",
       " 'LoveForDronePilots',\n",
       " 'Mujico',\n",
       " 'NEWPOLITIC',\n",
       " 'NoNewNormalBan',\n",
       " 'NoNoNewNormal',\n",
       " 'NoShitSherlock',\n",
       " 'NolibsWatch',\n",
       " 'OpenTheSchools',\n",
       " 'PeoplesPartyofCanada',\n",
       " 'Republican',\n",
       " 'RepublicanValues',\n",
       " 'RuralNewsNetwork',\n",
       " 'SatanicRitualAbuse',\n",
       " 'Slovakia',\n",
       " 'SurprisingNoOne',\n",
       " 'Thailand',\n",
       " 'TheBeliefInstinct',\n",
       " 'TheFightThatMatters',\n",
       " 'TheGreatResistance',\n",
       " 'ThePeoplesPower',\n",
       " 'TopConspiracy',\n",
       " 'TrueAntiVaccination',\n",
       " 'TruthLeaks',\n",
       " 'UnbiasedCanada',\n",
       " 'Unite99percent',\n",
       " 'UpliftingNews',\n",
       " 'VaccineDiscussion',\n",
       " 'VaccinePsychology',\n",
       " 'WikiLeaks',\n",
       " 'altnewz',\n",
       " 'antimask',\n",
       " 'antivaccine',\n",
       " 'betternews',\n",
       " 'bioethics',\n",
       " 'brasilivre',\n",
       " 'cnn',\n",
       " 'conservatives',\n",
       " 'conspiracies',\n",
       " 'conspiracy',\n",
       " 'conspiracy_commons',\n",
       " 'conspiracyhub',\n",
       " 'conspiro',\n",
       " 'coronabr',\n",
       " 'corruptscience',\n",
       " 'covidmx',\n",
       " 'covidskepticscanada',\n",
       " 'czech',\n",
       " 'de',\n",
       " 'europe',\n",
       " 'femacamps',\n",
       " 'free_space',\n",
       " 'indianews',\n",
       " 'newsokuexp',\n",
       " 'noagenda',\n",
       " 'nottheonion',\n",
       " 'politics',\n",
       " 'privacy',\n",
       " 'prohealth',\n",
       " 'science',\n",
       " 'sciencememes',\n",
       " 'taiwan',\n",
       " 'technology',\n",
       " 'theculling',\n",
       " 'u_Additional-Let-5102',\n",
       " 'u_Blackthorn319',\n",
       " 'u_Coletinho',\n",
       " 'u_Death5talker451968',\n",
       " 'u_Dyckia_Dude',\n",
       " 'u_Groundbreaking_Depth',\n",
       " 'u_Jdiaz1887',\n",
       " 'u_MigueBoga',\n",
       " 'u_Sevensevenseve7',\n",
       " 'u_approvedppe',\n",
       " 'u_axiom0083',\n",
       " 'u_margretbullsworth',\n",
       " 'u_potchichi',\n",
       " 'u_rklokh',\n",
       " 'u_theindependentonline',\n",
       " 'u_upinthetrees1989',\n",
       " 'uknews',\n",
       " 'uwaterloo',\n",
       " 'vacci_nation',\n",
       " 'vandwellers',\n",
       " 'walkaway',\n",
       " 'whatsreallygoinon',\n",
       " 'worldnews']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance(df): #calculate the importance (similar to tf-idf)\n",
    "    # OPEN\n",
    "    # - make work with Crosspost from\n",
    "    imp = df.groupby(['crosspost_from']).agg({'subreddit': 'count', 'count': 'sum', 'crosspost_from_num' : 'sum'})\n",
    "    imp = imp.rename(columns ={'count': 'total'}) \n",
    "    imp.drop(['subreddit'], axis=1, inplace=True)\n",
    "    df2 = df.join(imp, on='crosspost_from', how='left')\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle('../../Files/2021-01-02.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['crosspost_parent_list'].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>t</th>\n",
       "      <th>crosspost_from</th>\n",
       "      <th>crosspost_from_id</th>\n",
       "      <th>crosspost_from_subs</th>\n",
       "      <th>crosspost_from_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>DebateVaccines</td>\n",
       "      <td>t5_3jwf7</td>\n",
       "      <td>2347</td>\n",
       "      <td>t3_l9iiqp</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>1612117931</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1.612111e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>t5_2qh4r</td>\n",
       "      <td>1455036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>DebateVaccines</td>\n",
       "      <td>t5_3jwf7</td>\n",
       "      <td>2325</td>\n",
       "      <td>t3_l81oft</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'GLOBAL...</td>\n",
       "      <td>1612056522</td>\n",
       "      <td>doingbasiclifeprep</td>\n",
       "      <td>1.612049e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'GLOBAL'...</td>\n",
       "      <td>IAmA</td>\n",
       "      <td>t5_2qzb6</td>\n",
       "      <td>20780922</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>DebateVaccines</td>\n",
       "      <td>t5_3jwf7</td>\n",
       "      <td>2317</td>\n",
       "      <td>t3_l7apag</td>\n",
       "      <td>[{'all_awardings': [{'award_sub_type': 'GLOBAL...</td>\n",
       "      <td>1611915334</td>\n",
       "      <td>EuCleo</td>\n",
       "      <td>1.611908e+09</td>\n",
       "      <td>{'all_awardings': [{'award_sub_type': 'GLOBAL'...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>t5_2x4yx</td>\n",
       "      <td>2363477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>DebateVaccines</td>\n",
       "      <td>t5_3jwf7</td>\n",
       "      <td>2314</td>\n",
       "      <td>t3_l77ln3</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>1611865305</td>\n",
       "      <td>dannylenwinn</td>\n",
       "      <td>1.611858e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>ForUnitedStates</td>\n",
       "      <td>t5_2i3wxn</td>\n",
       "      <td>2757</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>DebateVaccines</td>\n",
       "      <td>t5_3jwf7</td>\n",
       "      <td>2314</td>\n",
       "      <td>t3_l746cr</td>\n",
       "      <td>[{'all_awardings': [], 'allow_live_comments': ...</td>\n",
       "      <td>1611857406</td>\n",
       "      <td>dannylenwinn</td>\n",
       "      <td>1.611850e+09</td>\n",
       "      <td>{'all_awardings': [], 'allow_live_comments': F...</td>\n",
       "      <td>ForUnitedStates</td>\n",
       "      <td>t5_2i3wxn</td>\n",
       "      <td>2754</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       subreddit subreddit_id subreddit_subscribers crosspost_parent  \\\n",
       "0      0  DebateVaccines     t5_3jwf7                  2347        t3_l9iiqp   \n",
       "1      3  DebateVaccines     t5_3jwf7                  2325        t3_l81oft   \n",
       "2     15  DebateVaccines     t5_3jwf7                  2317        t3_l7apag   \n",
       "3     17  DebateVaccines     t5_3jwf7                  2314        t3_l77ln3   \n",
       "4     18  DebateVaccines     t5_3jwf7                  2314        t3_l746cr   \n",
       "\n",
       "                               crosspost_parent_list created_utc  \\\n",
       "0  [{'all_awardings': [], 'allow_live_comments': ...  1612117931   \n",
       "1  [{'all_awardings': [{'award_sub_type': 'GLOBAL...  1612056522   \n",
       "2  [{'all_awardings': [{'award_sub_type': 'GLOBAL...  1611915334   \n",
       "3  [{'all_awardings': [], 'allow_live_comments': ...  1611865305   \n",
       "4  [{'all_awardings': [], 'allow_live_comments': ...  1611857406   \n",
       "\n",
       "               author       created  \\\n",
       "0           [deleted]  1.612111e+09   \n",
       "1  doingbasiclifeprep  1.612049e+09   \n",
       "2              EuCleo  1.611908e+09   \n",
       "3        dannylenwinn  1.611858e+09   \n",
       "4        dannylenwinn  1.611850e+09   \n",
       "\n",
       "                                                   t   crosspost_from  \\\n",
       "0  {'all_awardings': [], 'allow_live_comments': F...       conspiracy   \n",
       "1  {'all_awardings': [{'award_sub_type': 'GLOBAL'...             IAmA   \n",
       "2  {'all_awardings': [{'award_sub_type': 'GLOBAL'...      Coronavirus   \n",
       "3  {'all_awardings': [], 'allow_live_comments': F...  ForUnitedStates   \n",
       "4  {'all_awardings': [], 'allow_live_comments': F...  ForUnitedStates   \n",
       "\n",
       "  crosspost_from_id  crosspost_from_subs  crosspost_from_num  \n",
       "0          t5_2qh4r              1455036                   1  \n",
       "1          t5_2qzb6             20780922                   1  \n",
       "2          t5_2x4yx              2363477                   1  \n",
       "3         t5_2i3wxn                 2757                  26  \n",
       "4         t5_2i3wxn                 2754                  17  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = reddit.submission('t3_p8ibgd')\n",
    "submission.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000025?line=0'>1</a>\u001b[0m r[\u001b[39m'\u001b[39;49m\u001b[39mcrosspost_parent_list\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "r['crosspost_parent_list'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2['crosspost_parent'].notna()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000027?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m df2\u001b[39m.\u001b[39miterrows(): \u001b[39m# Pull crosspost_from information from field 'crosspost_parent_list' (which is in a json format)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000027?line=1'>2</a>\u001b[0m     \u001b[39m# Using the field has the advantage that we can deal with deleted posts which we could not find using the reddit API.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000027?line=2'>3</a>\u001b[0m     t \u001b[39m=\u001b[39m r[\u001b[39m'\u001b[39;49m\u001b[39mcrosspost_parent_list\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000027?line=3'>4</a>\u001b[0m     r[\u001b[39m'\u001b[39m\u001b[39mcrosspost_from\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m t[\u001b[39m'\u001b[39m\u001b[39msubreddit\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000027?line=4'>5</a>\u001b[0m     r[\u001b[39m'\u001b[39m\u001b[39mcrosspost_from_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m t[\u001b[39m'\u001b[39m\u001b[39msubreddit_id\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "df2.to_pickle(f'../../Files/test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base36encode(number, alphabet='0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n",
    "    \"\"\"Converts an integer to a base36 string.\"\"\"\n",
    "    if not isinstance(number, (int)):\n",
    "        raise TypeError('number must be an integer')\n",
    "\n",
    "    base36 = ''\n",
    "    sign = ''\n",
    "\n",
    "    if number < 0:\n",
    "        sign = '-'\n",
    "        number = -number\n",
    "\n",
    "    if 0 <= number < len(alphabet):\n",
    "        return sign + alphabet[number]\n",
    "\n",
    "    while number != 0:\n",
    "        number, i = divmod(number, len(alphabet))\n",
    "        base36 = alphabet[i] + base36\n",
    "\n",
    "    return sign + base36\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime_obj = datetime.datetime.fromtimestamp(1642024930).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(datetime_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=spark_test, master=local[4]) created by __init__ at /tmp/ipykernel_320226/2751826978.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/3147567/Thesis/test.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/test.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf, SparkContext \n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/test.ipynb#ch0000011vscode-remote?line=1'>2</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext(\u001b[39m\"\u001b[39;49m\u001b[39mlocal[4]\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mspark_test\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=138'>139</a>\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=139'>140</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=140'>141</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=141'>142</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=143'>144</a>\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=144'>145</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=146'>147</a>\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m~/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py:342\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=338'>339</a>\u001b[0m     callsite \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_callsite\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=340'>341</a>\u001b[0m     \u001b[39m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=341'>342</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=342'>343</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot run multiple SparkContexts at once; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=343'>344</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexisting SparkContext(app=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, master=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=344'>345</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m created by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=345'>346</a>\u001b[0m         \u001b[39m%\u001b[39m (currentAppName, currentMaster,\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=346'>347</a>\u001b[0m             callsite\u001b[39m.\u001b[39mfunction, callsite\u001b[39m.\u001b[39mfile, callsite\u001b[39m.\u001b[39mlinenum))\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=347'>348</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/3147567/.conda/envs/reddit_env/lib/python3.9/site-packages/pyspark/context.py?line=348'>349</a>\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=spark_test, master=local[4]) created by __init__ at /tmp/ipykernel_320226/2751826978.py:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext \n",
    "sc = SparkContext(\"local[4]\", 'spark_test')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"Files/RS_2005-07.bz2\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"archived\":true,\"author\":\"zaz\",\"author_flair_background_color\":null,\"author_flair_css_class\":null,\"author_flair_richtext\":[],\"author_flair_text\":null,\"author_flair_text_color\":null,\"author_flair_type\":\"text\",\"brand_safe\":true,\"can_gild\":true,\"contest_mode\":false,\"created_utc\":1120181161,\"distinguished\":null,\"domain\":\"cnn.com\",\"edited\":false,\"gilded\":0,\"hidden\":false,\"hide_score\":false,\"id\":\"204\",\"is_crosspostable\":true,\"is_reddit_media_domain\":false,\"is_self\":false,\"is_video\":false,\"link_flair_css_class\":null,\"link_flair_richtext\":[],\"link_flair_text\":null,\"link_flair_text_color\":\"dark\",\"link_flair_type\":\"text\",\"locked\":false,\"media\":null,\"media_embed\":{},\"no_follow\":false,\"num_comments\":0,\"num_crossposts\":0,\"over_18\":false,\"parent_whitelist_status\":\"all_ads\",\"permalink\":\"\\\\/r\\\\/reddit.com\\\\/comments\\\\/204\\\\/cnncom_dj_admits_false_tale_about_missing_teen\\\\/\",\"rte_mode\":\"markdown\",\"score\":4,\"secure_media\":null,\"secure_media_embed\":{},\"selftext\":\"\",\"send_replies\":true,\"spoiler\":false,\"stickied\":false,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"subreddit_name_prefixed\":\"r\\\\/reddit.com\",\"subreddit_type\":\"archived\",\"suggested_sort\":null,\"thumbnail\":\"default\",\"thumbnail_height\":null,\"thumbnail_width\":null,\"title\":\"CNN.com - DJ admits false tale about missing teen - Jun 30, 2005\",\"url\":\"http:\\\\/\\\\/www.cnn.com\\\\/2005\\\\/LAW\\\\/06\\\\/30\\\\/aruba.missing\\\\/index.html\",\"whitelist_status\":\"all_ads\"}']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lines.map(lambda x: x[0]).take(1)\n",
    "lines.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-11:\n",
      "Process SpawnPoolWorker-10:\n",
      "Process SpawnPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "Traceback (most recent call last):\n",
      "AttributeError: Can't get attribute 'f' on <module '__main__' (built-in)>\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "AttributeError: Can't get attribute 'f' on <module '__main__' (built-in)>\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/jakobschlierf/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'f' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb Cell 62'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000061?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m*\u001b[39mx\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000061?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m Pool(\u001b[39m5\u001b[39m) \u001b[39mas\u001b[39;00m p:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jakobschlierf/Desktop/Master/Thesis/Github/Test_run/test.ipynb#ch0000061?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(p\u001b[39m.\u001b[39;49mmap(f, [\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m]))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reddit_env_test/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "\n",
    "with Pool(5) as p:\n",
    "    print(p.map(f, [1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "p = pstats.Stats('../../Files/logs/Preproc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 23 17:57:46 2022    ../../Files/logs/Preproc\n",
      "\n",
      "         10130611 function calls (9901322 primitive calls) in 24.725 seconds\n",
      "\n",
      "   Ordered by: function name\n",
      "   List reduced from 17795 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 {blis.cy.init}\n",
      "     3674    0.000    0.000    0.000    0.000 {built-in method _CheckCalledFromGeneratedFile}\n",
      "    19383    0.043    0.000    0.044    0.000 {built-in method __new__ of type object at 0x10b5b1a50}\n",
      "     1252    0.007    0.000    0.007    0.000 {built-in method _abc._abc_init}\n",
      "     3952    0.003    0.000    0.006    0.000 {built-in method _abc._abc_instancecheck}\n",
      "       80    0.000    0.000    0.001    0.000 {built-in method _abc._abc_register}\n",
      " 3181/880    0.006    0.000    0.007    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _abc.get_cache_token}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method _bisect.insort_right}\n",
      "     6348    0.008    0.000    0.008    0.000 {built-in method _codecs.lookup}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7fe4e9434b80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sort_stats('name').print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 23 17:57:46 2022    ../../Files/logs/Preproc\n",
      "\n",
      "         10130611 function calls (9901322 primitive calls) in 24.725 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 17795 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "  359/355    5.343    0.015    5.352    0.015 {built-in method _imp.create_dynamic}\n",
      "     4448    3.471    0.001    3.471    0.001 {built-in method io.open_code}\n",
      "     8685    1.104    0.000    1.123    0.000 thinc/backends/numpy_ops.pyx:90(gemm)\n",
      "    18772    0.810    0.000    0.810    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "     4448    0.497    0.000    0.497    0.000 {built-in method marshal.loads}\n",
      "     2320    0.460    0.000    4.773    0.002 spacy/pipeline/trainable_pipe.pyx:40(__call__)\n",
      "        1    0.449    0.449    0.449    0.449 {built-in method mkl._py_mkl_service.get_version}\n",
      "     6890    0.411    0.000    0.531    0.000 {built-in method io.open}\n",
      "    37346    0.397    0.000    0.398    0.000 {built-in method posix.stat}\n",
      "        3    0.382    0.127    0.382    0.127 {built-in method _ctypes.dlopen}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7fe4e9434b80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sort_stats('tottime').print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('../../Files/Submissions/profile_target/AwakenTheSheep.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     television input affect immediate mindframe hu...\n",
       "1     importance daily affirmation success   jack ca...\n",
       "2     censorware internet news content personalize t...\n",
       "3                      american city shell company rich\n",
       "4                january [ NUM]th capitol protest stage\n",
       "5     fight right let deep state manipulate voice st...\n",
       "6                            patient cure customer lose\n",
       "7      funny thing people scoff mere idea actually true\n",
       "8            psa unity amp love powerful force universe\n",
       "9                         elite sacrifice animal moloch\n",
       "10                        elite sacrifice animal moloch\n",
       "11                            u_p[num]tr[num]ot suspend\n",
       "12    illuminati want exterminate [ NUM]rd populatio...\n",
       "13                               statism mental illness\n",
       "14                                            mean like\n",
       "15            join rantianimalabuseritual raconspiracya\n",
       "16    recommendationadd post flair animal sacrificea...\n",
       "17    recommendationif rawakenthesheep member micros...\n",
       "18                             china fauci origin covid\n",
       "19    bet sub popular famousthe mainstream medium st...\n",
       "20                 completely exterminate animal abuser\n",
       "21                                       think know lie\n",
       "22                        official rule rawakenthesheep\n",
       "23                 indoctrination powerful tool control\n",
       "24    recommendation moderatoradd post write officia...\n",
       "25                                     ignorance strong\n",
       "26                       free people not need tell free\n",
       "27                              sacrifice animal moloch\n",
       "28                                         common sense\n",
       "29                                     uhhhhwut ta fuck\n",
       "30                                     uhhhhwut ta fuck\n",
       "31                                         join new sub\n",
       "32                              legality equal morality\n",
       "33                             politician werk fer derp\n",
       "34                              wakey wakey egg n bakey\n",
       "35                                 willing join new sub\n",
       "36                                    creep need expose\n",
       "37    fly bit million mile life plane forever contra...\n",
       "38                              modern society nutshell\n",
       "39                           need flood savethechildren\n",
       "40                                              ve hear\n",
       "41              possible elite involve animal sacrifice\n",
       "42          federal reserve explain [ NUM ] minute need\n",
       "43    isaac kappy expose elite hollywood pedophile t...\n",
       "44    socialistscommunist claim support minority rig...\n",
       "45                                             argument\n",
       "46    follow main rule allow achieve goal [ num ] te...\n",
       "47                     obedience virtue ignorance bliss\n",
       "48         welcome critical thinker truther open minded\n",
       "49                               rawakenthesheep lounge\n",
       "50             abraham hick   powerful word feel worthy\n",
       "51                    watch organize create manifest yo\n",
       "52    abraham hicks   understand gift contrast blessing\n",
       "53                            happen utramatikthetrooth\n",
       "54    human stimulus response   [ NUM ] [ NUM ] live...\n",
       "55             abraham hicks   attract good instead bad\n",
       "56               consciousness come beautifully explain\n",
       "57    abraham hick   shift financial struggle abundance\n",
       "58                   abraham hick   stop explain belief\n",
       "Name: cleanTitle, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cleanTitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Identifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir):\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(\".pickle\") and os.path.getsize(os.path.join(dir, file)) > 459:\n",
    "            files.append(os.path.join(dir, file))\n",
    "    return files.sort()\n",
    "\n",
    "\n",
    "def identify_missing_files(dir, files):\n",
    "    missing_files = []\n",
    "    for file in files:\n",
    "        if not os.path.exists(os.path.join(dir, file)) or os.path.getsize(os.path.join(dir, file)) <= 459:\n",
    "            missing_files.append(file)\n",
    "    return missing_files\n",
    "\n",
    "\n",
    "def splittimeframe(subreddit, start, end, split): # splits the time into a series of smaller files to pull\n",
    "    split_list = []\n",
    "    step = (end - start) / split\n",
    "    for i in range(split):\n",
    "            s = int(start + i * step)\n",
    "            e = int((start + (i + 1) * step) - 1)\n",
    "            if i == split - 1:\n",
    "                    e += 86400\n",
    "            split_list.append(f'{subreddit}-{s}.pickle')\n",
    "\n",
    "    return split_list\n",
    "\n",
    "def find_existing_pulls(type, subreddits): #remove existing pulls from subreddits list\n",
    "    done = os.listdir(f'../../Files/{type}/score/')\n",
    "    done.append([e for e in os.listdir(f'../../Files/{type}/')])\n",
    "    for i in done:\n",
    "        done[done.index(i)] = i[:-7]\n",
    "    res = [i for i in subreddits if i not in done]\n",
    "    return res\n",
    "\n",
    "def merge_splits(files, subreddit, ptype): # merges the files down\n",
    "    merge_candidates = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            if os.path.getsize(f'../../Files/{ptype}/temp/{file}') > 459:\n",
    "                if file[:-18] == subreddit:\n",
    "                    merge_candidates.append(f'../../Files/{ptype}/temp/{file}')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    merge_candidates.sort()\n",
    "    if len(merge_candidates) == 24:\n",
    "        df = pd.concat([pd.read_pickle(candidate) for candidate in merge_candidates])\n",
    "        df.to_pickle(f'../../Files/{ptype}/{subreddit}.pickle')\n",
    "        print(f'{subreddit} merged')\n",
    "        for i in merge_candidates:\n",
    "            os.remove(i)\n",
    "    else:\n",
    "        print(f'{subreddit} has only {len(merge_candidates)} files to merge. Merging not possible.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/3147567/Thesis/github/Test_run/test.ipynb Cell 125\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/github/Test_run/test.ipynb#Y235sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(args\u001b[39m.\u001b[39msubreddits, newline\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/github/Test_run/test.ipynb#Y235sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(f)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsba/home/3147567/Thesis/github/Test_run/test.ipynb#Y235sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     subreddits \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(reader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "with open(args.subreddits, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    subreddits = list(reader)\n",
    "\n",
    "subreddits = list(chain.from_iterable(subreddits))\n",
    "\n",
    "if args.job == 'Identify':\n",
    "    subreddits_not_completed = find_existing_pulls(args.ptype, subreddits)\n",
    "\n",
    "    start = int(datetime.datetime(2020, 3, 1).timestamp())\n",
    "    end = int(datetime.datetime(2022, 3, 31).timestamp())\n",
    "    split = 24\n",
    "    missing_parts = []\n",
    "    for subreddit in subreddits_not_completed:\n",
    "        \n",
    "        subreddit_parts = splittimeframe(subreddit, start, end, split)\n",
    "        missing = identify_missing_files(f'../../Files/{args.ptype}/temp/', subreddit_parts)\n",
    "        for m in missing:\n",
    "            missing_parts.append(m)\n",
    "\n",
    "\n",
    "    file = open(f'../../Files/{args.ptype}/temp/missing.csv', \"w\")\n",
    "    writer = csv.writer(file, delimiter = \"\\n\")\n",
    "    for list_ in missing_parts:\n",
    "        writer.writerow([list_])\n",
    "    file.close()\n",
    "\n",
    "if args.job == 'Merge':\n",
    "    files = []\n",
    "    files = os.listdir(f'../../Files/{args.ptype}/temp/')\n",
    "    files.pop(files.index('missing.csv'))\n",
    "\n",
    "    t = [file[:-18] for file in files]\n",
    "\n",
    "    subreddits = []\n",
    "    for i in t:\n",
    "        if i not in subreddits:\n",
    "            subreddits.append(i)\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        merge_splits(files, subreddit, args.ptype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Preprocessing/subreddits_sm.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    subreddits = list(reader)\n",
    "\n",
    "subreddits = list(chain.from_iterable(subreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_not_completed = find_existing_pulls('Comments', subreddits)\n",
    "\n",
    "start = int(datetime.datetime(2020, 3, 1).timestamp())\n",
    "end = int(datetime.datetime(2022, 3, 31).timestamp())\n",
    "split = 24\n",
    "missing_parts = []\n",
    "for subreddit in subreddits_not_completed:\n",
    "    \n",
    "    subreddit_parts = splittimeframe(subreddit, start, end, split)\n",
    "    missing = identify_missing_files(f'../../Files/Comments/temp/', subreddit_parts)\n",
    "    for m in missing:\n",
    "        missing_parts.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_not_completed = find_existing_pulls('Comments', subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"Comments\"\n",
    "done = os.listdir(f'../../Files/{type}/score/')\n",
    "done.extend(os.listdir(f'../../Files/{type}/'))\n",
    "for i in done:\n",
    "    done[done.index(i)] = i[:-7]\n",
    "res = [i for i in subreddits if i not in done]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Government_is_lame',\n",
       " 'WikiLeaks',\n",
       " 'DemocraticSocialism',\n",
       " 'weddingshaming',\n",
       " 'collapse',\n",
       " 'InternationalLeft',\n",
       " 'aspiememes',\n",
       " 'TheFightThatMatters',\n",
       " 'LockdownCriticalLeft',\n",
       " 'Astuff',\n",
       " 'DebateVaccines',\n",
       " 'POTUSWatch',\n",
       " 'The_Ultimate',\n",
       " 'libertarianmeme',\n",
       " 'myanmar',\n",
       " 'LouderWithCrowder',\n",
       " 'Cringetopia',\n",
       " 'AncientTruehistory',\n",
       " 'AOC',\n",
       " 'blursedimages',\n",
       " 'ConservativeMemes',\n",
       " 'conspiracyNOPOL',\n",
       " 'GreenAndPleasant',\n",
       " 'QAnonCasualties',\n",
       " 'VACCINES',\n",
       " 'aww',\n",
       " 'ConspiracyUltra',\n",
       " 'LateStageImperialism',\n",
       " 'natureismetal',\n",
       " 'wisconsin',\n",
       " 'Labour',\n",
       " 'NoNoNewNormal',\n",
       " 'Slovakia',\n",
       " 'AutisticPride',\n",
       " 'altnewz',\n",
       " 'Sino',\n",
       " 'NEWPOLITIC',\n",
       " 'CPUSA',\n",
       " 'WomenInNews',\n",
       " 'worldnewsvideo',\n",
       " 'AmericanFascism2020',\n",
       " 'EcoNewsNetwork',\n",
       " 'OliverMarkusMalloy',\n",
       " 'Anarchism',\n",
       " 'ReallyAmerican',\n",
       " 'thanksihateit',\n",
       " 'NotoQanon',\n",
       " 'didnthappen',\n",
       " 'Africa',\n",
       " 'thedavidpakmanshow',\n",
       " 'KidsAreFuckingStupid',\n",
       " 'PositiveNewsCovid19',\n",
       " 'uninsurable',\n",
       " 'uspolitics',\n",
       " 'wokekids',\n",
       " 'Palestine',\n",
       " 'southafrica',\n",
       " 'CapitolConsequences',\n",
       " 'DeSantis',\n",
       " 'daverubin',\n",
       " 'AmericanPandemics',\n",
       " 'CoronavirusVariants',\n",
       " 'BadChoicesGoodStories',\n",
       " 'MurderedByAOC',\n",
       " 'noagenda',\n",
       " 'NoNewNormalBan',\n",
       " 'SocialJusticeInAction',\n",
       " 'The_Chocker',\n",
       " 'JordanPeterson',\n",
       " 'WeirdNews4U',\n",
       " 'anime_titties',\n",
       " 'conspiracyhub',\n",
       " 'BorderCollie',\n",
       " 'JustUnsubbed',\n",
       " 'InsaneParler',\n",
       " 'TexasPolitics',\n",
       " 'engrish',\n",
       " 'Anarcho_Capitalism',\n",
       " 'CoronavirusUS',\n",
       " 'theyknew',\n",
       " 'climateskeptics',\n",
       " 'CovidIsACult',\n",
       " 'PBS_NewsHour',\n",
       " 'labor',\n",
       " 'catsaysmao',\n",
       " 'VoluntaristMemes',\n",
       " 'Destiny',\n",
       " 'TheMajorityReport',\n",
       " 'ShitAmericansSay',\n",
       " 'InformedTankie',\n",
       " 'stupidpeoplefacebook',\n",
       " 'Fuckthealtright',\n",
       " 'CommunismMemes',\n",
       " 'MarchAgainstNazis',\n",
       " 'COMPLETEANARCHY',\n",
       " 'ToiletPaperUSA',\n",
       " 'ConservativesOnly',\n",
       " 'wisconsinpolitics',\n",
       " 'Economics',\n",
       " 'CityPorn',\n",
       " 'sciencememes',\n",
       " 'fragilecommunism',\n",
       " 'BreadTube',\n",
       " 'Makesmybloodboil',\n",
       " 'January6',\n",
       " 'AskWomenOfColorOver30',\n",
       " 'Political_Revolution',\n",
       " 'sydney',\n",
       " 'dontdeadopeninside',\n",
       " 'composernews',\n",
       " 'GAA',\n",
       " 'WhatTheTwitter',\n",
       " 'oldmaps',\n",
       " 'megafaunarewilding',\n",
       " 'NoNewNormal',\n",
       " '',\n",
       " 'worldnews247',\n",
       " 'VanLife',\n",
       " 'Archive.',\n",
       " 'CovidVaccinated',\n",
       " 'priorityP',\n",
       " 'islam',\n",
       " 'WalmartCelebrities',\n",
       " 'WMAWCBF',\n",
       " 'ClassPoliticsTwitter',\n",
       " 'Trumpgret',\n",
       " 'alberta',\n",
       " 'Etobicoke',\n",
       " 'StallmanWasRight',\n",
       " 'uspolicy',\n",
       " 'Awwducational',\n",
       " 'theyknew',\n",
       " 'vacci_nation',\n",
       " 'CovidVaccineInjury',\n",
       " '',\n",
       " 'onguardforthee',\n",
       " 'EndlessWar',\n",
       " 'AccidentalComedy',\n",
       " 'gay_irl',\n",
       " 'BeansInThings',\n",
       " 'enoughpetersonspam',\n",
       " 'LandlordLove',\n",
       " 'AreTheStraightsOK',\n",
       " 'Anticonsumption',\n",
       " 'mapporncirclejerk',\n",
       " 'GoldandBlack',\n",
       " 'ParentingWithoutFear',\n",
       " 'israelexposed',\n",
       " 'wholesome',\n",
       " 'EnoughAntifaSpam',\n",
       " 'Jung',\n",
       " 'dontyouknowwhoiam',\n",
       " 'EntitledBitch',\n",
       " 'tifu',\n",
       " 'TheHallOfShame',\n",
       " 'Politsturm',\n",
       " 'TrueAntiVaccination',\n",
       " '',\n",
       " 'Vaccine',\n",
       " 'Keep_Track',\n",
       " 'HermanCainAward',\n",
       " 'WtWBBot',\n",
       " 'TrollFaceIncident',\n",
       " 'oddlyspecific',\n",
       " 'criticalblunder',\n",
       " 'Edmonton']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../../Files/Comments/score/POTUSWatch.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('../../Files/Comments/score/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.remove('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd = []\n",
    "for file in files:\n",
    "    df = pd.read_pickle(os.path.join('../../Files/Comments/score/', file))\n",
    "    if 'cleanBody' not in df.columns:\n",
    "        tbd.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['myanmar.pickle']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('reddit_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7913a71c8e34b61ec22a3bd55dc7c3ae241fdb6cefe9aa5fb754c5121c0e8c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
