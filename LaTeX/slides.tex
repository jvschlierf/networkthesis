\documentclass[aspectratio=169]{beamer}
% Document metadata
\title{LDA: Latent Dirichlet Allocation}
\subtitle{20605: Machine Learning II \\ Final Presentation}
\author[JVS]{Jakob Schlierf: 3147567}
\institute{Universit√† Bocconi}
\date{18.01.2022}

% Image for the title page (use includegraphics option to properly size/place it)
\titlegraphic{\includegraphics[height=\paperheight]{library.jpg}}


\usetheme[sectionstyle=style2]{trigon}

% Define logos to use (comment if no logo)
\biglogo{Bocconi.jpg} % Used on titlepage only
\smalllogo{20160428_bocconi.jpg} % Used on top right corner of regular frames

% ------ If you want to change the theme default colors, do it here ------
\definecolor{tTheme}{HTML}{3F6FB6}   % Blue
\definecolor{tPrim}{HTML}{3F6FB6}   % Blue
\definecolor{tSec}{HTML}{393838}    % Grey
%\definecolor{tAccent}{HTML}{F07F3C} % Orange

\usepackage{appendixnumberbeamer} % To use \appendix command
\pdfstringdefDisableCommands{% Fix hyperref translate warning with \appendix
  \def\translate#1{#1}%
}
\usepackage{pgf-pie} % For pie charts
\usepackage{caption} % For subfigures
\usepackage{subcaption} % For subfigures
\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{trigon}}\xspace}
\usepackage[scale=2]{ccicons} % Icons for CC-BY-SA
\usepackage{booktabs} % Better tables
\usepackage{hyperref}
\usepackage{algorithm} 
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage[
backend=biber,
style=numeric,
sorting=ynt,
]{biblatex}

\addbibresource{bibliography.bib}


% \newif\ifframeinlbf
% \frameinlbftrue
% \makeatletter
% \newcommand\listofframes{\@starttoc{lbf}}
% \makeatother


% \addtobeamertemplate{frametitle}{}{%
%   \ifframeinlbf
%   \addcontentsline{lbf}{section}{\protect\makebox[2em][l]{%
%     \protect\usebeamercolor[fg]{structure}\insertframenumber\hfill}%
%   \insertframetitle\par}%
%   \else\fi
% }


\begin{document}
%--------------------------------------
% Create title frame
\titleframe{}

\begin{frame}{Table of Content}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents
\end{frame}

%==============================================
\section{Fundamentals}
%==============================================

\subsection{Motivation}
\begin{frame}{\insertsubsectionhead}
  \framesubtitle{Goal \& Previous Methods}
  
  \begin{itemize}
    \item Goal: Compare \& shortly describe collections of discrete data (e.g. text documents) \cite{LDA}
    \item Previous Methods such as \textbf{LSI} \cite{LSI}, \textbf{pLSI} \cite{PLSI} or \textbf{Mixture of Unigrams} \cite{Unigram} have shortcomings that make them not ideal
    \item Crucially, we don't know the descriptions beforehand or don't have labeled data to train a classifier. (Unsupervised Training)
    \item In the context of Natural Language Processing (NLP), this is called Topic Modeling 
  \end{itemize}
  \begin{exampleblock}{LDA}
    Latent Dirichlet Allocation proposed by Blei, Ng \& Jordan \cite{LDA} provides a method to solve this problem probabilistically. The method was first used (yet not named) by Pritchard, Stephens \& Donnelly \cite{BIOLDA} for genetic application.
  \end{exampleblock}
\end{frame}


\subsection{Notation \& Assumptions}
\begin{frame}{\insertsubsectionhead}
  
  \textbf{Notation}
  \begin{itemize} 
    \item Word: basic unit of discrete data from a vocabulary indexed by $w^n$ with $n \in \{1, \dots, V\}$, represented as unit-basis vector of the vocabulary.
    \item Document: sequence of $N$ words, $\bm{w} = \{w_1, w_2, \dots, w_N\}$
    \item Corpus: collection of $M$ documents, $D = \{\bm{w}_1, \bm{w}_2, \dots, \bm{w}_M\}$
  \end{itemize}
  \textbf{Assumptions}
  \begin{itemize}
    \item "Bag of Words": words, documents exchangeable (not words across documents!)
    \item Words are generated by a layered Bayesian Model with a latent "topics" variable 
  \end{itemize}
  \begin{exampleblock}{Note: Generalization}
    Applications other than text exist (such as in bioinformatics \cite{BIOLDA} \cite{liu2016overview} \cite{ldabioinformatics}); for intuition, text collection nomenclature will be used throughout this presentation.
  \end{exampleblock}
\end{frame}

%==============================================
\section{Generative Model}
%==============================================


\begin{frame}{\insertsectionhead}   
\framesubtitle{Base Model}
  
 In the most general case, LDA assumes this generative process for each $\textbf{w}$ in $D$:
  \begin{enumerate}
    \item Choose $N \sim Poisson(\xi)$ (Number of words in Document)
    \item Choose $\theta \sim  Dir(\alpha)$ (Topic Mixture)
    \item For each of the $N$ words $W_n$:
    \begin{enumerate}[(a)]
      \item Choose a topic $z_n \sim Multinomial(\theta)$ 
      \item Choose a word $w_n$ from $p(w_n| z_n, \beta) $
    \end{enumerate}
  \end{enumerate}  
  \textbf{Additional simplifying assumptions:} (in this simplified case)
  \begin{itemize}
    \item Dimensionality of Dirichlet (\& thus that of the topic variable $z$) is known \& fixed 
    \item Word probabilities param. by $k \times V$ matrix $\beta$, assumed fixed and to be estimated
    \item Poisson assumption not critical, $N$ independent of $\theta$ \& z
  \end{itemize} 
\end{frame}

\subsection{Base Model}
\begin{frame}{\insertsubsectionhead}
  \framesubtitle{Graphical Intuition}
  \begin{figure}
    \begin{center}
      \includegraphics[height=3cm]{LDA.png}
    \end{center}
    \caption{Graphic Representation of the base LDA Model \cite{LDA}}
  \end{figure}
  \begin{exampleblock}{Note}
    In the simplified case: ML-estimates $\beta^*$ assign 0 to a word not in the training vocabulary
  \end{exampleblock}
\end{frame}

\subsection{Smoothing}
\begin{frame}{\insertsubsectionhead}
  \framesubtitle{Extending the Base Model}
  Solution: Introduce an additional Dirichlet parameter $\eta$ that generates $\beta_i$
  \begin{figure}
    \begin{center}
      \includegraphics[height=3cm]{extended_LDA.png}
    \end{center}
    \caption{Graphic Representation of extended LDA Model \cite{LDA}}
  \end{figure}
  $\beta$ is now a $k \times V$ random matrix in that we resample, allowing for changing word/topic probabilities
\end{frame}

%==============================================
\section{Inference \& Estimation}
%==============================================



\begin{frame}{Inference}
  \framesubtitle{Base Case}
  \fontsize{10pt}{7pt}\selectfont
  % Talk about intractability & alternative approaches
  Using Bayes' Theorem, we want to infer the posterior (multinomial) distribution of topics for a given document $\textbf{w}$
    \begin{align*}  
        p(\theta, \textbf{z}|\textbf{w}, \alpha, \beta) &= \frac{p(\theta, \textbf{z}, \textbf{w}|\alpha, \beta)}{p(\textbf{w}|\alpha, \beta)}
        \\where\\
        p(\theta, \textbf{z}, \textbf{w}|\alpha, \beta)) &= p(\theta|\alpha)\prod^N_{n=1}p(z_n|\theta)p(w_n|z_n,\beta)
        \\and\\
        p(\textbf{w}|\alpha, \beta) &= \frac{\Gamma( \sum_i \alpha_i)}{\prod_i \Gamma(\alpha_i)} \int\left(\prod^k_{i=1} \theta_i^{\alpha_i-1}\right) \left(\prod^{N}_{n=1} \sum_{i = 1}^{k}\prod_{j=1}^V(\theta_i\beta_{ij})^{w^j_n}\right) \,d\theta    
    \end{align*} 
    However, $p(\textbf{w}|\alpha, \beta)$ is intractable due to the coupling between $\theta$ and $\beta$, can only be estimated using a variety of methods (e.g. MCMC, Laplace- or Variational Approximation).
\end{frame}


\begin{frame}{Parameter Estimation}
  \framesubtitle{Example using Variational Inference}

  Blei, Ng \& Jordan \cite{LDA} describe a variational inference approximation using an EM algorithm
  \begin{itemize}
    \item Main Idea: Obtain the tightest lower bound on the log-likelihood using variational parameters $\gamma$ \& $\phi$
    \item Estimate $(\gamma^*, \phi^* ) =\arg \min_{(\gamma, \phi)} D(q(\theta , \textbf{z} | \gamma, \phi) || p(\theta, \textbf{z} | \textbf{w}, \alpha, \beta)) $
    \item Using the derivative of the KL divergence, we obtain update equations for $\gamma_i$ and $\phi_{ni}$
    \item Lastly, using an Expectiation Maximization algorithm, we update both our variational- \& non-variational parameters
  \end{itemize}
  
  \begin{exampleblock}{Note}
    The Variational Inference is quite efficient, requiring $O((N + 1)k)$ operations per iteration.
  \end{exampleblock}
\end{frame}

\begin{frame}{Parameter Estimation (contd.)}
\framesubtitle{EM-Algorithm}
  \begin{itemize}
    \item E-Step: Calculate optimizing values for $\gamma_i$ and $\phi_{ni}$ for each document
    \begin{itemize}
      \item $\gamma_i = \alpha_i + \sum_{n=1}^N \phi_{ni}$
      \item $\phi_{ni} \propto \beta_{iw_n} \exp\{E_q[\log(\theta_i) |\gamma]\}$
      \item where $\exp\{E_q[\log(\theta_i) |\gamma]\} = \Psi(\gamma_i) - \Psi \left(\sum_{j=1}^k \gamma_j\right)$
    \end{itemize}
    \item M-Step: Maximize lower bound of the log-likelihood with respect to parameters $\alpha$ and $\beta$:
    \begin{itemize}
      \item $\beta_{ij} \propto \sum^{M}_{d=1}\sum^{N_d}_{n=1} \phi^*_{dni} w^{j}_{dn} $
      \item for $\alpha$ we use the Newton-Raphson method to obtain the update
    \end{itemize}
  \end{itemize}
  
\end{frame} 

%==============================================
\section{Implementation \& Outlook}
%==============================================


\begin{frame}{Implementation}
  \begin{figure}
    \begin{center}
      \includegraphics[width=15cm]{topics.png}
    \end{center}
    \caption{Implementation of a Topic Model (King James Bible - New Testament) according to \cite{hovy_2021}}
  \end{figure}
\end{frame}

\begin{frame}{Implementation (contd.)}
  \begin{figure}
    \begin{center}
      \includegraphics[width=15cm]{topicdifference.png}
    \end{center}
    \caption{Deviation from mean to illustrate differences}
  \end{figure}
\end{frame}

\begin{frame}{Outlook}
  \framesubtitle{}
  \begin{itemize}
    
    \item Paper by Blei, Ng \& Jordan \cite{LDA} has been cited by around 40,000 other papers (including 133 in 2022!)
    \item More than 40 variations of the original algorithm have been developed \cite{jelodar2019latent}
    \item Implementations in many languages (C++, Java, R, Matlab, Python) are easily accessible
    \item Along with LSI and it is the primary tool for Topic Modeling \cite{jelodar2019latent}
  \end{itemize}
  \begin{exampleblock}{Note}
    Latent Dirichlet Allocation remains a valuable tool for topic modeling in NLP and other areas
  \end{exampleblock}
\end{frame}

%==============================================
\section{Appendix}
%==============================================

\begin{frame}{Derivations}
  \framesubtitle{Generating Model}
  De Finetti's representation theorem is used in the bag of words assumption \\
  Distribution of $k$-dimensional Dirichlet Variable $\theta$
  \begin{center}
    \begin{math}
      p(\theta | \alpha) = \frac{\Gamma\left(\sum^{k}_{i=1} \alpha_i\right)}{\prod^k_{i=1}\Gamma(\alpha_i)}\theta^{\alpha_{1}-1}_1 \dots\theta_k^{\alpha_{k}-1}
    \end{math}
  \end{center}
  Marginal Distribution of a document $\textbf{w}$
  \begin{center}
    \begin{math}
      p(\textbf{w} | \alpha, \beta) = \int p(\theta | \alpha) \left(\prod^N_{n=1}\sum_{z_{n}}p(z_{n}|\theta)p(w_{n}|z_{n}, \beta)\right) \,d\theta  
    \end{math}
  \end{center}
  Probability of Corpus 
  \begin{center}
  $p(\textit{D} | \alpha, \beta) = \prod^{M}_{d=1}\int p(\theta | \alpha) \left(\prod^N_{n=1}\sum_{z_{dn}}p(z_{dn}|\theta_d)p(\textbf{w}_{dn}|z_{dn}, \beta)\right) \,d\theta$
  \end{center}
\end{frame}

\begin{frame}{Derivations (contd.)}
  \framesubtitle{Inference}
\begin{itemize}
  \item $p(\textbf{w}|\alpha, \beta)$ is intractable due coupling between $\theta$ and $\beta$ in the summation over the latent topics \cite{dickey1983multiple}
  \item Instead, we use Jensen's Inequality $g(E[X]) \leq E[g(X)]$ for a convex function g, then
  \item Consider a family of lower bounds, indexed by a set of variational parameters
  \item We optimize these variational parameters to achieve the tightest possible lower bound for the log-likelihood
\end{itemize}
\end{frame}

\begin{frame}{Derivations (contd.)}
  \framesubtitle{Parameter Maximization Procedure}
  \begin{itemize}
    \item $\log p(\textbf{w}| \alpha, \beta) = \mathcal{L} (\gamma, \phi; \alpha, \beta) + D(q(\theta , \textbf{z} | \gamma, \phi) || p(\theta, \textbf{z} | \textbf{w}, \alpha, \beta))$
    \item Maximizing the lower bound $\mathcal{L} (\gamma, \phi; \alpha, \beta)$ with respect to $\gamma$ and $\phi$ is equivalent to minimizing the KL divergence, which is what we want to achieve.
    \item From this, we can derive the updating functions for both the variational- \& non-variational parameters and the non-variational parameters
    \item $\alpha$ is updated via the following Newton-Raphson: $\alpha_{new} = \alpha_{old} - \textit{H}(\alpha_{old})^{-1} g(\alpha_{old})$ where the Hessian is $\frac{\partial L}{\partial \alpha_i \alpha_j} = \delta(i,j)\textit{M}\Psi'(\alpha_i) - \Psi'\left(\sum^k_{j=1}\alpha_j\right) $
    \item In the extended case, we have the additional variational parameter $\lambda$, which updates: $\lambda_{ij} = \eta + \sum^{M}_{d=1}\sum^{N_d}_{n=1}\phi^*_{dni}w^j_{dn}$ \& $\eta$ which updates similarly to $\alpha$
  \end{itemize}
\end{frame}

\begin{frame}{Variational Inference Algorithm}
  \begin{algorithm}[H]
    \begin{algorithmic}[1] 
      \State Initialize $\phi^{0}_{ni}$ $:=$ $1/k$ for all $i$ and $n$ \Comment{$\phi$ multinomial parameters}
      \State Initialize $\gamma_{i}$ $:=$ $\alpha_{i}$ + $N/k$ for all $i$ \Comment{$\gamma$ Dirichlet parameter}
      \Repeat
        \For{$n=1$ to $N$}
          \For{ $i=1$ to $N$}
            \State $\phi^{t+1}_{ni}$ $:=\beta_{iw_{n}} \exp(\Psi(\gamma^t_i))$ \Comment{$\Psi$ first derivative of $\log\Gamma$ function (approximated)}
          \EndFor 
          \State normalize $\phi^{t+1}_{n}$ to sum to $1$.
        \EndFor 
        \State $\gamma^{t+1} := \alpha + \sum^{N}_{n=1} \phi^{t+1}_{n}$
      \Until{convergence}
    \caption{Pseudocode for variational inference (for simplified model), adapted from \cite{LDA}}
    \end{algorithmic}
  \end{algorithm}
\end{frame}

\begin{frame}{Other Approaches}
  Prior approaches with the same goal include:
  \begin{itemize}
    \item \textbf{TFIDF}: Calculate a words' "importance" by term frequency over inverse document frequency \cite{TFIDF}
    \item \textbf{Unigram Mixture Model}: Probabilistically extends given labels for documents to unlabeled documents using a naive Bayes classifier in EM iterations \cite{Unigram}
    \item \textbf{LSI}: Perform SVD on term-frequency matrix to increase sparcity \cite{LSI}
    \item \textbf{pLSI}: Instead of SVD, use \textbf{\textit{p}}robabilistic generative model with a latent topic variable, estimate the model with existing methods (e.g. EM-Algorithm)\cite{PLSI}
  \end{itemize}  
  \begin{block}{Note}
    These approaches have significant shortcomings, explained in the following slide
  \end{block}
\end{frame}

\begin{frame}{Other Approaches}
  \framesubtitle{Shortcomings of Competing Methods}
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      Category $\backslash$ Method & TFIDF & Unigram Mixtures & LSI & pLSI \\
      \hline
      Large Reduction in Sizes & X &  &  & \\
      \hline
      Intra- / Interdocument Frequencies & X &  &  &  \\
      \hline
      Generates "Topics" & X & & X &  \\
      \hline
      Out-of-Sample Predictions & X & X & X & X \\
      \hline
    \end{tabular}
  \end{center}
  \begin{block}{Note}
    Existing Methods have significant shortcomings compared to LDA, which fulfills all listed criteria \cite{LDA}.
  \end{block}
\end{frame}

\begin{frame}{Other Approaches}
  \framesubtitle{Graphic Intuition}
  \begin{figure}
    \begin{center}
      \includegraphics[width=6.5cm]{Wordsimplex.png}
    \end{center}
    \caption{Graphic Representation of Unigram Mixtures, pLSI \& LDA \cite{LDA}}
  \end{figure}
\end{frame}

\AtNextBibliography{\small}
\begin{frame}[allowframebreaks]{Bibliography}
  \printbibliography[heading=none]
\end{frame}
  
\end{document}


